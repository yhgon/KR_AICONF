{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "current_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6KgnC5Qp9zM"
      },
      "source": [
        "# sound cls model with efficientNet-pytorch for DreamAI Healthcare Hackathon\n",
        "by Hyungfon Ryu | NVAITC\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYJGfeMGqRSC"
      },
      "source": [
        "## code include\n",
        "\n",
        "config.json\n",
        "- csv  generator\n",
        "- dataloader(GPU version, CPU version) \n",
        "- inference script\n",
        "- train script\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdspZ0uaqvcF"
      },
      "source": [
        "##best configuration for urban sound \n",
        " - 2 sec clip \n",
        " - 16khz\n",
        " - nfft : 4096\n",
        " - win_size : 1024\n",
        " - hop_size : 256\n",
        " - nmel :  128 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sOTDWee7_Ro"
      },
      "source": [
        "%%file config10.json\n",
        "{\n",
        "    \"train_config\": {\n",
        "        \"fp16_run\": true,\n",
        "        \"output_directory\": \"checkpoints\",\n",
        "        \"epochs\": 100000,\n",
        "        \"learning_rate\": 1e-4,\n",
        "        \"sigma\": 1.0,\n",
        "        \"iters_per_checkpoint\": 2000,\n",
        "        \"batch_size\": 12,\n",
        "        \"seed\": 1234,\n",
        "        \"checkpoint_path\": \"\",\n",
        "        \"with_tensorboard\": false\n",
        "    },\n",
        "    \"data_config\": {\n",
        "        \"training_files\": \"/home/hryu/cls.txt\",\n",
        "        \"segment_length\": 16000,\n",
        "        \"sampling_rate\": 16000,\n",
        "        \"filter_length\": 4096,\n",
        "        \"hop_length\": 512,\n",
        "        \"win_length\": 1024,\n",
        "        \"mel_fmin\": 0.0,\n",
        "        \"mel_fmax\": 8000.0\n",
        "    },\n",
        "    \"dist_config\": {\n",
        "        \"dist_backend\": \"nccl\",\n",
        "        \"dist_url\": \"tcp://localhost:54321\"\n",
        "    },\n",
        "\n",
        "    \"waveglow_config\": {\n",
        "        \"n_mel_channels\": 80,\n",
        "        \"n_flows\": 12,\n",
        "        \"n_group\": 8,\n",
        "        \"n_early_every\": 4,\n",
        "        \"n_early_size\": 2,\n",
        "        \"WN_config\": {\n",
        "            \"n_layers\": 8,\n",
        "            \"n_channels\": 256,\n",
        "            \"kernel_size\": 3\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpaQhWwMxTtY"
      },
      "source": [
        "%%file get_csv_cls.py\n",
        "# get file lists of audioDataset\n",
        "import os\n",
        "import glob\n",
        "import csv\n",
        "\n",
        "def datasetsWithCSV(filename='out.csv', info='cls_dir.txt', target_dir='./', output_dir='./'):\n",
        "    import os\n",
        "    \n",
        "    # search directory for class\n",
        "    file_list = sorted(os.listdir(target_dir))\n",
        "    print (\"file_list: {}\".format(file_list))\n",
        "    \n",
        "    # save mapping of class \n",
        "    info_filename = os.path.join(output_dir, info )\n",
        "    f = open(info_filename, 'w', encoding='utf-8')\n",
        "    for i, list in enumerate(file_list):\n",
        "        print(i, list )\n",
        "        f.write( '{},{}\\n'.format(i,list))            \n",
        "    f.close()\n",
        "\n",
        "\n",
        "    #\n",
        "\n",
        "    csv_filename = os.path.join(output_dir, filename )\n",
        "    f = open(csv_filename, 'w', encoding='utf-8')\n",
        "    \n",
        "    for i in range(len(file_list)):\n",
        "        \n",
        "        data_lists = glob.glob(os.path.join(target_dir+'/'+file_list[i], '**', '*.wav'), recursive=True)\n",
        "                \n",
        "        wr = csv.writer(f)\n",
        "\n",
        "        for idx, data in enumerate(data_lists):\n",
        "            #print(idx, data)\n",
        "            filename = data.split('/')[-1]\n",
        "            cls_dir = data.split('/')[-2]\n",
        "            full_wavefile = os.path.join(cls_dir,filename,  )\n",
        "            #print(idx, data, cls_dir, full_wavefile, full_wavefile )\n",
        "            wr.writerow([full_wavefile,i])\n",
        "            \n",
        "        print(i,\" number of files :\",len(data_lists))\n",
        "    \n",
        "    f.close()\n",
        "\n",
        "\n",
        "def main(filename, info, target_dir, output_dir ):\n",
        "    datasetsWithCSV(filename=filename, info=info, target_dir=target_dir, output_dir=output_dir  )\n",
        "\n",
        "\n",
        "\n",
        "if __name__ ==\"__main__\":\n",
        "    import argparse\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('-f', '--filename',   type=str, default='cls.csv')\n",
        "    parser.add_argument('-i', '--info',       type=str, default='cls_dir.txt')    \n",
        "    parser.add_argument('-d', '--target_dir', type=str, default='./')          \n",
        "    parser.add_argument('-w', '--output_dir', type=str, default='./')         \n",
        "    args = parser.parse_args()\n",
        "    \n",
        "    main(args.filename, args.info, args.target_dir, args.output_dir )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J44BrYb_8Uab"
      },
      "source": [
        "!python /home/hryu/mel2samp.py -f /home/hryu/new.txt -c /home/hryu/config2.json -o /Protein/mel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Y32_Vpd8Qxw"
      },
      "source": [
        "mel = torch.load('/Protein/mel2/choi01_00000_cut_file_s00001_e00008.wav.pt').cpu()\n",
        "plt.pcolormesh( mel  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GccMw-nU8bqa"
      },
      "source": [
        "%%file mel2samp.py\n",
        "## mel2sample \n",
        "\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "import argparse\n",
        "import json\n",
        "\n",
        "import time\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import numpy as np\n",
        "from scipy.signal import get_window\n",
        "from scipy.io.wavfile import read\n",
        "\n",
        "from librosa.util import pad_center, tiny\n",
        "import librosa.util as librosa_util\n",
        "from librosa.filters import mel as librosa_mel_fn\n",
        "\n",
        "MAX_WAV_VALUE = 32768.0\n",
        "\n",
        "def window_sumsquare(window, n_frames, hop_length=200, win_length=800,\n",
        "                     n_fft=800, dtype=np.float32, norm=None):\n",
        "\n",
        "    if win_length is None:\n",
        "        win_length = n_fft\n",
        "\n",
        "    n = n_fft + hop_length * (n_frames - 1)\n",
        "    x = np.zeros(n, dtype=dtype)\n",
        "\n",
        "    # Compute the squared window at the desired length\n",
        "    win_sq = get_window(window, win_length, fftbins=True)\n",
        "    win_sq = librosa_util.normalize(win_sq, norm=norm)**2\n",
        "    win_sq = librosa_util.pad_center(win_sq, n_fft)\n",
        "\n",
        "    # Fill the envelope\n",
        "    for i in range(n_frames):\n",
        "        sample = i * hop_length\n",
        "        x[sample:min(n, sample + n_fft)] += win_sq[:max(0, min(n_fft, n - sample))]\n",
        "    return x\n",
        "\n",
        "\n",
        "def griffin_lim(magnitudes, stft_fn, n_iters=30):\n",
        "    \"\"\"\n",
        "    PARAMS\n",
        "    ------\n",
        "    magnitudes: spectrogram magnitudes\n",
        "    stft_fn: STFT class with transform (STFT) and inverse (ISTFT) methods\n",
        "    \"\"\"\n",
        "\n",
        "    angles = np.angle(np.exp(2j * np.pi * np.random.rand(*magnitudes.size())))\n",
        "    angles = angles.astype(np.float32)\n",
        "    angles = torch.autograd.Variable(torch.from_numpy(angles))\n",
        "    signal = stft_fn.inverse(magnitudes, angles).squeeze(1)\n",
        "\n",
        "    for i in range(n_iters):\n",
        "        _, angles = stft_fn.transform(signal)\n",
        "        signal = stft_fn.inverse(magnitudes, angles).squeeze(1)\n",
        "    return signal\n",
        "\n",
        "\n",
        "def dynamic_range_compression(x, C=1, clip_val=1e-5):\n",
        "    \"\"\"\n",
        "    PARAMS\n",
        "    ------\n",
        "    C: compression factor\n",
        "    \"\"\"\n",
        "    return torch.log(torch.clamp(x, min=clip_val) * C)\n",
        "\n",
        "\n",
        "def dynamic_range_decompression(x, C=1):\n",
        "    \"\"\"\n",
        "    PARAMS\n",
        "    ------\n",
        "    C: compression factor used to compress\n",
        "    \"\"\"\n",
        "    return torch.exp(x) / C\n",
        "\n",
        "class STFT(torch.nn.Module):\n",
        "    \"\"\"adapted from Prem Seetharaman's https://github.com/pseeth/pytorch-stft\"\"\"\n",
        "    def __init__(self, filter_length=800, hop_length=200, win_length=800,\n",
        "                 window='hann'):\n",
        "        super(STFT, self).__init__()\n",
        "        self.filter_length = filter_length\n",
        "        self.hop_length = hop_length\n",
        "        self.win_length = win_length\n",
        "        self.window = window\n",
        "        self.forward_transform = None\n",
        "        scale = self.filter_length / self.hop_length\n",
        "        fourier_basis = np.fft.fft(np.eye(self.filter_length))\n",
        "\n",
        "        cutoff = int((self.filter_length / 2 + 1))\n",
        "        fourier_basis = np.vstack([np.real(fourier_basis[:cutoff, :]),\n",
        "                                   np.imag(fourier_basis[:cutoff, :])])\n",
        "\n",
        "        forward_basis = torch.FloatTensor(fourier_basis[:, None, :])\n",
        "        inverse_basis = torch.FloatTensor(\n",
        "            np.linalg.pinv(scale * fourier_basis).T[:, None, :])\n",
        "\n",
        "        if window is not None:\n",
        "            assert(filter_length >= win_length)\n",
        "            # get window and zero center pad it to filter_length\n",
        "            fft_window = get_window(window, win_length, fftbins=True)\n",
        "            fft_window = pad_center(fft_window, filter_length)\n",
        "            fft_window = torch.from_numpy(fft_window).float()\n",
        "\n",
        "            # window the bases\n",
        "            forward_basis *= fft_window\n",
        "            inverse_basis *= fft_window\n",
        "\n",
        "        self.register_buffer('forward_basis', forward_basis.float())\n",
        "        self.register_buffer('inverse_basis', inverse_basis.float())\n",
        "\n",
        "    def transform(self, input_data):\n",
        "        num_batches = input_data.size(0)\n",
        "        num_samples = input_data.size(1)\n",
        "\n",
        "        self.num_samples = num_samples\n",
        "\n",
        "        # similar to librosa, reflect-pad the input\n",
        "        input_data = input_data.view(num_batches, 1, num_samples)\n",
        "        input_data = F.pad(\n",
        "            input_data.unsqueeze(1),\n",
        "            (int(self.filter_length / 2), int(self.filter_length / 2), 0, 0),\n",
        "            mode='reflect')\n",
        "        input_data = input_data.squeeze(1)\n",
        "\n",
        "        forward_transform = F.conv1d(\n",
        "            input_data,\n",
        "            Variable(self.forward_basis, requires_grad=False),\n",
        "            stride=self.hop_length,\n",
        "            padding=0)\n",
        "\n",
        "        cutoff = int((self.filter_length / 2) + 1)\n",
        "        real_part = forward_transform[:, :cutoff, :]\n",
        "        imag_part = forward_transform[:, cutoff:, :]\n",
        "\n",
        "        magnitude = torch.sqrt(real_part**2 + imag_part**2)\n",
        "        phase = torch.autograd.Variable(\n",
        "            torch.atan2(imag_part.data, real_part.data))\n",
        "\n",
        "        return magnitude, phase\n",
        "\n",
        "    def inverse(self, magnitude, phase):\n",
        "        recombine_magnitude_phase = torch.cat(\n",
        "            [magnitude*torch.cos(phase), magnitude*torch.sin(phase)], dim=1)\n",
        "\n",
        "        inverse_transform = F.conv_transpose1d(\n",
        "            recombine_magnitude_phase,\n",
        "            Variable(self.inverse_basis, requires_grad=False),\n",
        "            stride=self.hop_length,\n",
        "            padding=0)\n",
        "\n",
        "        if self.window is not None:\n",
        "            window_sum = window_sumsquare(\n",
        "                self.window, magnitude.size(-1), hop_length=self.hop_length,\n",
        "                win_length=self.win_length, n_fft=self.filter_length,\n",
        "                dtype=np.float32)\n",
        "            # remove modulation effects\n",
        "            approx_nonzero_indices = torch.from_numpy(\n",
        "                np.where(window_sum > tiny(window_sum))[0])\n",
        "            window_sum = torch.autograd.Variable(\n",
        "                torch.from_numpy(window_sum), requires_grad=False)\n",
        "            window_sum = window_sum.cuda() if magnitude.is_cuda else window_sum\n",
        "            inverse_transform[:, :, approx_nonzero_indices] /= window_sum[approx_nonzero_indices]\n",
        "\n",
        "            # scale by hop ratio\n",
        "            inverse_transform *= float(self.filter_length) / self.hop_length\n",
        "\n",
        "        inverse_transform = inverse_transform[:, :, int(self.filter_length/2):]\n",
        "        inverse_transform = inverse_transform[:, :, :-int(self.filter_length/2):]\n",
        "\n",
        "        return inverse_transform\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        self.magnitude, self.phase = self.transform(input_data)\n",
        "        reconstruction = self.inverse(self.magnitude, self.phase)\n",
        "        return reconstruction\n",
        "\n",
        "\n",
        "class TacotronSTFT(torch.nn.Module):\n",
        "    def __init__(self, filter_length=1024, hop_length=256, win_length=1024,\n",
        "                 n_mel_channels=80, sampling_rate=22050, mel_fmin=0.0,\n",
        "                 mel_fmax=8000.0):\n",
        "        super(TacotronSTFT, self).__init__()\n",
        "        self.n_mel_channels = n_mel_channels\n",
        "        self.sampling_rate = sampling_rate\n",
        "        self.stft_fn = STFT(filter_length, hop_length, win_length)\n",
        "        mel_basis = librosa_mel_fn(\n",
        "            sampling_rate, filter_length, n_mel_channels, mel_fmin, mel_fmax)\n",
        "        mel_basis = torch.from_numpy(mel_basis).float()\n",
        "        self.register_buffer('mel_basis', mel_basis)\n",
        "\n",
        "    def spectral_normalize(self, magnitudes):\n",
        "        output = dynamic_range_compression(magnitudes)\n",
        "        return output\n",
        "\n",
        "    def spectral_de_normalize(self, magnitudes):\n",
        "        output = dynamic_range_decompression(magnitudes)\n",
        "        return output\n",
        "\n",
        "    def mel_spectrogram(self, y):\n",
        "        \"\"\"Computes mel-spectrograms from a batch of waves\n",
        "        PARAMS\n",
        "        ------\n",
        "        y: Variable(torch.FloatTensor) with shape (B, T) in range [-1, 1]\n",
        "        RETURNS\n",
        "        -------\n",
        "        mel_output: torch.FloatTensor of shape (B, n_mel_channels, T)\n",
        "        \"\"\"\n",
        "        assert(torch.min(y.data) >= -1)\n",
        "        assert(torch.max(y.data) <= 1)\n",
        "\n",
        "        magnitudes, phases = self.stft_fn.transform(y)\n",
        "        magnitudes = magnitudes.data\n",
        "        mel_output = torch.matmul(self.mel_basis, magnitudes)\n",
        "        mel_output = self.spectral_normalize(mel_output)\n",
        "        return mel_output\n",
        "\n",
        "def files_to_list(filename, split=\"|\"):\n",
        "    with open(filename, encoding='utf-8') as f:\n",
        "        filepaths_and_cls = [line.strip().split(split) for line in f]\n",
        "    return filepaths_and_cls\n",
        "\n",
        "\n",
        "def load_wav_to_torch(full_path):\n",
        "    import librosa\n",
        "    \"\"\"\n",
        "    Loads wavdata into torch array\n",
        "    \"\"\"\n",
        "    #sampling_rate, data = read(full_path) #scipy version \n",
        "    data, sampling_rate = librosa.load(full_path, sr=16000, res_type='polyphase', mono=True  )\n",
        "\n",
        "    return torch.from_numpy(data).float(), sampling_rate\n",
        "\n",
        "class Mel2Samp(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    This is the main class that calculates the spectrogram and returns the\n",
        "    spectrogram, audio pair.\n",
        "    \"\"\"\n",
        "    def __init__(self, training_files, segment_length, filter_length,\n",
        "                 hop_length, win_length, sampling_rate, mel_fmin, mel_fmax):\n",
        "        self.audiopaths_and_cls = files_to_list(training_files)\n",
        "        random.seed(1234)\n",
        "        random.shuffle(self.audiopaths_and_cls)\n",
        "        self.stft = TacotronSTFT(filter_length=filter_length,\n",
        "                                 hop_length=hop_length,\n",
        "                                 win_length=win_length,\n",
        "                                 sampling_rate=sampling_rate,\n",
        "                                 mel_fmin=mel_fmin, mel_fmax=mel_fmax)\n",
        "        self.segment_length = segment_length\n",
        "        self.sampling_rate = sampling_rate\n",
        "\n",
        "    \n",
        "\n",
        "    def get_mel(self, audio):\n",
        "        audio_norm = audio / MAX_WAV_VALUE\n",
        "        audio_norm = audio_norm.unsqueeze(0)\n",
        "        audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
        "        melspec = self.stft.mel_spectrogram(audio_norm)\n",
        "        melspec = torch.squeeze(melspec, 0)\n",
        "        return melspec\n",
        "\n",
        "    def get_cls(self, cls):\n",
        "        #print(\"DEBUG : cls inside : before {} {:04d} \".format( cls, int(cls) ) ) \n",
        "         \n",
        "        num_cls = torch.tensor( int(cls) )\n",
        "        #print(\"DEBUG : cls inside : after \", num_cls)        \n",
        "        return num_cls        \n",
        "\n",
        "    def get_mel_cls_pair(self, audiopath_and_cls):\n",
        "        audiopath, cls = audiopath_and_cls[0], audiopath_and_cls[1]\n",
        "        audio, sr = load_wav_to_torch(audiopath)\n",
        "        mel = self.get_mel(audio)\n",
        "        cls = self.get_cls(cls)\n",
        "        #print('DEBUG', cls  )\n",
        "        return (  mel, cls )        \n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Read audio\n",
        "        filepath_cls = self.audiopaths_and_cls[index]\n",
        "        filepath=filepath_cls[0]\n",
        "        cls = filepath_cls[1]\n",
        "        #print(\"DEBUG :\", filepath_cls  )\n",
        "        #print(\"DEBUG :\",  os.path.basename(filepath)   )\n",
        "        #print(\"DEBUG :\",   cls )\n",
        "        \n",
        "        audio, sampling_rate = load_wav_to_torch(filepath)\n",
        "        if sampling_rate != self.sampling_rate:\n",
        "            raise ValueError(\"{} SR doesn't match target {} SR\".format(\n",
        "                sampling_rate, self.sampling_rate))\n",
        "\n",
        "        # Take segment\n",
        "        if audio.size(0) >= self.segment_length:\n",
        "            max_audio_start = audio.size(0) - self.segment_length\n",
        "            audio_start = random.randint(0, max_audio_start)\n",
        "            audio = audio[audio_start:audio_start+self.segment_length]\n",
        "        else:\n",
        "            audio = torch.nn.functional.pad(audio, (0, self.segment_length - audio.size(0)), 'constant').data\n",
        "\n",
        "        mel = self.get_mel(audio)\n",
        "        cls = self.get_cls(cls)\n",
        "        #print('DEBUG mel : ', mel  )\n",
        "        #print('DEBUG cls : ', cls  )\n",
        "         \n",
        "\n",
        "        return (mel, cls)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audiopaths_and_cls)\n",
        " \n",
        "\n",
        "# ===================================================================\n",
        "# Takes directory of clean audio and makes directory of spectrograms\n",
        "# Useful for making test sets\n",
        "# ===================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    # Get defaults so it can work with no Sacred\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('-f', \"--filelist_path\", required=True)\n",
        "    parser.add_argument('-c', '--config', type=str,\n",
        "                        help='JSON file for configuration')\n",
        "    parser.add_argument('-o', '--output_dir', type=str,\n",
        "                        help='Output directory')\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    with open(args.config) as f:\n",
        "        data = f.read()\n",
        "    data_config = json.loads(data)[\"data_config\"]\n",
        "    mel2samp = Mel2Samp(**data_config)\n",
        "\n",
        "    filepaths_cls = files_to_list(args.filelist_path)\n",
        " \n",
        "\n",
        "    # Make directory if it doesn't exist\n",
        "    if not os.path.isdir(args.output_dir):\n",
        "        os.makedirs(args.output_dir)\n",
        "        os.chmod(args.output_dir, 0o775)\n",
        "\n",
        "\n",
        "    for i,filepath_cls in enumerate(filepaths_cls):\n",
        "        filepath=filepath_cls[0]\n",
        "        cls = filepath_cls[1]\n",
        "        tic_start = time.time()\n",
        "        audio, sr = load_wav_to_torch(filepath)\n",
        "        tic_load = time.time()\n",
        "        dur_load = tic_load - tic_start\n",
        "        print(\"{:d}  cls:{} load {:4.2f}sec\".format(i,   cls , dur_load), end='' )\n",
        "        melspectrogram = mel2samp.get_mel(audio)\n",
        "        tic_mel  = time.time()\n",
        "        dur_mel = tic_mel - tic_load\n",
        "        print(\"  mel {:4.2f}sec\".format(dur_mel) , end='')\n",
        "        filename = os.path.basename(filepath)        \n",
        "        new_filepath = args.output_dir + '/' + filename + '.pt'\n",
        "        torch.save(melspectrogram, new_filepath)\n",
        "        tic_save = time.time()\n",
        "        dur_save = tic_save - tic_mel\n",
        "        #print(\"DEBUG: save {:4.2f}sec : {:4.2f}sec cls:{} mel:{}\".format( dur_save, tic_save - tic_start,cls, melspectrogram ) )\n",
        "        print(\" save {:4.2f}sec : {:4.2f}sec  \".format( dur_save, tic_save - tic_start, ) )\n",
        "        #print( \"cls{} {:04d}  {:04d}  {:04d}  \".format(cls, int(cls),   torch.tensor( 0 ), torch.tensor( int(cls)  )   )   )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1ko3upJ33kS"
      },
      "source": [
        "%%file mel2samp_GPU_infer.py\n",
        "## mel2sample \n",
        "\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "import argparse\n",
        "import json\n",
        "\n",
        "import time\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import numpy as np\n",
        "from scipy.signal import get_window\n",
        "from scipy.io.wavfile import read\n",
        "\n",
        "from librosa.util import pad_center, tiny\n",
        "import librosa.util as librosa_util\n",
        "from librosa.filters import mel as librosa_mel_fn\n",
        "\n",
        "MAX_WAV_VALUE = 32768.0\n",
        "\n",
        "def window_sumsquare(window, n_frames, hop_length=200, win_length=800,\n",
        "                     n_fft=800, dtype=np.float32, norm=None):\n",
        "\n",
        "    if win_length is None:\n",
        "        win_length = n_fft\n",
        "\n",
        "    n = n_fft + hop_length * (n_frames - 1)\n",
        "    x = np.zeros(n, dtype=dtype)\n",
        "\n",
        "    # Compute the squared window at the desired length\n",
        "    win_sq = get_window(window, win_length, fftbins=True)\n",
        "    win_sq = librosa_util.normalize(win_sq, norm=norm)**2\n",
        "    win_sq = librosa_util.pad_center(win_sq, n_fft)\n",
        "\n",
        "    # Fill the envelope\n",
        "    for i in range(n_frames):\n",
        "        sample = i * hop_length\n",
        "        x[sample:min(n, sample + n_fft)] += win_sq[:max(0, min(n_fft, n - sample))]\n",
        "    return x\n",
        "\n",
        "\n",
        "def griffin_lim(magnitudes, stft_fn, n_iters=30):\n",
        "    \"\"\"\n",
        "    PARAMS\n",
        "    ------\n",
        "    magnitudes: spectrogram magnitudes\n",
        "    stft_fn: STFT class with transform (STFT) and inverse (ISTFT) methods\n",
        "    \"\"\"\n",
        "\n",
        "    angles = np.angle(np.exp(2j * np.pi * np.random.rand(*magnitudes.size())))\n",
        "    angles = angles.astype(np.float32)\n",
        "    angles = torch.autograd.Variable(torch.from_numpy(angles))\n",
        "    signal = stft_fn.inverse(magnitudes, angles).squeeze(1)\n",
        "\n",
        "    for i in range(n_iters):\n",
        "        _, angles = stft_fn.transform(signal)\n",
        "        signal = stft_fn.inverse(magnitudes, angles).squeeze(1)\n",
        "    return signal\n",
        "\n",
        "\n",
        "def dynamic_range_compression(x, C=1, clip_val=1e-5):\n",
        "    \"\"\"\n",
        "    PARAMS\n",
        "    ------\n",
        "    C: compression factor\n",
        "    \"\"\"\n",
        "    return torch.log(torch.clamp(x, min=clip_val) * C)\n",
        "\n",
        "\n",
        "def dynamic_range_decompression(x, C=1):\n",
        "    \"\"\"\n",
        "    PARAMS\n",
        "    ------\n",
        "    C: compression factor used to compress\n",
        "    \"\"\"\n",
        "    return torch.exp(x) / C\n",
        "\n",
        "class STFT(torch.nn.Module):\n",
        "    \"\"\"adapted from Prem Seetharaman's https://github.com/pseeth/pytorch-stft\"\"\"\n",
        "    def __init__(self, filter_length=800, hop_length=200, win_length=800,\n",
        "                 window='hann'):\n",
        "        super(STFT, self).__init__()\n",
        "        self.device = torch.device('cuda')\n",
        "        self.filter_length = filter_length\n",
        "        self.hop_length = hop_length\n",
        "        self.win_length = win_length\n",
        "        self.window = window\n",
        "        self.forward_transform = None\n",
        "        scale = self.filter_length / self.hop_length\n",
        "        fourier_basis = np.fft.fft(np.eye(self.filter_length))\n",
        "\n",
        "        cutoff = int((self.filter_length / 2 + 1))\n",
        "        fourier_basis = np.vstack([np.real(fourier_basis[:cutoff, :]),\n",
        "                                   np.imag(fourier_basis[:cutoff, :])])\n",
        "\n",
        "        forward_basis = torch.FloatTensor(fourier_basis[:, None, :]).to(self.device)\n",
        "        inverse_basis = torch.FloatTensor(\n",
        "            np.linalg.pinv(scale * fourier_basis).T[:, None, :]).to(self.device)\n",
        "\n",
        "        if window is not None:\n",
        "            assert(filter_length >= win_length)\n",
        "            # get window and zero center pad it to filter_length\n",
        "            fft_window = get_window(window, win_length, fftbins=True)\n",
        "            fft_window = pad_center(fft_window, filter_length)\n",
        "            fft_window = torch.from_numpy(fft_window).float().to(self.device)\n",
        "\n",
        "            # window the bases\n",
        "            forward_basis *= fft_window\n",
        "            inverse_basis *= fft_window\n",
        "\n",
        "        self.register_buffer('forward_basis', forward_basis.float()) \n",
        "        self.register_buffer('inverse_basis', inverse_basis.float()) \n",
        "\n",
        "    def transform(self, input_data):\n",
        "        num_batches = input_data.size(0)\n",
        "        num_samples = input_data.size(1)\n",
        "\n",
        "        self.num_samples = num_samples\n",
        "\n",
        "        # similar to librosa, reflect-pad the input\n",
        "        input_data = input_data.view(num_batches, 1, num_samples).to(self.device)\n",
        "        input_data = F.pad(\n",
        "            input_data.unsqueeze(1),\n",
        "            (int(self.filter_length / 2), int(self.filter_length / 2), 0, 0),\n",
        "            mode='reflect')\n",
        "        input_data = input_data.squeeze(1)\n",
        "\n",
        "        forward_transform = F.conv1d(\n",
        "            input_data,\n",
        "            Variable(self.forward_basis, requires_grad=False),\n",
        "            stride=self.hop_length,\n",
        "            padding=0)\n",
        "\n",
        "        cutoff = int((self.filter_length / 2) + 1)\n",
        "        real_part = forward_transform[:, :cutoff, :]\n",
        "        imag_part = forward_transform[:, cutoff:, :]\n",
        "\n",
        "        magnitude = torch.sqrt(real_part**2 + imag_part**2)\n",
        "        phase = torch.autograd.Variable(\n",
        "            torch.atan2(imag_part.data, real_part.data))\n",
        "\n",
        "        return magnitude, phase\n",
        "\n",
        "    def inverse(self, magnitude, phase):\n",
        "        recombine_magnitude_phase = torch.cat(\n",
        "            [magnitude*torch.cos(phase), magnitude*torch.sin(phase)], dim=1)\n",
        "\n",
        "        inverse_transform = F.conv_transpose1d(\n",
        "            recombine_magnitude_phase,\n",
        "            Variable(self.inverse_basis, requires_grad=False),\n",
        "            stride=self.hop_length,\n",
        "            padding=0)\n",
        "\n",
        "        if self.window is not None:\n",
        "            window_sum = window_sumsquare(\n",
        "                self.window, magnitude.size(-1), hop_length=self.hop_length,\n",
        "                win_length=self.win_length, n_fft=self.filter_length,\n",
        "                dtype=np.float32)\n",
        "            # remove modulation effects\n",
        "            approx_nonzero_indices = torch.from_numpy(\n",
        "                np.where(window_sum > tiny(window_sum))[0])\n",
        "            window_sum = torch.autograd.Variable(\n",
        "                torch.from_numpy(window_sum), requires_grad=False)\n",
        "            window_sum = window_sum.cuda() if magnitude.is_cuda else window_sum\n",
        "            inverse_transform[:, :, approx_nonzero_indices] /= window_sum[approx_nonzero_indices]\n",
        "\n",
        "            # scale by hop ratio\n",
        "            inverse_transform *= float(self.filter_length) / self.hop_length\n",
        "\n",
        "        inverse_transform = inverse_transform[:, :, int(self.filter_length/2):]\n",
        "        inverse_transform = inverse_transform[:, :, :-int(self.filter_length/2):]\n",
        "\n",
        "        return inverse_transform\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        self.magnitude, self.phase = self.transform(input_data)\n",
        "        reconstruction = self.inverse(self.magnitude, self.phase)\n",
        "        return reconstruction\n",
        "\n",
        "\n",
        "class TacotronSTFT(torch.nn.Module):\n",
        "    def __init__(self, filter_length=1024, hop_length=256, win_length=1024,\n",
        "                 n_mel_channels=80, sampling_rate=22050, mel_fmin=0.0,\n",
        "                 mel_fmax=8000.0):\n",
        "        super(TacotronSTFT, self).__init__()\n",
        "        self.n_mel_channels = n_mel_channels\n",
        "        self.sampling_rate = sampling_rate\n",
        "        self.stft_fn = STFT(filter_length, hop_length, win_length)\n",
        "        mel_basis = librosa_mel_fn(\n",
        "            sampling_rate, filter_length, n_mel_channels, mel_fmin, mel_fmax)\n",
        "        mel_basis = torch.from_numpy(mel_basis).float()\n",
        "        self.register_buffer('mel_basis', mel_basis)\n",
        "\n",
        "    def spectral_normalize(self, magnitudes):\n",
        "        output = dynamic_range_compression(magnitudes)\n",
        "        return output\n",
        "\n",
        "    def spectral_de_normalize(self, magnitudes):\n",
        "        output = dynamic_range_decompression(magnitudes)\n",
        "        return output\n",
        "\n",
        "    def mel_spectrogram(self, y):\n",
        "        \"\"\"Computes mel-spectrograms from a batch of waves\n",
        "        PARAMS\n",
        "        ------\n",
        "        y: Variable(torch.FloatTensor) with shape (B, T) in range [-1, 1]\n",
        "        RETURNS\n",
        "        -------\n",
        "        mel_output: torch.FloatTensor of shape (B, n_mel_channels, T)\n",
        "        \"\"\"\n",
        "        assert(torch.min(y.data) >= -1)\n",
        "        assert(torch.max(y.data) <= 1)\n",
        "\n",
        "        magnitudes, phases = self.stft_fn.transform(y)\n",
        "        magnitudes = magnitudes.data\n",
        "        mel_output = torch.matmul(self.mel_basis, magnitudes)\n",
        "        mel_output = self.spectral_normalize(mel_output)\n",
        "        return mel_output\n",
        "\n",
        "def files_to_list(filename, split=\"|\"):\n",
        "    with open(filename, encoding='utf-8') as f:\n",
        "        filepaths_and_cls = [line.strip().split(split) for line in f]\n",
        "    return filepaths_and_cls\n",
        "\n",
        "\n",
        "def load_wav_to_torch(full_path):\n",
        "    import librosa\n",
        "    \"\"\"\n",
        "    Loads wavdata into torch array\n",
        "    \"\"\"\n",
        "    #sampling_rate, data = read(full_path) #scipy version \n",
        "    #data, sampling_rate = librosa.load(full_path, sr=16000, res_type='polyphase', mono=True, duration=2  )\n",
        "    data, sampling_rate = librosa.load(full_path, sr=16000, res_type='polyphase', mono=True  )    \n",
        "\n",
        "    return torch.from_numpy(data).float(), sampling_rate\n",
        "\n",
        "class Mel2Samp(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    This is the main class that calculates the spectrogram and returns the\n",
        "    spectrogram, audio pair.\n",
        "    \"\"\"\n",
        "    def __init__(self, \n",
        "                 segment_length, \n",
        "                 filter_length, \n",
        "                 n_mel_channels,\n",
        "                 hop_length, win_length, \n",
        "                 sampling_rate, \n",
        "                 mel_fmin, \n",
        "                 mel_fmax, \n",
        "                 files_list):\n",
        "        self.files_list=files_list \n",
        "        self.device = torch.device('cuda')\n",
        "\n",
        "        self.stft = TacotronSTFT(filter_length=filter_length,\n",
        "                                 n_mel_channels=n_mel_channels,\n",
        "                                 hop_length=hop_length,\n",
        "                                 win_length=win_length,\n",
        "                                 sampling_rate=sampling_rate,\n",
        "                                 mel_fmin=mel_fmin, \n",
        "                                 mel_fmax=mel_fmax).to(self.device)\n",
        "        self.segment_length = segment_length\n",
        "        self.sampling_rate = sampling_rate\n",
        "\n",
        "\n",
        "    def get_mel(self, audio):\n",
        "        audio_norm = audio / MAX_WAV_VALUE\n",
        "        audio_norm = audio_norm.unsqueeze(0)\n",
        "        audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
        "        melspec = self.stft.mel_spectrogram(audio_norm)\n",
        "        melspec = torch.squeeze(melspec, 0)\n",
        "        return melspec  \n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Read audio\n",
        "\n",
        "        filepath = self.files_list \n",
        "\n",
        "        #print(\"DEBUG :\", filepath_cls  )\n",
        "        #print(\"DEBUG :\",  os.path.basename(filepath)   )\n",
        "        #print(\"DEBUG :\",   cls )\n",
        "        \n",
        "        audio, sampling_rate = load_wav_to_torch(filepath)\n",
        "        if sampling_rate != self.sampling_rate:\n",
        "            raise ValueError(\"{} SR doesn't match target {} SR\".format(\n",
        "                sampling_rate, self.sampling_rate))\n",
        "\n",
        "        # Take segment\n",
        "        if audio.size(0) >= self.segment_length:\n",
        "            max_audio_start = audio.size(0) - self.segment_length\n",
        "            audio_start = random.randint(0, max_audio_start)\n",
        "            audio = audio[audio_start:audio_start+self.segment_length]\n",
        "        else:\n",
        "            audio = torch.nn.functional.pad(audio, (0, self.segment_length - audio.size(0)), 'constant').data\n",
        "\n",
        "        audio.to(self.device)\n",
        "        mel = self.get_mel(audio)\n",
        "\n",
        "        #print('DEBUG mel : ', mel  )\n",
        "        #print('DEBUG cls : ', cls  )\n",
        "         \n",
        "\n",
        "        return (mel)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files_list)\n",
        " \n",
        "\n",
        "# ===================================================================\n",
        "# Takes directory of clean audio and makes directory of spectrograms\n",
        "# Useful for making test sets\n",
        "# ===================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    # Get defaults so it can work with no Sacred\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('-f', \"--filelist_path\", required=True)\n",
        "    parser.add_argument('-c', '--config', type=str,\n",
        "                        help='JSON file for configuration')\n",
        "    parser.add_argument('-o', '--output_dir', type=str,\n",
        "                        help='Output directory')\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    with open(args.config) as f:\n",
        "        data = f.read()\n",
        "    data_config = json.loads(data)[\"data_config\"]\n",
        "    print(data_config)\n",
        "    mel2samp = Mel2Samp(**data_config)\n",
        "\n",
        "    filepaths_cls = files_to_list(args.filelist_path)\n",
        " \n",
        "\n",
        "    # Make directory if it doesn't exist\n",
        "    if not os.path.isdir(args.output_dir):\n",
        "        os.makedirs(args.output_dir)\n",
        "        os.chmod(args.output_dir, 0o775)\n",
        "\n",
        "\n",
        "    for i,filepath_cls in enumerate(filepaths_cls):\n",
        "        filepath=filepath_cls[0]\n",
        "        cls = filepath_cls[1]\n",
        "        #print(filepath)\n",
        "        \n",
        "        filename = filepath.split('/')[-1]\n",
        "        cls_dir = filepath.split('/')[-2]\n",
        "        #print(cls_dir, filename)\n",
        "        dir_cls = os.path.join(args.output_dir, cls_dir)\n",
        "        if not os.path.isdir(dir_cls):\n",
        "            os.makedirs(dir_cls)\n",
        "            os.chmod(dir_cls, 0o775)\n",
        "        \n",
        "        tic_start = time.time()\n",
        "        audio, sr = load_wav_to_torch(filepath)\n",
        "        tic_load = time.time()\n",
        "        dur_load = tic_load - tic_start\n",
        "        #print(\"{:d}  cls:{} load {:4.2f}sec\".format(i,   cls , dur_load), end='' )\n",
        "        audio.to(torch.device('cuda'))\n",
        "        melspectrogram = mel2samp.get_mel(audio)\n",
        "        tic_mel  = time.time()\n",
        "        dur_mel = tic_mel - tic_load\n",
        "        #print(\"  mel {:4.2f}sec\".format(dur_mel) , end='')\n",
        "        filename_body, ext = os.path.splitext(filename)        \n",
        "        new_filepath = os.path.join( dir_cls,   filename_body + '.pt')\n",
        "        torch.save(melspectrogram.cpu(), new_filepath)\n",
        "        tic_save = time.time()\n",
        "        dur_save = tic_save - tic_mel\n",
        "        #print(\"DEBUG: save {:4.2f}sec : {:4.2f}sec cls:{} mel:{}\".format( dur_save, tic_save - tic_start,cls, melspectrogram ) )\n",
        "        #print(\" save {:4.2f}sec : {:4.2f}sec  \".format( dur_save, tic_save - tic_start, ) )\n",
        "        #print( \"cls{} {:04d}  {:04d}  {:04d}  \".format(cls, int(cls),   torch.tensor( 0 ), torch.tensor( int(cls)  )   )   )\n",
        "        if i % 1000 ==0 :\n",
        "            print( i )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKbyy87f8A0l"
      },
      "source": [
        "%%file train.py\n",
        "import time\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.ion()\n",
        "import json\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler as lrs\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import random\n",
        "\n",
        "\n",
        "from mel2samp import Mel2Samp\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint_path, model, optimizer):\n",
        "    assert os.path.isfile(checkpoint_path)\n",
        "    print(\"Loading checkpoint '{}'\".format(checkpoint_path))\n",
        "    checkpoint_dict = torch.load(checkpoint_path, map_location='cpu')\n",
        "    model.load_state_dict(checkpoint_dict['state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint_dict['optimizer'])\n",
        "    learning_rate = checkpoint_dict['learning_rate']\n",
        "    epoch = checkpoint_dict['epoch']\n",
        "    print(\"Loaded checkpoint '{}' from epoch {}\" .format(\n",
        "        checkpoint_path, epoch))\n",
        "    return model, optimizer, learning_rate, epoch\n",
        "\n",
        "def save_checkpoint(model, optimizer, learning_rate, epoch, filepath):\n",
        "    print(\"Saving model and optimizer state at epoch {} to {}\".format(\n",
        "          epoch, filepath))\n",
        "    torch.save({'epoch': epoch,\n",
        "                'state_dict': model.state_dict(),\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "                'learning_rate': learning_rate}, filepath)   \n",
        "\n",
        "\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "class model_efficientNet(nn.Module):\n",
        "\n",
        "    def __init__(self, model_name, num_classes):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.bw2col = nn.Sequential(\n",
        "            nn.BatchNorm2d(1),\n",
        "            nn.Conv2d(1, 10, 1, padding=0), nn.ReLU(),\n",
        "            nn.Conv2d(10, 3, 1, padding=0), nn.ReLU())\n",
        "\n",
        "        self.enet_model = EfficientNet.from_pretrained(model_name)\n",
        "        print('input_size : ', self.enet_model.get_image_size(model_name))\n",
        "\n",
        "        self.final = nn.Sequential(\n",
        "            nn.Linear(1280, 512), nn.ReLU(), nn.BatchNorm1d(512),\n",
        "            nn.Linear(512, num_classes))\n",
        "            \n",
        "        self.soft = nn.Softmax(  )\n",
        "\n",
        "    def forward(self, x):\n",
        "        #print('1', x.shape)\n",
        "        x = self.bw2col(x)\n",
        "        #print('2 bw2col',x.shape)\n",
        "        x = self.enet_model.extract_features(x)\n",
        "        #print('3 efficientnet ',x.shape)\n",
        "        x = x.max(dim=-1)[0].max(dim=-1)[0]\n",
        "        #print('4 dim reduct',x.shape)\n",
        "        x = self.final(x)\n",
        "        #print('5 final',x.shape)\n",
        "        x = self.soft(x)\n",
        "        #print('6 soft',x.shape)\n",
        "        return x\n",
        "    \n",
        "\n",
        "def train(config='/home/hryu/config10.json',   ch_dir='/Protein/gc2020/en_0730_6pm'):\n",
        " \n",
        "    ## 4096(nfft), 1024(win), 512(hop)\n",
        "    with open(config) as f:\n",
        "        data = f.read()\n",
        "    config = json.loads(data)\n",
        "    train_config = config[\"train_config\"]\n",
        "    global data_config\n",
        "    data_config = config[\"data_config\"]\n",
        "\n",
        "    print(data_config)\n",
        "    dataset = Mel2Samp(**data_config) \n",
        "\n",
        "    val_split = 0.8 \n",
        "    train_set_len = int(len(dataset) * val_split)\n",
        "    valid_set_len = len(dataset) - train_set_len \n",
        "\n",
        "    train_set, valid_set = torch.utils.data.random_split(dataset, [train_set_len, valid_set_len ])\n",
        "\n",
        "    print('Total : ',  len(dataset) )\n",
        "    print('Train : ',  len(train_set) )\n",
        "    print('Valid : ',  len(valid_set) )\n",
        "\n",
        "\n",
        "    num_gpus = 1\n",
        "    batch_size = 256 \n",
        "    train_sampler = DistributedSampler(train_set) if num_gpus > 1 else None\n",
        "    valid_sampler = DistributedSampler(valid_set) if num_gpus > 1 else None\n",
        "\n",
        "    from torch.utils.data import DataLoader\n",
        "    train_loader = DataLoader(train_set, \n",
        "                              num_workers=20, \n",
        "                              shuffle=True,\n",
        "                              sampler=train_sampler,\n",
        "                              batch_size=batch_size,\n",
        "                              pin_memory=True,\n",
        "                              drop_last=True)\n",
        "\n",
        "    val_loader = DataLoader(valid_set, \n",
        "                              num_workers=20, \n",
        "                              shuffle=True,\n",
        "                              sampler=valid_sampler,\n",
        "                              batch_size=batch_size,\n",
        "                              pin_memory=True,\n",
        "                              drop_last=True)\n",
        "\n",
        "    print(len(train_loader))\n",
        "    print(len(val_loader))\n",
        "\n",
        "\n",
        "    print(train_loader)\n",
        "\n",
        "    cuda = True\n",
        "    device = torch.device('cuda:0' if cuda else 'cpu')\n",
        "    my_model = model_efficientNet('efficientnet-b1', 16).to(device)\n",
        "    my_model = my_model.train()\n",
        "\n",
        "\n",
        "    learning_rate = 0.001\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(my_model.parameters(), lr=learning_rate )\n",
        "\n",
        "\n",
        "    restart = False\n",
        "\n",
        "    epoch_offset = 0\n",
        "\n",
        "    if restart==True :\n",
        "\n",
        "        checkpoint_path = '/Protein/gc2020/en_0730_6pm/model_005.pt'\n",
        "        my_model, optimizer, _learning_rate, epoch = load_checkpoint( checkpoint_path, my_model, optimizer)\n",
        "        epoch_offset = epoch + 1\n",
        "\n",
        "    import numpy as np \n",
        "    my_model.train()\n",
        "    print(len(train_loader))\n",
        "    epochs = 400\n",
        "    epoch_save = 1\n",
        "    i_total = 0\n",
        "\n",
        "    print(\"start\")\n",
        "    for e in range(epoch_offset, epochs):\n",
        "        tic_epoch = time.time()\n",
        "        total_iter = 0\n",
        "        correct_iter = 0 \n",
        "\n",
        "        for i, batch in enumerate(train_loader):\n",
        "            tic_iter = time.time()\n",
        "            my_model.zero_grad()\n",
        "            mel, cls = batch\n",
        "            #print(mel.shape)\n",
        "            mel = torch.autograd.Variable(mel.cuda())\n",
        "            mymel=mel.view(mel.shape[0],1,  mel.shape[1], mel.shape[2])\n",
        "            #print('mel  shape : ', mymel.shape  )\n",
        "\n",
        "            pred = my_model(mymel)\n",
        "            #print('  pred shape : ', pred,   pred.shape  )\n",
        "            #print('  cls shape : ',   cls   )\n",
        "            cls = torch.autograd.Variable(cls.cuda())\n",
        "            loss = criterion(pred, cls)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            _, predicted = torch.max(pred, 1)\n",
        "            total_iter   += cls.size(0)\n",
        "            correct_iter += (predicted == cls).sum().item()\n",
        "            accu_iter = 100 * correct_iter / total_iter\n",
        "\n",
        "            toc_iter = time.time()\n",
        "            dur_iter = toc_iter - tic_iter\n",
        "            print(\"\\n e{:03d}/{:03d} i {:06d}  {:03d}/{:03d} | loss: {:08.8f} acc_iter : {:4.2f}% {:5.2f}sec/iter\".format( e, epochs, i_total, i, len(train_loader),  loss.item()  , accu_iter, dur_iter ) ,end='' )\n",
        "            i_total +=1\n",
        "\n",
        "        accu_iter = 100 * correct_iter / total_iter    \n",
        "        toc_epoch = time.time()\n",
        "        dur_epoch = toc_epoch - tic_epoch\n",
        "        remain_epoch = epochs - e -1\n",
        "        dur_remain =  dur_epoch * remain_epoch\n",
        "        print(' | acc_train : {:4.2f}%  | {:6.4f}sec/epoch  remain {:4.2f}min for {:d}epoch'.format(accu_iter, dur_epoch,  dur_remain/60, remain_epoch), end='')\n",
        "\n",
        "        correct_val = 0\n",
        "        total_val = 0\n",
        "        tic_val = time.time()\n",
        "        with torch.no_grad():\n",
        "\n",
        "            for i, batch in enumerate(val_loader):\n",
        "                mel, cls = batch\n",
        "                mel = torch.autograd.Variable(mel.cuda())\n",
        "                mel.shape[0], mel.shape[1], mel.shape[2]\n",
        "                mymel=mel.view(mel.shape[0],1,  mel.shape[1], mel.shape[2])\n",
        "                cls = torch.autograd.Variable(cls.cuda()) \n",
        "\n",
        "                pred = my_model(mymel)\n",
        "                _, predicted = torch.max(pred, 1)\n",
        "                total_val += cls.size(0)\n",
        "                correct_val += (predicted == cls).sum().item()\n",
        "        accu_val = 100 * correct_val / total_val\n",
        "        toc_val = time.time()\n",
        "        dur_val = toc_val - tic_val\n",
        "        print(' | Accu_val : {:4.2f}% {:6.4f}sec'.format(accu_val, dur_val ) )\n",
        "        if (e % epoch_save ==0 ) :\n",
        "            PATH = os.path.join( ch_dir,'model_{:03d}.pt'.format(e) )\n",
        "\n",
        "            #torch.save(my_model.state_dict(), PATH)\n",
        "            save_checkpoint(my_model, optimizer, learning_rate, e, PATH)\n",
        "\n",
        "            print('checkpoin saved {} with Accu_val  {:4.2f}% '.format(PATH, accu_val ))\n",
        "\n",
        "def main(config,  ch_dir ):\n",
        "    train(config=config,  ch_dir=ch_dir )\n",
        "\n",
        "\n",
        "\n",
        "if __name__ ==\"__main__\":\n",
        "    import argparse\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('-c', '--config',   type=str, default='/home/hryu/config10.json')  \n",
        "    parser.add_argument('-d', '--ch_dir', type=str, default='/Protein/2020/ch_01')       \n",
        "    args = parser.parse_args()\n",
        "    print(args)\n",
        "    \n",
        "    main(args.config,   args.ch_dir  )\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}