{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "yhIlh9JbvXxL",
        "khKyDPa96rue",
        "BeusyeyL0CZi",
        "E4BqRymM276h",
        "z3ZI2Ky15anx",
        "IQth2eLC7a17",
        "MFGXlvsk7mAN",
        "-mLqBfG987hZ",
        "8Jvtrp829Mha",
        "kYRjGg0v-Ghn",
        "cSB-UEwhsBKr",
        "59FqEtfa3pQ9",
        "dxezmp_BIkr_",
        "3Tqlzxf4K20r",
        "zwZjniFJN-oh",
        "hV00cu7nILuH"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# GPU Bootcamp for OpenACC/OpenMP\n",
        "\n",
        "Welcome to the KSC 22 GPU bootcamp! This notebook will introduce you Nways to Accelerate GPU.\n",
        "\n",
        "<img src=\"https://www.nvidia.com/content/dam/en-zz/Solutions/about-nvidia/logo-and-brand/02-nvidia-logo-color-wht-500x200-4c25-d@2x.png\" height=\"100\"></td></tr>\n",
        "</table>\n",
        "\n"
      ],
      "metadata": {
        "id": "lDVAFF63zo-2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DevOps - install NVIDIA HPC SDK\n",
        " "
      ],
      "metadata": {
        "id": "YTh-f9nn8yJi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### step1. enabling GPU Support and check CUDA driva and CUDA toolkit in Google Colab. \n",
        "\n",
        "To use GPU resources through Colab, change the runtime to GPU:\n",
        "\n",
        " 1. From the \"Runtime\" menu select \"Change Runtime Type\"\n",
        " 2. Choose \"GPU\" from the drop-down menu\n",
        " 3. Click \"SAVE\"\n",
        "This will reset the notebook and probably ask you if you are a robot (these instructions assume you are not). Running\n",
        "\n",
        "`!nvidia-smi`\n",
        "\n",
        "in a cell will verify this has worked and show you what kind of hardware you have access to."
      ],
      "metadata": {
        "id": "y4bJM7ZMzotH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-V495jxzhTc",
        "outputId": "c3c14666-f505-47f2-8dd4-aa9bbb8d2e5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAME=\"Ubuntu\"\n",
            "VERSION=\"18.04.6 LTS (Bionic Beaver)\"\n",
            "ID=ubuntu\n",
            "ID_LIKE=debian\n",
            "PRETTY_NAME=\"Ubuntu 18.04.6 LTS\"\n",
            "VERSION_ID=\"18.04\"\n",
            "HOME_URL=\"https://www.ubuntu.com/\"\n",
            "SUPPORT_URL=\"https://help.ubuntu.com/\"\n",
            "BUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\n",
            "PRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\n",
            "VERSION_CODENAME=bionic\n",
            "UBUNTU_CODENAME=bionic\n",
            "Tue Sep 20 04:06:38 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0    24W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "Built on Mon_Oct_12_20:09:46_PDT_2020\n",
            "Cuda compilation tools, release 11.1, V11.1.105\n",
            "Build cuda_11.1.TC455_06.29190527_0\n",
            "/usr/local/cuda/bin/nvcc\n"
          ]
        }
      ],
      "source": [
        "# OS version. Probably Ubuntu 18.04 LTS.\n",
        "! cat /etc/os-release\n",
        "\n",
        "# GPU information\n",
        "!nvidia-smi\n",
        "!nvcc --version\n",
        "!which nvcc"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### step2. install NVIDIA HPC SDK \n",
        "\n",
        " Let's install GPU compilers to the virtual machine. Following https://developer.nvidia.com/nvidia-hpc-sdk-downloads, we download and install debian packages. It will take some time( 7 minutes). It depends on network speed.\n",
        "\n",
        "as you see in nvidia-smi log, \n",
        " - 460.32.03 CUDA driver for CUDA toolkit 11.2 was installed in Google Colab.\n",
        " - CUDA toolkit 11.1 was installed in colab\n",
        "\n",
        "So we will uninstall old cuda toolkit and install NVIDIA HPC SDK 22-7 with CUDA version 11.7. It takes 1 minutes to uninstall and 6 minutes to install NVIDIA HPC SDK 22.7 with CUDA toolkit 11.7.\n",
        "\n",
        "<b> Caution !!! - after restarting the colab, you need to re-install the SDK because factory reset. It will take 7 minutes. </b>\n",
        "\n",
        "for KISTI Neuron user,\n",
        "you can install NVIDIA HPC SDK on Neuron system with below script\n",
        "\n",
        "```\n",
        "$ mkdir -p /scratch/$USER/temp\n",
        "$ module load singularity/3.9.7\n",
        "$ export SINGULARITY_CACHEDIR=/scratch/$USER/temp\n",
        "$ cd /scratch/$USER\n",
        "$ singularity build --sandbox nvhpc_2203_116_centos7.sid docker://nvcr.io/nvidia/nvhpc:22.3-devel-cuda11.6-centos7\n",
        "$ cd /scratch/$USER/workdir\n",
        "$ singularity run --nv /scratch/$USER/nvhpc_2203_116_centos7.sid bash \n",
        "```"
      ],
      "metadata": {
        "id": "ZSBVH_yDdScI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time \n",
        "# remove cuda toolkit\n",
        "!echo \"remove nvidia modules\"\n",
        "!time apt autoremove nvidia-*  > /etc/null\n",
        "!echo \"remove cuda toolkit\"\n",
        "!time apt autoremove cuda-*   > /etc/null\n",
        "# install NVIDIA HPC SDK\n",
        "!echo \"update CUDA repository\"\n",
        "! echo 'deb [trusted=yes] https://developer.download.nvidia.com/hpc-sdk/ubuntu/amd64 /' | sudo tee /etc/apt/sources.list.d/nvhpc.list\n",
        "!time apt-get update -y > /etc/null \n",
        "!echo \"install NVIDIA HPC SDK\"\n",
        "!time apt-get install -y nvhpc-22-7 \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nkwp_6-7VRDp",
        "outputId": "41897e24-5057-48e9-d8e8-170e94e8e649"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "remove nvidia modules\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "\n",
            "real\t0m26.679s\n",
            "user\t0m4.308s\n",
            "sys\t0m5.443s\n",
            "remove cuda toolkit\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "\n",
            "real\t0m22.064s\n",
            "user\t0m13.461s\n",
            "sys\t0m3.054s\n",
            "update CUDA repository\n",
            "deb [trusted=yes] https://developer.download.nvidia.com/hpc-sdk/ubuntu/amd64 /\n",
            "W: An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY A4B469963BF863CC\n",
            "W: Failed to fetch https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/InRelease  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY A4B469963BF863CC\n",
            "W: Some index files failed to download. They have been ignored, or old ones used instead.\n",
            "\n",
            "real\t0m7.591s\n",
            "user\t0m4.027s\n",
            "sys\t0m1.509s\n",
            "install NVIDIA HPC SDK\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  nvhpc-2022\n",
            "The following NEW packages will be installed:\n",
            "  nvhpc-2022 nvhpc-22-7\n",
            "0 upgraded, 2 newly installed, 0 to remove and 31 not upgraded.\n",
            "Need to get 2,824 MB of archives.\n",
            "After this operation, 10.3 GB of additional disk space will be used.\n",
            "Get:1 https://developer.download.nvidia.com/hpc-sdk/ubuntu/amd64  nvhpc-22-7 22.7 [2,824 MB]\n",
            "Get:2 https://developer.download.nvidia.com/hpc-sdk/ubuntu/amd64  nvhpc-2022 22.7 [1,198 B]\n",
            "Fetched 2,824 MB in 45s (63.4 MB/s)\n",
            "Selecting previously unselected package nvhpc-22-7.\n",
            "(Reading database ... 111889 files and directories currently installed.)\n",
            "Preparing to unpack .../nvhpc-22-7_22.7_amd64.deb ...\n",
            "Unpacking nvhpc-22-7 (22.7) ...\n",
            "Selecting previously unselected package nvhpc-2022.\n",
            "Preparing to unpack .../nvhpc-2022_22.7_amd64.deb ...\n",
            "Unpacking nvhpc-2022 (22.7) ...\n",
            "Setting up nvhpc-2022 (22.7) ...\n",
            "Setting up nvhpc-22-7 (22.7) ...\n",
            "\n",
            "real\t5m31.679s\n",
            "user\t5m9.546s\n",
            "sys\t0m52.293s\n",
            "CPU times: user 2.76 s, sys: 394 ms, total: 3.15 s\n",
            "Wall time: 6min 28s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### step3. configure environment variables\n",
        "We need to configure  **PATH** and **LD_LIBRARY_PATH** variables. Thease files was installed as below : \n",
        " - compiler binary : `/opt/nvidia/hpc_sdk/Linux_x86_64/2022/compilers/bin/` \n",
        " - HPC SDK library : `/opt/nvidia/hpc_sdk/Linux_x86_64/2022/compilers/lib`\n",
        " - CUDA library : `/opt/nvidia/hpc_sdk/Linux_x86_64/22.7/cuda/11.7/lib64`\n",
        " . \n"
      ],
      "metadata": {
        "id": "Bl6js_1ag9C6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiler version\n",
        "! /opt/nvidia/hpc_sdk/Linux_x86_64/2022/compilers/bin/nvcc --version\n",
        "! /opt/nvidia/hpc_sdk/Linux_x86_64/2022/compilers/bin/nvfortran --version\n",
        "! /opt/nvidia/hpc_sdk/Linux_x86_64/2022/compilers/bin/nvc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGX5fSK6BJ9M",
        "outputId": "9261bdae-e4ce-4f0f-b3ff-1407e93ed549"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Tue_May__3_18:49:52_PDT_2022\n",
            "Cuda compilation tools, release 11.7, V11.7.64\n",
            "Build cuda_11.7.r11.7/compiler.31294372_0\n",
            "\n",
            "nvfortran 22.7-0 64-bit target on x86-64 Linux -tp skylake-avx512 \n",
            "NVIDIA Compilers and Tools\n",
            "Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n",
            "\n",
            "nvc 22.7-0 64-bit target on x86-64 Linux -tp skylake-avx512 \n",
            "NVIDIA Compilers and Tools\n",
            "Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### check and configure cuda toolkit\n",
        "\n",
        " We need to configure right location for CUDA and NVIDIA HPC SDK libraries and binary path."
      ],
      "metadata": {
        "id": "UaP-CSD79uof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# default for colab\n",
        "!echo $LD_LIBRARY_PATH\n",
        "!echo $PATH"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1hRERzMARSS",
        "outputId": "907bc66c-8d51-44e8-d5fa-9545077aece8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib64-nvidia\n",
            "/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%env LD_LIBRARY_PATH=/opt/nvidia/hpc_sdk/Linux_x86_64/2022/compilers/lib:/opt/nvidia/hpc_sdk/Linux_x86_64/22.7/cuda/11.7/lib64:/usr/lib64-nvidia:/usr/lib\t\n",
        "%env PATH=/opt/bin:/opt/nvidia/hpc_sdk/Linux_x86_64/2022/compilers/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0p9BxQokBFKK",
        "outputId": "765040b1-6433-4874-9452-541cf32e8d15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: LD_LIBRARY_PATH=/opt/nvidia/hpc_sdk/Linux_x86_64/2022/compilers/lib:/opt/nvidia/hpc_sdk/Linux_x86_64/22.7/cuda/11.7/lib64:/usr/lib64-nvidia:/usr/lib\n",
            "env: PATH=/opt/bin:/opt/nvidia/hpc_sdk/Linux_x86_64/2022/compilers/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n",
        "!dmesg | grep NVRM\n",
        "!nvcc --version\n",
        "!nvfortran --version\n",
        "!nvc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbVv77_yASvJ",
        "outputId": "c0fdb4a2-4afc-49bd-e88c-c8e029ee2df1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Sep 20 04:26:26 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0    24W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "[   13.355075] NVRM: loading NVIDIA UNIX x86_64 Kernel Module  460.32.03  Sun Dec 27 19:00:34 UTC 2020\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Tue_May__3_18:49:52_PDT_2022\n",
            "Cuda compilation tools, release 11.7, V11.7.64\n",
            "Build cuda_11.7.r11.7/compiler.31294372_0\n",
            "\n",
            "nvfortran 22.7-0 64-bit target on x86-64 Linux -tp skylake-avx512 \n",
            "NVIDIA Compilers and Tools\n",
            "Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n",
            "\n",
            "nvc 22.7-0 64-bit target on x86-64 Linux -tp skylake-avx512 \n",
            "NVIDIA Compilers and Tools\n",
            "Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### step4. evalute toolkit"
      ],
      "metadata": {
        "id": "ju4yagtc-dZW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "current situation\n",
        "- cuda driver : 460.x ( cuda 11.2)\n",
        "- cuda toolkit : cuda toolkit 11.7  ( need 515.x)\n",
        "- nvidia HPC sdk : 22.7 for cuda toolkit 11.7 \n",
        "              \n",
        "https://docs.nvidia.com/deploy/cuda-compatibility/ "
      ],
      "metadata": {
        "id": "7AiE2K1LemPT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**compile simple C code**"
      ],
      "metadata": {
        "id": "6xVAaL7C-4-E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "pi_serial.c \n",
        " - `%%file ` command is save command in jupyter.\n",
        " - simple pi calculation with integral \n",
        "\n",
        "<center>\n",
        "$  \\displaystyle  \\frac{\\pi} {4} =\\arctan(1) =   \\int\\limits_0^1 \\! \\frac{1}{1+ x^{2}}\\, dx  $  \n",
        "</center>"
      ],
      "metadata": {
        "id": "914lPpy4mLjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file pi_serial.c\n",
        "\n",
        "#include <stdio.h> \n",
        " \n",
        "main() {\n",
        "\n",
        " double pi = 0.0; \n",
        " long i;\n",
        " long N = 4000000000 ; \n",
        "\n",
        " for (i=0; i<N; i++) {\n",
        "\t double t = (double)((i+0.05)/N);\n",
        "\t pi += 4.0/(1.0+t*t);\n",
        " }\n",
        " printf(\"pi = %f with N %d \\n\", pi/N, N);\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaPc5jsN-5Qd",
        "outputId": "a01f2ae9-711e-45a0-aa4b-5109418d0aaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing pi_serial.c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvc pi_serial.c -o a.out_serial && time  ./a.out_serial"
      ],
      "metadata": {
        "id": "isUV7F2j-5Ta",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40d93ce3-28fd-400c-d115-5449309a9894"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pi = 3.141593 with N -294967296 \n",
            "\n",
            "real\t0m12.414s\n",
            "user\t0m12.345s\n",
            "sys\t0m0.010s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "simple GPU acceleration with OpenACC"
      ],
      "metadata": {
        "id": "DvLwVusgq0CA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file pi_acc.c\n",
        "\n",
        "#include <stdio.h> \n",
        "\n",
        "main() {\n",
        "\n",
        " double pi = 0.0; \n",
        " long i;\n",
        " long N = 400000000 ; \n",
        "\n",
        "#pragma acc data copy(pi)\n",
        "#pragma acc parallel\n",
        "#pragma acc for \n",
        " for (i=0; i<N; i++) {\n",
        "\t double t = (double)((i+0.05)/N);\n",
        "\t pi += 4.0/(1.0+t*t);\n",
        " }\n",
        " printf(\"pi = %f with N %d \\n\", pi/N, N);\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4VhhA5H-5ZY",
        "outputId": "946970ed-1c8b-4065-a731-ad78fc0ccc61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing pi_acc.c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvc -acc -Minfo=accel  -ta=tesla pi_acc.c -o a.out_acc && time  ./a.out_acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rAsP6Jw-5cY",
        "outputId": "5287c68d-c1bc-4dd9-e566-a59a2e3a1709"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main:\n",
            "     13, Generating copy(pi) [if not already present]\n",
            "         Loop is parallelizable\n",
            "         Generating NVIDIA GPU code\n",
            "         13, #pragma acc loop vector(128) /* threadIdx.x */\n",
            "             Generating implicit reduction(+:pi)\n",
            "pi = 3.141593 with N 400000000 \n",
            "\n",
            "real\t0m0.791s\n",
            "user\t0m0.585s\n",
            "sys\t0m0.166s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "compare result : \n",
        " - CPU : 12 sec  single core(virtual) : \n",
        " - GPU : 0.5 sec ( depends on colab GPU)"
      ],
      "metadata": {
        "id": "4aWn2jW9q5-y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 1  OpenACC\n",
        "\n",
        "<img src=\"https://www.nvidia.com/content/dam/en-zz/Solutions/about-nvidia/logo-and-brand/02-nvidia-logo-color-wht-500x200-4c25-d@2x.png\" height=\"100\"></td></tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "5PqgpeG_rlZS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OpenACC in Fortran\n",
        "\n",
        "\n",
        "Let's execute the cell below to display information about the GPUs running on the server by running the nvaccelinfo command, which ships with the NVIDIA HPC compiler that we will be using. To do this, execute the cell block below by giving it focus (clicking on it with your mouse), and hitting Ctrl-Enter, or pressing the play button in the toolbar above. If all goes well, you should see some output returned below the grey cell."
      ],
      "metadata": {
        "id": "yhIlh9JbvXxL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvaccelinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "884zOZFZveC1",
        "outputId": "613ba4b0-3870-4ef1-ee14-85fd1442acb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CUDA Driver Version:           11020\n",
            "NVRM version:                  NVIDIA UNIX x86_64 Kernel Module  460.32.03  Sun Dec 27 19:00:34 UTC 2020\n",
            "\n",
            "Device Number:                 0\n",
            "Device Name:                   Tesla V100-SXM2-16GB\n",
            "Device Revision Number:        7.0\n",
            "Global Memory Size:            16945512448\n",
            "Number of Multiprocessors:     80\n",
            "Concurrent Copy and Execution: Yes\n",
            "Total Constant Memory:         65536\n",
            "Total Shared Memory per Block: 49152\n",
            "Registers per Block:           65536\n",
            "Warp Size:                     32\n",
            "Maximum Threads per Block:     1024\n",
            "Maximum Block Dimensions:      1024, 1024, 64\n",
            "Maximum Grid Dimensions:       2147483647 x 65535 x 65535\n",
            "Maximum Memory Pitch:          2147483647B\n",
            "Texture Alignment:             512B\n",
            "Clock Rate:                    1530 MHz\n",
            "Execution Timeout:             No\n",
            "Integrated Device:             No\n",
            "Can Map Host Memory:           Yes\n",
            "Compute Mode:                  default\n",
            "Concurrent Kernels:            Yes\n",
            "ECC Enabled:                   Yes\n",
            "Memory Clock Rate:             877 MHz\n",
            "Memory Bus Width:              4096 bits\n",
            "L2 Cache Size:                 6291456 bytes\n",
            "Max Threads Per SMP:           2048\n",
            "Async Engines:                 2\n",
            "Unified Addressing:            Yes\n",
            "Managed Memory:                Yes\n",
            "Concurrent Managed Memory:     Yes\n",
            "Preemption Supported:          Yes\n",
            "Cooperative Launch:            Yes\n",
            "  Multi-Device:                Yes\n",
            "Default Target:                cc70\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This gives us lots of details about the GPU, for instance the device number, the type of device, and at the very bottom the command line argument we should use when targeting this GPU (see NVIDIA HPC Compiler Option). We will use this command line option a bit later to build for our GPU."
      ],
      "metadata": {
        "id": "W5_DZjD5vlzi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "Our goal for this lab is to learn the first steps in accelerating an application with OpenACC. We advocate the following 3-step development cycle for OpenACC.\n",
        "\n",
        "<center> OpenACC development cycle\n",
        "\n",
        "<img src=\"https://github.com/openhackathons-org/gpubootcamp/raw/5c077a6c70989492f4bd850240fdcaf8b16fd555/hpc/openacc/English/Fortran/jupyter_notebook/images/development-cycle.png\" width=600> </center>\n",
        "\n",
        "- **Analyze** your code to determine most likely places needing parallelization or optimization.\n",
        "\n",
        "- **Parallelize** your code by starting with the most time consuming parts and check for correctness.\n",
        "\n",
        "- **Optimize** your code to improve observed speed-up from parallelization.\n",
        "\n",
        "One should generally start the process at the top with the analyze step. For complex applications, it's useful to have a profiling tool available to learn where your application is spending its execution time and to focus your efforts there. The NVIDIA HPC Software Development Kit (SDK) comes with NVIDIA Nsight Systems/Compute for interactive HPC applications performance profiler. Since our example code is quite a bit simpler than a full application, we'll skip profiling the code and simply analyze the code by reading it."
      ],
      "metadata": {
        "id": "OvLag9DBvpVX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Guide for the LAB\n",
        "\n",
        "- For each lab, we will make sub directories. \n",
        "\n",
        "- We will use cell magic `%%file filename` to save source file. \n",
        "- you can save the file you can edit source code in the cell.  \n",
        "- after writing, you can browse file in file expolore "
      ],
      "metadata": {
        "id": "nyBHzu4dGZCY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 1 "
      ],
      "metadata": {
        "id": "rRycdZZQ6l8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/lab01"
      ],
      "metadata": {
        "id": "fS9p-HwUyxsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/lab01"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qMAqBvQy0if",
        "outputId": "4738fe9f-2b21-4507-a2bf-faec10249c48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/lab01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### prepare lab1 in colab\n",
        "\n",
        "#### baseline code and makefile\n",
        " - lab01/Makefile\n",
        " - lab01/jacobi.f90  : main program \n",
        " - lab01/laplace2d.f90 : subroutines \n"
      ],
      "metadata": {
        "id": "khKyDPa96rue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file Makefile\n",
        "FC := nvfortran\n",
        "ACCFLAGS_1 := -fast\n",
        "ACCFLAGS_2 := -fast -acc -ta=multicore -Minfo=accel\n",
        "ACCFLAGS_3 := -fast -acc -ta=tesla:managed -Minfo=accel\n",
        "\n",
        "laplace_serial: laplace2d.f90 jacobi.f90 \n",
        "\t${FC} ${ACCFLAGS_1} -o laplace_serial laplace2d.f90 jacobi.f90\n",
        "\n",
        "laplace_multicore: laplace2d.f90 jacobi.f90 \n",
        "\t${FC} ${ACCFLAGS_2} -o laplace_multicore laplace2d.f90 jacobi.f90 \n",
        "\n",
        "laplace_gpu: laplace2d.f90 jacobi.f90 \n",
        "\t${FC} ${ACCFLAGS_3} -o laplace_gpu laplace2d.f90 jacobi.f90 \n",
        "\n",
        "clean:\n",
        "\trm -f *.o laplace_serial laplace_multicore laplace_gpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-L8a9siCy6wI",
        "outputId": "074e301f-2a1e-49a3-c3b6-3cc6c9052c24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting Makefile\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file jacobi.f90\n",
        "\n",
        "program jacobi\n",
        "  use laplace2d\n",
        "  implicit none\n",
        "  integer, parameter :: fp_kind=kind(1.0d0)\n",
        "  integer, parameter :: n=4096, m=4096, iter_max=1000\n",
        "  integer :: i, j, iter\n",
        "  real(fp_kind), dimension (:,:), allocatable :: A, Anew\n",
        "  real(fp_kind) :: tol=1.0e-6_fp_kind, error=1.0_fp_kind\n",
        "  real(fp_kind) :: start_time, stop_time\n",
        "\n",
        "  call initialize(A, Anew, m, n)\n",
        "   \n",
        "  write(*,'(a,i5,a,i5,a)') 'Jacobi relaxation Calculation:', n, ' x', m, ' mesh'\n",
        " \n",
        "  call cpu_time(start_time) \n",
        "\n",
        "  iter=0\n",
        "  \n",
        "  do while ( error .gt. tol .and. iter .lt. iter_max )\n",
        "\n",
        "    error = calcNext(A, Anew, m, n)\n",
        "    call swap(A, Anew, m, n)\n",
        "\n",
        "    if(mod(iter,100).eq.0 ) write(*,'(i5,f10.6)'), iter, error\n",
        "\n",
        "    iter = iter + 1\n",
        "\n",
        "  end do\n",
        "\n",
        "  call cpu_time(stop_time) \n",
        "  write(*,'(a,f10.3,a)')  ' completed in ', stop_time-start_time, ' seconds'\n",
        "\n",
        "  deallocate (A,Anew)\n",
        "end program jacobi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWaQckPwwObH",
        "outputId": "dfa9615f-4c71-45c1-e69a-1e85cec399e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting jacobi.f90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file laplace2d.f90\n",
        "\n",
        "module laplace2d\n",
        "  public :: initialize\n",
        "  public :: calcNext\n",
        "  public :: swap\n",
        "  public :: dealloc\n",
        "  contains\n",
        "    subroutine initialize(A, Anew, m, n)\n",
        "      integer, parameter :: fp_kind=kind(1.0d0)\n",
        "      real(fp_kind),allocatable,intent(out)   :: A(:,:)\n",
        "      real(fp_kind),allocatable,intent(out)   :: Anew(:,:)\n",
        "      integer,intent(in)          :: m, n\n",
        "\n",
        "      allocate ( A(0:n-1,0:m-1), Anew(0:n-1,0:m-1) )\n",
        "\n",
        "      A    = 0.0_fp_kind\n",
        "      Anew = 0.0_fp_kind\n",
        "\n",
        "      A(0,:)    = 1.0_fp_kind\n",
        "      Anew(0,:) = 1.0_fp_kind\n",
        "    end subroutine initialize\n",
        "\n",
        "    function calcNext(A, Anew, m, n)\n",
        "      integer, parameter          :: fp_kind=kind(1.0d0)\n",
        "      real(fp_kind),intent(inout) :: A(0:n-1,0:m-1)\n",
        "      real(fp_kind),intent(inout) :: Anew(0:n-1,0:m-1)\n",
        "      integer,intent(in)          :: m, n\n",
        "      integer                     :: i, j\n",
        "      real(fp_kind)               :: error\n",
        "\n",
        "      error=0.0_fp_kind\n",
        "\n",
        "      do j=1,m-2\n",
        "        do i=1,n-2\n",
        "          Anew(i,j) = 0.25_fp_kind * ( A(i+1,j  ) + A(i-1,j  ) + &\n",
        "                                       A(i  ,j-1) + A(i  ,j+1) )\n",
        "          error = max( error, abs(Anew(i,j)-A(i,j)) )\n",
        "        end do\n",
        "      end do\n",
        "      calcNext = error\n",
        "    end function calcNext\n",
        "\n",
        "    subroutine swap(A, Anew, m, n)\n",
        "      integer, parameter        :: fp_kind=kind(1.0d0)\n",
        "      real(fp_kind),intent(out) :: A(0:n-1,0:m-1)\n",
        "      real(fp_kind),intent(in)  :: Anew(0:n-1,0:m-1)\n",
        "      integer,intent(in)        :: m, n\n",
        "      integer                   :: i, j\n",
        "\n",
        "      do j=1,m-2\n",
        "        do i=1,n-2\n",
        "          A(i,j) = Anew(i,j)\n",
        "        end do\n",
        "      end do\n",
        "    end subroutine swap\n",
        "\n",
        "    subroutine dealloc(A, Anew)\n",
        "      integer, parameter :: fp_kind=kind(1.0d0)\n",
        "      real(fp_kind),allocatable,intent(in) :: A\n",
        "      real(fp_kind),allocatable,intent(in) :: Anew\n",
        "\n",
        "      deallocate (A,Anew)\n",
        "    end subroutine\n",
        "end module laplace2d"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZYzyXo9wWx8",
        "outputId": "1512ab62-5bcc-45ba-9155-a7a76472b0ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting laplace2d.f90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analyze the Code\n",
        "In the section below you will learn about the algorithm implemented in the example code and see examples pulled out of the source code. If you want a sneak peek at the source code, you can take a look at the files linked below or open the downloaded file."
      ],
      "metadata": {
        "id": "sydaRw5h6wCN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# code Description\n",
        "The code simulates heat distribution across a 2-dimensional metal plate. In the beginning, the plate will be unheated, meaning that the entire plate will be room temperature. A constant heat will be applied to the edge of the plate and the code will simulate that heat distributing across the plate over time.\n",
        "\n",
        "This is a visual representation of the plate before the simulation starts(initial condition):\n",
        "<img src=\"https://github.com/openhackathons-org/gpubootcamp/raw/5c077a6c70989492f4bd850240fdcaf8b16fd555/hpc/openacc/English/Fortran/jupyter_notebook/images/plate1.png\" width=600>\n"
      ],
      "metadata": {
        "id": "GOLnPi9vwiqy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We can see that the plate is uniformly room temperature, except for the top edge. Within the laplace2d.f90 file, we see a function called `initialize`. This function is what **\"heats\"** the top edge of the plate.\n",
        "```fortran\n",
        "subroutine initialize(A, Anew, m, n)\n",
        "      integer, parameter :: fp_kind=kind(1.0d0)\n",
        "      real(fp_kind),allocatable,intent(out)   :: A(:,:)\n",
        "      real(fp_kind),allocatable,intent(out)   :: Anew(:,:)\n",
        "      integer,intent(in)          :: m, n\n",
        "\n",
        "      allocate ( A(0:n-1,0:m-1), Anew(0:n-1,0:m-1) )\n",
        "\n",
        "      A    = 0.0_fp_kind\n",
        "      Anew = 0.0_fp_kind\n",
        "\n",
        "      A(0,:)    = 1.0_fp_kind\n",
        "      Anew(0,:) = 1.0_fp_kind\n",
        "    end subroutine initialize\n",
        "```\n",
        "\n",
        "After the top edge is heated, the code will simulate the heat distributing across the length of the plate. We will keep the top edge at a constant heat as the simulation progresses.\n"
      ],
      "metadata": {
        "id": "0kbkSE5dxLM4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This is the plate after several iterations of our simulation:\n",
        "\n",
        "<img src=\"https://github.com/openhackathons-org/gpubootcamp/raw/5c077a6c70989492f4bd850240fdcaf8b16fd555/hpc/openacc/English/Fortran/jupyter_notebook/images/plate2.png\" width=600>\n",
        "\n",
        "That's the theory: simple heat distribution. However, we are more interested in how the code works."
      ],
      "metadata": {
        "id": "L2AtwhUpxNkI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Breakdown\n",
        "The 2-dimensional plate is represented by a 2-dimensional array containing double-precision floating point values. These doubles represent temperature; 0.0 is room temperature, and 1.0 is our max temperature. The 2-dimensional plate has two states, one represents the current temperature, and one represents the expected temperature values at the next step in our simulation. These two states are represented by arrays **`A`** and **`Anew`** respectively. The following is a visual representation of these arrays, with the top edge \"heated\"."
      ],
      "metadata": {
        "id": "dZYJWUCNxjGV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simulating this state in two arrays is very important for our calcNext function. Our calcNext is essentially our \"simulate\" function. **`calcNext`** will look at the inner elements of A (meaning everything except for the edges of the plate) and update each elements temperature based on the temperature of its neighbors. If we attempted to calculate in-place (using only **`A`**), then each element would calculate its new temperature based on the updated temperature of previous elements. This data dependency not only prevents parallelizing the code, but would also result in incorrect results when run in serial. By calculating into the temporary array **`Anew`** we ensure that an entire step of our simulation has completed before updating the A array."
      ],
      "metadata": {
        "id": "wpmu_n_gxxE7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This is the **`calcNext`** function:\n",
        "\n",
        "```fortran\n",
        "01 function calcNext(A, Anew, m, n)\n",
        "02   integer, parameter          :: fp_kind=kind(1.0d0)\n",
        "03   real(fp_kind),intent(inout) :: A(0:n-1,0:m-1)\n",
        "04   real(fp_kind),intent(inout) :: Anew(0:n-1,0:m-1)\n",
        "05   integer,intent(in)          :: m, n\n",
        "06   integer                     :: i, j\n",
        "07   real(fp_kind)               :: error\n",
        "08    \n",
        "09   error=0.0_fp_kind\n",
        "10    \n",
        "11   do j=1,m-2\n",
        "12     do i=1,n-2\n",
        "13        Anew(i,j) = 0.25_fp_kind * ( A(i+1,j  ) + A(i-1,j  ) + &\n",
        "14                                     A(i  ,j-1) + A(i  ,j+1) )\n",
        "15        error = max( error, abs(Anew(i,j)-A(i,j)) )\n",
        "16     end do\n",
        "17   end do\n",
        "18   calcNext = error\n",
        "19 end function calcNext\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "uSV8fdgbyAXg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see on lines 13 and 14 where we are calculating the value of **`Anew`** at**` i,j`** by averaging the current values of its neighbors. Line 15 is where we calculate the current rate of change for the simulation by looking at how much the **`i,j`** element changed during this step and finding the maximum value for this **`error`**. This allows us to short-circuit our simulation if it reaches a steady state before we've completed our maximum number of iterations.\n",
        "\n",
        "Lastly, our **`swap`** subroutine will copy the contents of **`Ane`**w to **`A`**."
      ],
      "metadata": {
        "id": "phZwPo3EyKG1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```fortran\n",
        "01 subroutine swap(A, Anew, m, n)\n",
        "02   integer, parameter        :: fp_kind=kind(1.0d0)\n",
        "03   real(fp_kind),intent(out) :: A(0:n-1,0:m-1)\n",
        "04   real(fp_kind),intent(in)  :: Anew(0:n-1,0:m-1)\n",
        "05   integer,intent(in)        :: m, n\n",
        "06   integer                   :: i, j\n",
        "07 \n",
        "08   do j=1,m-2\n",
        "09     do i=1,n-2\n",
        "10       A(i,j) = Anew(i,j)\n",
        "11     end do\n",
        "12   end do\n",
        "13 end subroutine swap\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "m95PVu3ByZvp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the Code\n",
        "Now that we've seen what the code does, let's build and run it. We need to record the results of our program before making any changes so that we can compare them to the results from the parallel code later on. It is also important to record the time that the program takes to run, as this will be our primary indicator to whether or not our parallelization is improving performance.\n",
        "\n",
        "# Compiling the Code with NVIDIA HPC\n",
        "For this lab we are using the NVIDIA HPC compiler to compiler our code. You will not need to memorize the compiler commands to complete this lab, however, they will be helpful to know if you want to parallelize your own personal code with OpenACC.\n",
        "\n",
        "nvc : this is the command to compile C code\n",
        "nvc++ : this is the command to compile C++ code\n",
        "nvfortran : this is the command to compile Fortran code\n",
        "-fast : this compiler flag instructs the compiler to use what it believes are the best possible optimizations for our system"
      ],
      "metadata": {
        "id": "ja4T8WTEyfYk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "```\n",
        "FC := nvfortran\n",
        "ACCFLAGS_1 := -fast\n",
        "\n",
        "laplace_serial: laplace2d.f90 jacobi.f90 \n",
        "\t${FC} ${ACCFLAGS_1} -o laplace_serial laplace2d.f90 jacobi.f90\n"
      ],
      "metadata": {
        "id": "ykVtwshZLGDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!make clean && make laplace_serial && echo \"Compilation Successful!\" && ./laplace_serial"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdYTbGIbzBYn",
        "outputId": "40b90879-74bd-42a7-bbb8-befcb4b0717d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm -f *.o laplace_serial laplace_multicore laplace_gpu\n",
            "nvfortran -fast -o laplace_serial laplace2d.f90 jacobi.f90\n",
            "laplace2d.f90:\n",
            "jacobi.f90:\n",
            "Compilation Successful!\n",
            "Jacobi relaxation Calculation: 4096 x 4096 mesh\n",
            "    0  0.250000\n",
            "  100  0.002397\n",
            "  200  0.001204\n",
            "  300  0.000804\n",
            "  400  0.000603\n",
            "  500  0.000483\n",
            "  600  0.000403\n",
            "  700  0.000345\n",
            "  800  0.000302\n",
            "  900  0.000269\n",
            " completed in     50.510 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Expected Output\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "laplace2d.f90:\n",
        "jacobi.f90:\n",
        "Compilation Successful!\n",
        "Jacobi relaxation Calculation: 4096 x 4096 mesh\n",
        "    0  0.250000\n",
        "  100  0.002397\n",
        "  200  0.001204\n",
        "  300  0.000804\n",
        "  400  0.000603\n",
        "  500  0.000483\n",
        "  600  0.000403\n",
        "  700  0.000345\n",
        "  800  0.000302\n",
        "  900  0.000269\n",
        " completed in     58.397 seconds\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "HGPsbCgXzJi4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding Code Results\n",
        "We see from the output that onces every hundred steps the program outputs the value of **`error`**, which is the maximum rate of change among the cells in our array. If these outputs change during any point while we parallelize our code, we know we've made a mistake. For simplicity, focus on the last output, which occurred at iteration 900 (the error value is 0.000269). It is also helpful to record the time the program took to run (it should have been about a minute). Our goal while parallelizing the code is ultimately to make it faster, so we need to know our \"base runtime\" in order to know if the code is running faster. Keep in mind that if you run the code multiple times you may get slightly different total runtimes, but you should get the same values for the error rates.\n",
        "\n",
        "# Parallelizing Loops with OpenACC\n",
        "At this point we know that most of the work done in our code happens in the **`calcNext`** and **`swap`** routines, so we'll focus our efforts there. We want the compiler to parallelize the loops in those two routines because that will give up the maximum speed-up over the baseline we just measured. To do this, we're going to use the OpenACC **` parallel loop`**  directive.\n",
        "\n"
      ],
      "metadata": {
        "id": "uWpMWjK7zT4Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OpenACC Directives\n",
        "Using OpenACC directives will allow us to parallelize our code without needing to explicitly alter our code. What this means is that, by using OpenACC directives, we can have a single code that will function as both a sequential code and a parallel code.\n",
        "\n",
        "## OpenACC Syntax\n",
        "**`!$acc <directive> <clauses>`**\n",
        "\n",
        "\"**`!$acc`**\" in Fortran is what's known as a \"compiler hint\" or \"compiler directive.\" These are very similar to programmer comments, since the line begins with a comment statement \"**`!`**\". After the comment is \"**`acc`**\". OpenACC compliant compilers with appropriate command line options can interpret this as an OpenACC directive that \"guide\" the compiler above and beyond what the programming language allows. If the compiler does not understand \"**`!$acc`**\" it can ignore it, rather than throw a syntax error because it's just a comment.\n",
        "\n",
        "**directives** are commands in OpenACC that will tell the compiler to do some action. For now, we will only use directives that allow the compiler to parallelize our code.\n",
        "\n",
        "**clauses** are additions/alterations to our directives. These include (but are not limited to) optimizations. The way that I prefer to think about it: directives describe a general action for our compiler to do (such as, paralellize our code), and clauses allow the programmer to be more specific (such as, how we specifically want the code to be parallelized).\n",
        "\n"
      ],
      "metadata": {
        "id": "kjaxM4ICzmAW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parallel and Loop Directives\n",
        "There are three directives we will cover in this lab: **`parallel`**, **`loop`**, and **`parallel loop`**. Once we understand all three of them, you will be tasked with parallelizing our laplace code with your preferred directive (or use all of them, if you'd like!)\n",
        "\n",
        "The parallel directive may be the most straight-forward of the directives. It will mark a region of the code for parallelization (this usually only includes parallelizing a single for loop.) Let's take a look:\n",
        "```\n",
        "!$acc parallel loop\n",
        "    do i=1,N\n",
        "        < loop code >\n",
        "    enddo\n",
        "```    \n",
        "We may also define a parallel region. The parallel region may have multiple loops (though this is often not recommended!) The parallel region is everything contained within the outer-most curly braces.\n",
        "```\n",
        "!$acc parallel\n",
        "    !$acc loop\n",
        "    do i=1,N\n",
        "        < loop code >\n",
        "    enddo\n",
        "!$acc end parallel    \n",
        "```    \n",
        "\n",
        "**`!$acc`** parallel loop will mark the next loop for parallelization. It is extremely important to include the loop, otherwise you will not be parallelizing the loop properly. The parallel directive tells the compiler to \"redundantly parallelize\" the code. The loop directive specifically tells the compiler that we want the loop parallelized. Let's look at an example of why the loop directive is so important. The **`parallel`** directive tells the compiler to create somewhere to run parallel code. OpenACC calls that somewhere a **`gang`**, which might be a thread on the CPU or maying a CUDA threadblock or OpenCL workgroup. It will choose how many gangs to create based on where you're running, only a few on a CPU (like 1 per CPU core) or lots on a GPU (1000's possibly). Gangs allow OpenACC code to scale from small CPUs to large GPUs because each one works completely independently of each other gang. That's why there's a space between gangs in the images below.\n"
      ],
      "metadata": {
        "id": "BeusyeyL0CZi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src=\"https://github.com/openhackathons-org/gpubootcamp/raw/5c077a6c70989492f4bd850240fdcaf8b16fd555/hpc/openacc/English/Fortran/jupyter_notebook/images/parallel1f.png\" width=600>\n",
        "\n",
        "<img src=\"https://github.com/openhackathons-org/gpubootcamp/raw/5c077a6c70989492f4bd850240fdcaf8b16fd555/hpc/openacc/English/Fortran/jupyter_notebook/images/parallel2f.png\" width=600>"
      ],
      "metadata": {
        "id": "6X27qoSX0iPu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There's a good chance that I don't want my loop to be run redundantly in every gang though, that seems wasteful and potentially dangerous. Instead I want to instruct the compiler to break up the iterations of my loop and to run them in parallel on the gangs. To do that, I simply add a loop directive to the interesting loops. This instructs the compiler that I want my loop to be parallelized and promises to the compiler that it's safe to do so. Now that I have both parallel and loop, things loop a lot better (and run a lot faster). Now the compiler is spreading my loop iterations to all of my gangs, but also running multiple iterations of the loop at the same time within each gang as a vector. Think of a vector like this, I have 10 numbers that I want to add to 10 other numbers (in pairs). Rather than looking up each pair of numbers, adding them together, storing the result, and then moving on to the next pair in-order, modern computer hardware allows me to add all 10 pairs together all at once, which is a lot more efficient."
      ],
      "metadata": {
        "id": "PrQgBnck0ptQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://github.com/openhackathons-org/gpubootcamp/raw/5c077a6c70989492f4bd850240fdcaf8b16fd555/hpc/openacc/English/Fortran/jupyter_notebook/images/parallel3f.png\" width=600>"
      ],
      "metadata": {
        "id": "JA0vB-yE0qeb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The acc parallel loop directive is both a promise and a request to the compiler. I as the programmer am promising that the loop can safely be parallelized and am requesting that the compiler do so in a way that makes sense for the machine I am targeting. The compiler may make completely different decisions if I'm compiling for a multicore CPU than it would for a GPU and that's the idea. OpenACC enables programmers to parallelize their codes without having to worry about the details of how best to do so for every possible machine.-"
      ],
      "metadata": {
        "id": "Vqa1wbc90uNV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Reduction Clause\n",
        "There's one very important clause that you'll need to know for our example code: the **`reduction`** clause. Take note of how the loops in **`calcNext`** each calculate an error value and then compare against the maximum value to find an absolute maximum. When executing this operation in parallel, it's necessary to do a reduction in order to ensure you always get the correct answer. A reduction takes all of the values of **`error`** calculated in the loops and reduces them down to a single answer, in this case the maximum. There are a variety of reductions that can be done, but for our example code we only care about the max operation. We will inform the compiler about our reduction by adding a **`reduction(max:error)`** clause to the**` acc parallel loop `**in the **`calcNext`** function."
      ],
      "metadata": {
        "id": "E4BqRymM276h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parallelize the Example Code\n",
        "At this point you have all of the tools you need to begin accelerating your application. The loops you will be parallelizing are in laplace2d.f90.   \n",
        "\n",
        "It is advisable to start with the **`calcNext`** routine and test your changes by compiling and running the code before moving on to the **`swap`** routine. OpenACC can be incrementally added to your application so that you can ensure each change is correct before getting too far along, which greatly simplifies debugging.\n",
        "\n",
        "Once you have made your changes, you can compile and run the application by running the cell below. Please note that our compiler options have changed a little bit, we've added the following two important flags:\n",
        "\n",
        " - **`-ta=multicore `**- This instructs the compiler to build your OpenACC loops and to target them to run across the cores of a multicore CPU.\n",
        "\n",
        "- **`-Minfo=accel`** - This instructs the compiler to give us some additional information about how it parallelized the code. We'll review how to interpret this feedback in just a moment.\n",
        "\n",
        "Go ahead and build and run the code, noting both the error value at the 900th iteration and the total runtime. If the error value changed, you may have made a mistake. Don't forget the**` reduction(max:error)`** clause on the loop in the **`calcNext`** function!"
      ],
      "metadata": {
        "id": "HCqorvwu2_Kr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file laplace2d.f90\n",
        "\n",
        "module laplace2d\n",
        "  public :: initialize\n",
        "  public :: calcNext\n",
        "  public :: swap\n",
        "  public :: dealloc\n",
        "  contains\n",
        "    subroutine initialize(A, Anew, m, n)\n",
        "      integer, parameter :: fp_kind=kind(1.0d0)\n",
        "      real(fp_kind),allocatable,intent(out)   :: A(:,:)\n",
        "      real(fp_kind),allocatable,intent(out)   :: Anew(:,:)\n",
        "      integer,intent(in)          :: m, n\n",
        "\n",
        "      allocate ( A(0:n-1,0:m-1), Anew(0:n-1,0:m-1) )\n",
        "\n",
        "      A    = 0.0_fp_kind\n",
        "      Anew = 0.0_fp_kind\n",
        "\n",
        "      A(0,:)    = 1.0_fp_kind\n",
        "      Anew(0,:) = 1.0_fp_kind\n",
        "    end subroutine initialize\n",
        "\n",
        "    function calcNext(A, Anew, m, n)\n",
        "      integer, parameter          :: fp_kind=kind(1.0d0)\n",
        "      real(fp_kind),intent(inout) :: A(0:n-1,0:m-1)\n",
        "      real(fp_kind),intent(inout) :: Anew(0:n-1,0:m-1)\n",
        "      integer,intent(in)          :: m, n\n",
        "      integer                     :: i, j\n",
        "      real(fp_kind)               :: error\n",
        "\n",
        "      error=0.0_fp_kind\n",
        "\n",
        "      ! ######### TODO  with reduction ################\n",
        "      !$acc parallel\n",
        "      !$acc loop\n",
        "\n",
        "      do j=1,m-2\n",
        "        do i=1,n-2\n",
        "          Anew(i,j) = 0.25_fp_kind * ( A(i+1,j  ) + A(i-1,j  ) + &\n",
        "                                       A(i  ,j-1) + A(i  ,j+1) )\n",
        "          error = max( error, abs(Anew(i,j)-A(i,j)) )\n",
        "        end do\n",
        "      end do\n",
        "    !$acc end parallel       \n",
        "      calcNext = error\n",
        "    end function calcNext\n",
        "\n",
        "    subroutine swap(A, Anew, m, n)\n",
        "      integer, parameter        :: fp_kind=kind(1.0d0)\n",
        "      real(fp_kind),intent(out) :: A(0:n-1,0:m-1)\n",
        "      real(fp_kind),intent(in)  :: Anew(0:n-1,0:m-1)\n",
        "      integer,intent(in)        :: m, n\n",
        "      integer                   :: i, j\n",
        "\n",
        "      ! ######### TODO  ################\n",
        "      !$acc parallel\n",
        "      !$acc loop      \n",
        "      do j=1,m-2\n",
        "        do i=1,n-2\n",
        "          A(i,j) = Anew(i,j)\n",
        "        end do\n",
        "      end do\n",
        "!$acc end parallel       \n",
        "    end subroutine swap\n",
        "\n",
        "    subroutine dealloc(A, Anew)\n",
        "      integer, parameter :: fp_kind=kind(1.0d0)\n",
        "      real(fp_kind),allocatable,intent(in) :: A\n",
        "      real(fp_kind),allocatable,intent(in) :: Anew\n",
        "\n",
        "      deallocate (A,Anew)\n",
        "    end subroutine\n",
        "end module laplace2d"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3q2lOWT3fr8",
        "outputId": "b99345e1-8593-4dd4-fffd-d097c2f140ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting laplace2d.f90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "\n",
        "FC := nvfortran\n",
        "\n",
        "ACCFLAGS_2 := -fast -acc -ta=multicore -Minfo=accel\n",
        "\n",
        "laplace_multicore: laplace2d.f90 jacobi.f90 \n",
        "\t${FC} ${ACCFLAGS_2} -o laplace_multicore laplace2d.f90 jacobi.f90 \n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "uRJ6zEMJLM47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! make clean && make laplace_multicore && echo \"Compilation Successful!\" && ./laplace_multicore"
      ],
      "metadata": {
        "id": "0bkv6bSn3pjg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7433e9f8-2b0b-4e67-f8dd-aaa1f907343c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm -f *.o laplace_serial laplace_multicore laplace_gpu\n",
            "nvfortran -fast -acc -ta=multicore -Minfo=accel -o laplace_multicore laplace2d.f90 jacobi.f90 \n",
            "laplace2d.f90:\n",
            "calcnext:\n",
            "     34, Generating Multicore code\n",
            "         37, !$acc loop gang\n",
            "     37, Generating implicit reduction(max:error)\n",
            "     38, Loop is parallelizable\n",
            "swap:\n",
            "     56, Generating Multicore code\n",
            "         58, !$acc loop gang\n",
            "     59, Loop is parallelizable\n",
            "jacobi.f90:\n",
            "Compilation Successful!\n",
            "Jacobi relaxation Calculation: 4096 x 4096 mesh\n",
            "    0  0.250000\n",
            "  100  0.002397\n",
            "  200  0.001204\n",
            "  300  0.000804\n",
            "  400  0.000603\n",
            "  500  0.000483\n",
            "  600  0.000403\n",
            "  700  0.000345\n",
            "  800  0.000302\n",
            "  900  0.000269\n",
            " completed in     44.516 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's the ouput you should see after running the above cell. Your total runtime may be slightly different, but it should be close. If you find yourself stuck on this part, you can take a look at our [solution](https://github.com/openhackathons-org/gpubootcamp/blob/5c077a6c70989492f4bd850240fdcaf8b16fd555/hpc/openacc/English/Fortran/source_code/lab1/solutions/laplace2d.parallel.f90).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "laplace2d.f90:\n",
        "calcnext:\n",
        "     58, Generating Multicore code\n",
        "         59, !$acc loop gang\n",
        "     58, Generating reduction(max:error)\n",
        "     60, Loop is parallelizable\n",
        "swap:\n",
        "     76, Generating Multicore code\n",
        "         77, !$acc loop gang\n",
        "     78, Loop is parallelizable\n",
        "jacobi.f90:\n",
        "Compilation Successful!\n",
        "Jacobi relaxation Calculation: 4096 x 4096 mesh\n",
        "    0  0.250000\n",
        "  100  0.002397\n",
        "  200  0.001204\n",
        "  300  0.000804\n",
        "  400  0.000603\n",
        "  500  0.000483\n",
        "  600  0.000403\n",
        "  700  0.000345\n",
        "  800  0.000302\n",
        "  900  0.000269\n",
        " completed in     32.012 seconds\n",
        "```\n",
        "\n",
        "Great! Now our code is running nearly twice as fast by using all of the cores on our CPU, but I really want to run the code on a GPU. Once you have accelerated both loop nests in the example application and are sure you're getting correct results, you only need to change one compiler option to build the code for the GPUs on our node.\n",
        "\n",
        "Here's the new compiler option we'll be using:\n",
        "\n",
        " - **`-ta=tesla:cc70,managed `**- Build the code for the NVIDIA Tesla GPU on our system, using managed memory\n",
        " \n",
        "Notice above that I'm using something called managed memory for this task. Since our CPU and GPU each have their own physical memory I need to move the data between these memories. To simplify things this week, I'm telling the compiler to manage all of that data movement for me. Next week you'll learn how and why to manage the data movement yourself."
      ],
      "metadata": {
        "id": "gt-bE5gY3ts-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "FC := nvfortran\n",
        "\n",
        "ACCFLAGS_3 := -fast -acc -ta=tesla:managed -Minfo=accel\n",
        "\n",
        "laplace_gpu: laplace2d.f90 jacobi.f90 \n",
        "\t${FC} ${ACCFLAGS_3} -o laplace_gpu laplace2d.f90 jacobi.f90 \n"
      ],
      "metadata": {
        "id": "vMTALqYZLX0B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! make clean && make laplace_gpu && echo \"Compilation Successful!\" && ./laplace_gpu"
      ],
      "metadata": {
        "id": "7-OEQhAb5VdU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22ece253-a7f6-437e-f396-a25e30ebd12d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm -f *.o laplace_serial laplace_multicore laplace_gpu\n",
            "nvfortran -fast -acc -ta=tesla:managed -Minfo=accel -o laplace_gpu laplace2d.f90 jacobi.f90 \n",
            "laplace2d.f90:\n",
            "calcnext:\n",
            "     34, Generating NVIDIA GPU code\n",
            "         37, !$acc loop gang ! blockidx%x\n",
            "             Generating implicit reduction(max:error)\n",
            "         38, !$acc loop vector(128) ! threadidx%x\n",
            "     34, Generating implicit copyin(a(:n-1,:m-1)) [if not already present]\n",
            "         Generating implicit copy(error) [if not already present]\n",
            "         Generating implicit copyout(anew(1:n-2,1:m-2)) [if not already present]\n",
            "     38, Loop is parallelizable\n",
            "swap:\n",
            "     56, Generating NVIDIA GPU code\n",
            "         58, !$acc loop gang ! blockidx%x\n",
            "         59, !$acc loop vector(128) ! threadidx%x\n",
            "     56, Generating implicit copyout(a(1:n-2,1:m-2)) [if not already present]\n",
            "         Generating implicit copyin(anew(1:n-2,1:m-2)) [if not already present]\n",
            "     59, Loop is parallelizable\n",
            "jacobi.f90:\n",
            "Compilation Successful!\n",
            "Jacobi relaxation Calculation: 4096 x 4096 mesh\n",
            "    0  0.250000\n",
            "  100  0.002397\n",
            "  200  0.001204\n",
            "  300  0.000804\n",
            "  400  0.000603\n",
            "  500  0.000483\n",
            "  600  0.000403\n",
            "  700  0.000345\n",
            "  800  0.000302\n",
            "  900  0.000269\n",
            " completed in      1.081 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wow! That ran a lot faster! This demonstrates the power of using OpenACC to accelerate an application. I made very minimal code changes and could run my code on multicore CPUs and GPUs by only changing my compiler option. Very cool!\n",
        "\n",
        "Just for your reference, here's the timings I saw at each step of the process. Please keep in mind that your times may be just a little bit different\n",
        "\n",
        "- original : 58s\n",
        "- Multicore : 32 s\n",
        "- GPU : 4 sec\n",
        "\n",
        "So how did the compiler perform this miracle of speeding up my code on both the CPU and GPU? Let's compiler the compiler output from those two versions:\n",
        "\n",
        "### CPU\n",
        "```\n",
        "calcnext:\n",
        "     58, Generating Multicore code\n",
        "         59, !$acc loop gang\n",
        "     58, Generating reduction(max:error)\n",
        "     60, Loop is parallelizable\n",
        "```\n",
        "\n",
        "### GPU\n",
        "```\n",
        "calcnext:\n",
        "     58, Accelerator kernel generated\n",
        "         Generating Tesla code\n",
        "         58, Generating reduction(max:error)\n",
        "         59, !$acc loop gang ! blockidx%x\n",
        "         60, !$acc loop vector(128) ! threadidx%x\n",
        "     58, Generating implicit copyin(a(:n-1,:m-1))\n",
        "         Generating implicit copy(error)\n",
        "         Generating implicit copyout(anew(1:n-2,1:m-2))\n",
        "     60, Loop is parallelizable\n",
        "```\n",
        "\n",
        "Notice the differences on lines 59 and 60 . The compiler recognized that the loops could be parallelized, but chose to break up the work in different ways. In a future lab you will learn more about how OpenACC breaks up the work, but for now it's enough to know that the compiler understood the differences between these two processors and changed its plan to make sense for each.\n",
        "\n",
        "# Conclusion\n",
        "That's it, you now have the necessary tools to start using OpenACC in your application! In future labs you will learn about how to manage the CPU and GPU memories and how to optimize your loops to run faster, so be sure to attend each week of this online course and to do each lab."
      ],
      "metadata": {
        "id": "z3ZI2Ky15anx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LAB2 \n",
        "<img src=\"https://www.nvidia.com/content/dam/en-zz/Solutions/about-nvidia/logo-and-brand/02-nvidia-logo-color-wht-500x200-4c25-d@2x.png\" height=\"100\"></td></tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "wOX-K-hI55Gl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# prepare lab2 in colab"
      ],
      "metadata": {
        "id": "PKr1EFE963rv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p  /content/lab2"
      ],
      "metadata": {
        "id": "_OIH03pS6PvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/lab2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fw1KO5FU6TwD",
        "outputId": "13784033-4b8f-466e-9c80-4a00715da328"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/lab2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file Makefile \n",
        "# Copyright (c) 2020 NVIDIA Corporation.  All rights reserved. \n",
        "FC := nvfortran\n",
        "ACCFLAGS_1 := -fast -ta=tesla -Minfo=accel\n",
        "ACCFLAGS_2 := -fast -ta=tesla:managed -Minfo=accel\n",
        "\n",
        "laplace_update: laplace2d.f90 jacobi.f90\n",
        "\t${FC} ${ACCFLAGS_1} -o laplace_update laplace2d.f90 jacobi.f90\n",
        "\n",
        "laplace_no_update: laplace2d.f90 jacobi.f90\n",
        "\t${FC} ${ACCFLAGS_1} -o laplace_no_update laplace2d.f90 jacobi.f90\n",
        "\n",
        "laplace_no_managed: laplace2d.f90 jacobi.f90\n",
        "\t${FC} ${ACCFLAGS_1} -o laplace laplace2d.f90 jacobi.f90\n",
        "\n",
        "laplace_managed: laplace2d.f90 jacobi.f90\n",
        "\t${FC} ${ACCFLAGS_2} -o laplace_managed laplace2d.f90 jacobi.f90\n",
        "\n",
        "clean:\n",
        "\trm -f *.o laplace laplace_*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyVhfgC67NC8",
        "outputId": "65bd4312-9dec-4898-c34a-8768476b0726"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting Makefile\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file jacobi.f90 \n",
        "\n",
        "program jacobi\n",
        "  use laplace2d\n",
        "  implicit none\n",
        "  integer, parameter :: fp_kind=kind(1.0d0)\n",
        "  integer, parameter :: n=4096, m=4096, iter_max=1000\n",
        "  integer :: i, j, iter\n",
        "  real(fp_kind), dimension (:,:), allocatable :: A, Anew\n",
        "  real(fp_kind) :: tol=1.0e-6_fp_kind, error=1.0_fp_kind\n",
        "  real(fp_kind) :: start_time, stop_time\n",
        "\n",
        "  call initialize(A, Anew, m, n)\n",
        "   \n",
        "  write(*,'(a,i5,a,i5,a)') 'Jacobi relaxation Calculation:', n, ' x', m, ' mesh'\n",
        " \n",
        "  call cpu_time(start_time) \n",
        "\n",
        "  iter=0\n",
        "  \n",
        "  do while ( error .gt. tol .and. iter .lt. iter_max )\n",
        "\n",
        "    error = calcNext(A, Anew, m, n)\n",
        "    call swap(A, Anew, m, n)\n",
        "\n",
        "    if(mod(iter,100).eq.0 ) write(*,'(i5,f10.6)'), iter, error\n",
        "\n",
        "    iter = iter + 1\n",
        "\n",
        "  end do\n",
        "\n",
        "  call cpu_time(stop_time) \n",
        "  write(*,'(a,f10.3,a)')  ' completed in ', stop_time-start_time, ' seconds'\n",
        "\n",
        "  deallocate (A,Anew)\n",
        "end program jacobi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjSVWx3e6-ed",
        "outputId": "9b0c2557-d018-4808-aebb-9e7f175197e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting jacobi.f90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file laplace2d.f90 \n",
        "\n",
        "module laplace2d\n",
        "  public :: initialize\n",
        "  public :: calcNext\n",
        "  public :: swap\n",
        "  public :: dealloc\n",
        "  contains\n",
        "    subroutine initialize(A, Anew, m, n)\n",
        "      integer, parameter :: fp_kind=kind(1.0d0)\n",
        "      real(fp_kind),allocatable,intent(out)   :: A(:,:)\n",
        "      real(fp_kind),allocatable,intent(out)   :: Anew(:,:)\n",
        "      integer,intent(in)          :: m, n\n",
        "\n",
        "      allocate ( A(0:n-1,0:m-1), Anew(0:n-1,0:m-1) )\n",
        "\n",
        "      A    = 0.0_fp_kind\n",
        "      Anew = 0.0_fp_kind\n",
        "\n",
        "      A(0,:)    = 1.0_fp_kind\n",
        "      Anew(0,:) = 1.0_fp_kind\n",
        "    end subroutine initialize\n",
        "\n",
        "    function calcNext(A, Anew, m, n)\n",
        "      integer, parameter          :: fp_kind=kind(1.0d0)\n",
        "      real(fp_kind),intent(inout) :: A(0:n-1,0:m-1)\n",
        "      real(fp_kind),intent(inout) :: Anew(0:n-1,0:m-1)\n",
        "      integer,intent(in)          :: m, n\n",
        "      integer                     :: i, j\n",
        "      real(fp_kind)               :: error\n",
        "\n",
        "      error=0.0_fp_kind\n",
        "\n",
        "      !$acc parallel loop reduction(max:error)\n",
        "      do j=1,m-2\n",
        "        do i=1,n-2\n",
        "          Anew(i,j) = 0.25_fp_kind * ( A(i+1,j  ) + A(i-1,j  ) + &\n",
        "                                       A(i  ,j-1) + A(i  ,j+1) )\n",
        "          error = max( error, abs(Anew(i,j)-A(i,j)) )\n",
        "        end do\n",
        "      end do\n",
        "      calcNext = error\n",
        "    end function calcNext\n",
        "\n",
        "    subroutine swap(A, Anew, m, n)\n",
        "      integer, parameter        :: fp_kind=kind(1.0d0)\n",
        "      real(fp_kind),intent(out) :: A(0:n-1,0:m-1)\n",
        "      real(fp_kind),intent(in)  :: Anew(0:n-1,0:m-1)\n",
        "      integer,intent(in)        :: m, n\n",
        "      integer                   :: i, j\n",
        "\n",
        "      !$acc parallel loop\n",
        "      do j=1,m-2\n",
        "        do i=1,n-2\n",
        "          A(i,j) = Anew(i,j)\n",
        "        end do\n",
        "      end do\n",
        "    end subroutine swap\n",
        "\n",
        "    subroutine dealloc(A, Anew)\n",
        "      integer, parameter :: fp_kind=kind(1.0d0)\n",
        "      real(fp_kind),allocatable,intent(in) :: A\n",
        "      real(fp_kind),allocatable,intent(in) :: Anew\n",
        "\n",
        "      deallocate (A,Anew)\n",
        "    end subroutine\n",
        "end module laplace2d"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7jm0Xy-7Cg9",
        "outputId": "f56e2d59-9f62-486e-af0d-137b16f60190"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting laplace2d.f90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Run the Code (With Managed Memory)\n",
        "In the previous lab, we added OpenACC loop directives and relied on a feature called CUDA Managed Memory to deal with the separate CPU & GPU memories for us. Just adding OpenACC to our two loop nests we achieved a considerable performance boost. However, managed memory is not compatible with all GPUs or all compilers and it sometimes performs worse than programmer-defined memory management. Let's start with our solution from the previous lab and use this as our performance baseline. Note the runtime from the follow cell."
      ],
      "metadata": {
        "id": "IQth2eLC7a17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! make clean && make laplace_managed && ./laplace_managed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6txMlLV7eT1",
        "outputId": "5bd8f516-e311-4f77-8964-fd8ae036f03c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm -f *.o laplace laplace_*\n",
            "nvfortran -fast -ta=tesla:managed -Minfo=accel -o laplace_managed laplace2d.f90 jacobi.f90\n",
            "laplace2d.f90:\n",
            "calcnext:\n",
            "     33, Generating NVIDIA GPU code\n",
            "         34, !$acc loop gang ! blockidx%x\n",
            "             Generating reduction(max:error)\n",
            "         35, !$acc loop vector(128) ! threadidx%x\n",
            "     33, Generating implicit copyin(a(:n-1,:m-1)) [if not already present]\n",
            "         Generating implicit copy(error) [if not already present]\n",
            "         Generating implicit copyout(anew(1:n-2,1:m-2)) [if not already present]\n",
            "     35, Loop is parallelizable\n",
            "swap:\n",
            "     51, Generating NVIDIA GPU code\n",
            "         52, !$acc loop gang ! blockidx%x\n",
            "         53, !$acc loop vector(128) ! threadidx%x\n",
            "     51, Generating implicit copyout(a(1:n-2,1:m-2)) [if not already present]\n",
            "         Generating implicit copyin(anew(1:n-2,1:m-2)) [if not already present]\n",
            "     53, Loop is parallelizable\n",
            "jacobi.f90:\n",
            "Jacobi relaxation Calculation: 4096 x 4096 mesh\n",
            "    0  0.250000\n",
            "  100  0.002397\n",
            "  200  0.001204\n",
            "  300  0.000804\n",
            "  400  0.000603\n",
            "  500  0.000483\n",
            "  600  0.000403\n",
            "  700  0.000345\n",
            "  800  0.000302\n",
            "  900  0.000269\n",
            " completed in      0.550 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building Without Managed Memory\n",
        "For this exercise we ultimately don't want to use CUDA Managed Memory, so let's removed the managed option from our compiler options. Try building and running the code now. What happens?"
      ],
      "metadata": {
        "id": "MFGXlvsk7mAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! make clean && make laplace_no_managed && ./laplace"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esiooKfe7osD",
        "outputId": "5f56b922-68a1-4386-9e68-6ddeab1619a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm -f *.o laplace laplace_*\n",
            "nvfortran -fast -ta=tesla -Minfo=accel -o laplace laplace2d.f90 jacobi.f90\n",
            "laplace2d.f90:\n",
            "calcnext:\n",
            "     33, Generating NVIDIA GPU code\n",
            "         34, !$acc loop gang ! blockidx%x\n",
            "             Generating reduction(max:error)\n",
            "         35, !$acc loop vector(128) ! threadidx%x\n",
            "     33, Generating implicit copyin(a(:n-1,:m-1)) [if not already present]\n",
            "         Generating implicit copy(error) [if not already present]\n",
            "         Generating implicit copyout(anew(1:n-2,1:m-2)) [if not already present]\n",
            "     35, Loop is parallelizable\n",
            "swap:\n",
            "     51, Generating NVIDIA GPU code\n",
            "         52, !$acc loop gang ! blockidx%x\n",
            "         53, !$acc loop vector(128) ! threadidx%x\n",
            "     51, Generating implicit copyout(a(1:n-2,1:m-2)) [if not already present]\n",
            "         Generating implicit copyin(anew(1:n-2,1:m-2)) [if not already present]\n",
            "     53, Loop is parallelizable\n",
            "jacobi.f90:\n",
            "Jacobi relaxation Calculation: 4096 x 4096 mesh\n",
            "    0  0.250000\n",
            "  100  0.002397\n",
            "  200  0.001204\n",
            "  300  0.000804\n",
            "  400  0.000603\n",
            "  500  0.000483\n",
            "  600  0.000403\n",
            "  700  0.000345\n",
            "  800  0.000302\n",
            "  900  0.000269\n",
            " completed in    117.390 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "OK, so we're able to run, but we're running very slowly. We'll address that in just a moment, but first we should address an issue that you might encounter if you ever try this same exercise in C/C++. Because Fortran arrays contain all of the necessary size and shape information for the compiler to move them to and from the device, this step works. Had you been programming in C/C++, however, the compiler would lack the information to move the arrays. Instead, you would have seen the following:\n",
        "\n",
        "```\n",
        "jacobi.c:\n",
        "laplace2d.c:\n",
        "NVC++-S-0155-Compiler failed to translate accelerator region (see -Minfo messages): Could not find allocated-variable index for symbol (laplace2d.c: 47)\n",
        "calcNext:\n",
        "     47, Accelerator kernel generated\n",
        "         Generating Tesla code\n",
        "         48, #pragma acc loop gang /* blockIdx.x */\n",
        "         50, #pragma acc loop vector(128) /* threadIdx.x */\n",
        "         54, Generating implicit reduction(max:error)\n",
        "     48, Accelerator restriction: size of the GPU copy of Anew,A is unknown\n",
        "     50, Loop is parallelizable\n",
        "NVC++-F-0704-Compilation aborted due to previous errors. (laplace2d.c)\n",
        "NVC++/x86-64 Linux 18.7-0: compilation aborted\n",
        "This error message is not very intuitive, so let me explain it to you.:\n",
        "```\n",
        "\n",
        "- NVC++-S-0155-Compiler failed to translate accelerator region (see -Minfo messages): Could not find allocated-variable index for symbol (laplace2d.c: 47) - The compiler doesn't like something about a variable from line 47 of our code.\n",
        "\n",
        "- 48, Accelerator restriction: size of the GPU copy of Anew,A is unknown - I don't see any further information about line 47, but at line 48 the compiler is struggling to understand the size and shape of the arrays Anew and A. It turns out, this is our problem.\n",
        "\n",
        "So, what these cryptic compiler errors are telling us is that the compiler needs to create copies of A and Anew on the GPU in order to run our code there, but it doesn't know how big they are, so it's giving up. We would need to give the compiler more information about these arrays before it can move forward in C/C++. In Fortran the compiler can help us out, but the goal here is to explicitly manage your data, so let's find out how to do that."
      ],
      "metadata": {
        "id": "QVakm0Xp7uCZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "OpenACC Data Clauses\n",
        "Data clauses allow the programmer to specify data transfers between the host and device (or in our case, the CPU and the GPU). Because they are clauses, they can be added to other directives, such as the parallel loop directive that we used in the previous lab. Let's look at an example where we do not use a data clause.\n",
        "```\n",
        "allocate(A(N))\n",
        "\n",
        "  !$acc parallel loop\n",
        "  do i=1,100\n",
        "    A(i) = 0\n",
        "  enddo\n",
        "```  \n",
        "We have allocated an array A outside of our parallel region. This means that A is allocated in the CPU memory. However, we access A inside of our loop, and that loop is contained within a parallel region. Within that parallel region, A(i) is attempting to access a memory location within the GPU memory. We didn't explicitly allocate A on the GPU, so one of two things will happen.\n",
        "\n",
        "The compiler will understand what we are trying to do, and automatically copy A from the CPU to the GPU.\n",
        "The program will check for an array A in GPU memory, it won't find it, and it will throw an error.\n",
        "Instead of hoping that we have a compiler that can figure this out, we could instead use a data clause.\n",
        "\n",
        "```\n",
        "allocate(A(N))\n",
        "\n",
        "  !$acc parallel loop copy(A(1:N))\n",
        "  do i=1,100\n",
        "    A(i) = 0\n",
        "  enddo\n",
        "```  \n",
        "We will learn the copy data clause first, because it is the easiest to use. With the inclusion of the copy data clause, our program will now copy the content of A from the CPU memory, into GPU memory. Then, during the execution of the loop, it will properly access A from the GPU memory. After the parallel region is finished, our program will copy A from the GPU memory back to the CPU memory. Let's look at one more direct example.\n",
        "\n",
        "```\n",
        "allocate(A(N))\n",
        "\n",
        "  do i=1,100\n",
        "    A(i) = 0\n",
        "  enddo\n",
        "\n",
        "  !$acc parallel loop copy(A(1:N))\n",
        "  do i=1,100\n",
        "    A(i) = 1\n",
        "  enddo\n",
        "```  \n",
        "Now we have two loops; the first loop will execute on the CPU (since it does not have an OpenACC parallel directive), and the second loop will execute on the GPU. Array A will be allocated on the CPU, and then the first loop will execute. This loop will set the contents of A to be all 0. Then the second loop is encountered; the program will copy the array A (which is full of 0's) into GPU memory. Then, we will execute the second loop on the GPU. This will edit the GPU's copy of A to be full of 1's.\n",
        "\n",
        "At this point, we have two seperate copies of A. The CPU copy is full of 0's, and the GPU copy is full of 1's. Now, after the parallel region finishes, the program will copy A back from the GPU to the CPU. After this copy, both the CPU and the GPU will contain a copy of A that contains all 1's. The GPU copy of A will then be deallocated.\n",
        "\n",
        "This image offers another step-by-step example of using the copy clause.\n",
        "\n"
      ],
      "metadata": {
        "id": "6JdPFC0f8TLL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://github.com/openhackathons-org/gpubootcamp/raw/5c077a6c70989492f4bd850240fdcaf8b16fd555/hpc/openacc/English/Fortran/jupyter_notebook/images/copy_step_by_step.png\" width=800>"
      ],
      "metadata": {
        "id": "AYrGYTx78jlL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are also able to copy multiple arrays at once by using the following syntax.\n",
        "```\n",
        "!$acc parallel loop copy(A(1:N), B(1:N))\n",
        "do i = 1, N\n",
        "    A(i) = B(i)\n",
        "end do\n",
        "```\n",
        "Of course, we might not want to copy our data both to and from the GPU memory. Maybe we only need the array's values as inputs to the GPU region, or maybe it's only the final results we care about, or perhaps the array is only used temporarily on the GPU and we don't want to copy it either directive. The following OpenACC data clauses provide a bit more control than just the copy clause.\n",
        "\n",
        " - **`copyin`** - Create space for the array and copy the input values of the array to the device. At the end of the region, the array is deleted without copying anything back to the host.\n",
        " - **`copyout`** - Create space for the array on the device, but don't initialize it to anything. At the end of the region, copy the results back and then delete the device array.\n",
        " - **`create`** - Create space of the array on the device, but do not copy anything to the device at the beginning of the region, nor back to the host at the end. The array will be deleted from the device at the end of the region.\n",
        " - **`present`** - Don't do anything with these variables. I've put them on the device somewhere else, so just assume they're available.\n",
        "\n",
        "You may also use them to operate on multiple arrays at once, by including those arrays as a comma separated list.\n",
        "\n",
        "`!$acc parallel loop copy( A(1:N), B(1:M), C(1:Q) )`\n",
        "You may also use more than one data clause at a time.\n",
        "\n",
        "`!$acc parallel loop create( A(1:N) ) copyin( B(1:M) ) copyout( C(1:Q) )`\n"
      ],
      "metadata": {
        "id": "LOdDEdZZ8hyK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Array Shaping\n",
        "\n",
        "The shape of the array specifies how much data needs to be transferred. Let's look at an example:\n",
        "\n",
        "\n",
        "\n",
        "```fortran\n",
        "!$acc parallel loop copy(A(1:N))\n",
        "do i = 1, N\n",
        "  A(i) = 0\n",
        "end do\n",
        "```\n",
        "\n",
        "\n",
        "Focusing specifically on the copy(A(1:N)), the shape of the array is defined within the brackets. The syntax for array shape is (starting_index:ending_index). This means that (in the code example) we are copying data from array A, starting at index 1 (the start of the array), and copying until index N (which is most likely the length of the entire array).\n",
        "\n",
        "We are also able to only copy a portion of the array:\n",
        "\n",
        "`!$acc parallel loop copy(A(2:N-2))`\n",
        "This would copy all of the elements of A except for the first, and last element.\n",
        "\n",
        "Lastly, if you do not specify a starting index, 1 is assumed. This means that\n",
        "\n",
        "`!$acc parallel loop copy(A(1:N))`\n",
        "is equivalent to\n",
        "\n",
        "`!$acc parallel loop copy(A(:N))`\n",
        "And since we're in Fortran, this can be shorted even more to just\n",
        "\n",
        "`!$acc parallel loop copy(A(:))`"
      ],
      "metadata": {
        "id": "-mLqBfG987hZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making the Sample Code Work with Explicit Data Management\n",
        "In order to build our example code without CUDA managed memory but with explicit programmer-controlled data management, we need to give the compiler more information about the arrays. How do our two loop nests use the arrays A and Anew? The calcNext function take A as input and generates Anew as output, but also needs Anew copied in because we need to maintain that hot boundary at the top. So you will want to add a copyin clause for A and a copy clause for Anew on your region. The swap function takes Anew as input and A as output, so it needs the exact opposite data clauses. It's also necessary to tell the compiler the size of the two arrays by using array shaping. Our arrays are m times n in size, so we'll tell the compiler their shape starts at 0 and has n*m elements, using the syntax above. Go ahead and add data clauses to the two parallel loop directives in laplace2d.f90.\n",
        "\n",
        "modify**` laplace2d.f90`** Then try to build again."
      ],
      "metadata": {
        "id": "8Jvtrp829Mha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file laplace2d.f90\n",
        "\n",
        "module laplace2d\n",
        "  public :: initialize\n",
        "  public :: calcNext\n",
        "  public :: swap\n",
        "  public :: dealloc\n",
        "  contains\n",
        "    subroutine initialize(A, Anew, m, n)\n",
        "      integer, parameter :: fp_kind=kind(1.0d0)\n",
        "      real(fp_kind),allocatable,intent(out)   :: A(:,:)\n",
        "      real(fp_kind),allocatable,intent(out)   :: Anew(:,:)\n",
        "      integer,intent(in)          :: m, n\n",
        "\n",
        "      allocate ( A(0:n-1,0:m-1), Anew(0:n-1,0:m-1) )\n",
        "\n",
        "      A    = 0.0_fp_kind\n",
        "      Anew = 0.0_fp_kind\n",
        "\n",
        "      A(0,:)    = 1.0_fp_kind\n",
        "      Anew(0,:) = 1.0_fp_kind\n",
        "    end subroutine initialize\n",
        "\n",
        "    function calcNext(A, Anew, m, n)\n",
        "      integer, parameter          :: fp_kind=kind(1.0d0)\n",
        "      real(fp_kind),intent(inout) :: A(0:n-1,0:m-1)\n",
        "      real(fp_kind),intent(inout) :: Anew(0:n-1,0:m-1)\n",
        "      integer,intent(in)          :: m, n\n",
        "      integer                     :: i, j\n",
        "      real(fp_kind)               :: error\n",
        "\n",
        "      error=0.0_fp_kind\n",
        "\n",
        "      !$acc parallel loop reduction(max:error) copyin(A(:,:)) copy(Anew(:,:))\n",
        "      do j=1,m-2\n",
        "        do i=1,n-2\n",
        "          Anew(i,j) = 0.25_fp_kind * ( A(i+1,j  ) + A(i-1,j  ) + &\n",
        "                                       A(i  ,j-1) + A(i  ,j+1) )\n",
        "          error = max( error, abs(Anew(i,j)-A(i,j)) )\n",
        "        end do\n",
        "      end do\n",
        "      calcNext = error\n",
        "    end function calcNext\n",
        "\n",
        "    subroutine swap(A, Anew, m, n)\n",
        "      integer, parameter        :: fp_kind=kind(1.0d0)\n",
        "      real(fp_kind),intent(out) :: A(0:n-1,0:m-1)\n",
        "      real(fp_kind),intent(in)  :: Anew(0:n-1,0:m-1)\n",
        "      integer,intent(in)        :: m, n\n",
        "      integer                   :: i, j\n",
        "\n",
        "      !$acc parallel loop copyin(Anew(:,:)) copyout(A(:,:))\n",
        "      do j=1,m-2\n",
        "        do i=1,n-2\n",
        "          A(i,j) = Anew(i,j)\n",
        "        end do\n",
        "      end do\n",
        "    end subroutine swap\n",
        "\n",
        "    subroutine dealloc(A, Anew)\n",
        "      integer, parameter :: fp_kind=kind(1.0d0)\n",
        "      real(fp_kind),allocatable,intent(in) :: A\n",
        "      real(fp_kind),allocatable,intent(in) :: Anew\n",
        "\n",
        "      deallocate (A,Anew)\n",
        "    end subroutine\n",
        "end module laplace2d"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vs_q4RMW-mUp",
        "outputId": "1a7cfad1-7798-4fc9-812a-ce48db177ec6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting laplace2d.f90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!make clean && make laplace_no_managed && ./laplace"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "corI7nKH9Yag",
        "outputId": "dc48af10-4197-4d8f-cd21-a74369450e67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm -f *.o laplace laplace_*\n",
            "nvfortran -fast -ta=tesla -Minfo=accel -o laplace laplace2d.f90 jacobi.f90\n",
            "laplace2d.f90:\n",
            "calcnext:\n",
            "     33, Generating copyin(a(:,:)) [if not already present]\n",
            "         Generating NVIDIA GPU code\n",
            "         34, !$acc loop gang ! blockidx%x\n",
            "             Generating reduction(max:error)\n",
            "         35, !$acc loop vector(128) ! threadidx%x\n",
            "     33, Generating implicit copy(error) [if not already present]\n",
            "         Generating copy(anew(:,:)) [if not already present]\n",
            "     35, Loop is parallelizable\n",
            "swap:\n",
            "     51, Generating copyout(a(:,:)) [if not already present]\n",
            "         Generating copyin(anew(:,:)) [if not already present]\n",
            "         Generating NVIDIA GPU code\n",
            "         52, !$acc loop gang ! blockidx%x\n",
            "         53, !$acc loop vector(128) ! threadidx%x\n",
            "     53, Loop is parallelizable\n",
            "jacobi.f90:\n",
            "Jacobi relaxation Calculation: 4096 x 4096 mesh\n",
            "    0  0.250000\n",
            "  100  0.002397\n",
            "  200  0.001204\n",
            "  300  0.000804\n",
            "  400  0.000603\n",
            "  500  0.000483\n",
            "  600  0.000403\n",
            "  700  0.000345\n",
            "  800  0.000302\n",
            "  900  0.000269\n",
            " completed in    139.783 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hint : [solution](https://github.com/openhackathons-org/gpubootcamp/blob/5c077a6c70989492f4bd850240fdcaf8b16fd555/hpc/openacc/English/Fortran/source_code/lab2/solutions/laplace2d.f90)"
      ],
      "metadata": {
        "id": "PPGyHEE3BWx1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Well, the good news is that it should have built correctly and run. If it didn't, check your data clauses carefully. The bad news is that now it runs a whole lot slower than it did in the last exercise. Let's try to figure out why. The NVIDIA HPC compiler provides your executable with built-in timers, so let's start by enabling them and seeing what it shows. You can enable these timers by setting the environment variable **`PGI_ACC_TIME=1`**. Run the cell below to get the program output with the built-in profiler enabled."
      ],
      "metadata": {
        "id": "VyWYOoMuCFZ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!make clean && make laplace_no_managed && PGI_ACC_TIME=1 ./laplace "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHulfizFCBOs",
        "outputId": "d66c5a3a-657c-4acd-9955-d11d4de8412c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm -f *.o laplace laplace_*\n",
            "nvfortran -fast -ta=tesla -Minfo=accel -o laplace laplace2d.f90 jacobi.f90\n",
            "laplace2d.f90:\n",
            "calcnext:\n",
            "     33, Generating copyin(a(:,:)) [if not already present]\n",
            "         Generating NVIDIA GPU code\n",
            "         34, !$acc loop gang ! blockidx%x\n",
            "             Generating reduction(max:error)\n",
            "         35, !$acc loop vector(128) ! threadidx%x\n",
            "     33, Generating implicit copy(error) [if not already present]\n",
            "         Generating copy(anew(:,:)) [if not already present]\n",
            "     35, Loop is parallelizable\n",
            "swap:\n",
            "     51, Generating copyout(a(:,:)) [if not already present]\n",
            "         Generating copyin(anew(:,:)) [if not already present]\n",
            "         Generating NVIDIA GPU code\n",
            "         52, !$acc loop gang ! blockidx%x\n",
            "         53, !$acc loop vector(128) ! threadidx%x\n",
            "     53, Loop is parallelizable\n",
            "jacobi.f90:\n",
            "Jacobi relaxation Calculation: 4096 x 4096 mesh\n",
            "libcupti.so not found\n",
            "    0  0.250000\n",
            "  100  0.002397\n",
            "  200  0.001204\n",
            "  300  0.000804\n",
            "  400  0.000603\n",
            "  500  0.000483\n",
            "  600  0.000403\n",
            "  700  0.000345\n",
            "  800  0.000302\n",
            "  900  0.000269\n",
            " completed in    170.598 seconds\n",
            "\n",
            "Accelerator Kernel Timing data\n",
            "/content/lab2/laplace2d.f90\n",
            "  calcnext  NVIDIA  devicenum=0\n",
            "    time(us): 32,538,707\n",
            "    33: compute region reached 1000 times\n",
            "        33: kernel launched 1000 times\n",
            "            grid: [4094]  block: [128]\n",
            "            elapsed time(us): total=250,237 max=312 min=247 avg=250\n",
            "        33: reduction kernel launched 1000 times\n",
            "            grid: [1]  block: [256]\n",
            "            elapsed time(us): total=27,467 max=41 min=26 avg=27\n",
            "    33: data region reached 4000 times\n",
            "        33: data copyin transfers: 17000\n",
            "             device time(us): total=22,222,114 max=2,215 min=10 avg=1,307\n",
            "        41: data copyout transfers: 10000\n",
            "             device time(us): total=10,316,593 max=2,556 min=8 avg=1,031\n",
            "/content/lab2/laplace2d.f90\n",
            "  swap  NVIDIA  devicenum=0\n",
            "    time(us): 21,423,170\n",
            "    51: compute region reached 1000 times\n",
            "        51: kernel launched 1000 times\n",
            "            grid: [4094]  block: [128]\n",
            "            elapsed time(us): total=244,246 max=303 min=239 avg=244\n",
            "    51: data region reached 2000 times\n",
            "        51: data copyin transfers: 8000\n",
            "             device time(us): total=11,118,293 max=2,207 min=1,366 avg=1,389\n",
            "        57: data copyout transfers: 9000\n",
            "             device time(us): total=10,304,877 max=2,377 min=8 avg=1,144\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your output should look something like what you see below.\n",
        "```\n",
        " completed in    182.941 seconds\n",
        "\n",
        "Accelerator Kernel Timing data\n",
        "/content/lab2/laplace2d.f90\n",
        "  calcnext  NVIDIA  devicenum=0\n",
        "    time(us): 53,265,706\n",
        "    58: compute region reached 1000 times\n",
        "        58: kernel launched 1000 times\n",
        "            grid: [4094]  block: [128]\n",
        "             device time(us): total=2,398,959 max=2,415 min=2,380 avg=2,398\n",
        "            elapsed time(us): total=2,454,466 max=2,489 min=2,435 avg=2,454\n",
        "        58: reduction kernel launched 1000 times\n",
        "            grid: [2]  block: [256]\n",
        "             device time(us): total=19,011 max=20 min=19 avg=19\n",
        "            elapsed time(us): total=46,816 max=67 min=43 avg=46\n",
        "    58: data region reached 4000 times\n",
        "        58: data copyin transfers: 17000\n",
        "             device time(us): total=33,881,820 max=2,141 min=6 avg=1,993\n",
        "        66: data copyout transfers: 10000\n",
        "             device time(us): total=16,965,916 max=2,135 min=9 avg=1,696\n",
        "/content/lab2/laplace2d.f90\n",
        "  swap  NVIDIA  devicenum=0\n",
        "    time(us): 36,205,726\n",
        "    76: compute region reached 1000 times\n",
        "        76: kernel launched 1000 times\n",
        "            grid: [4094]  block: [128]\n",
        "             device time(us): total=2,306,221 max=2,319 min=2,293 avg=2,306\n",
        "            elapsed time(us): total=2,363,356 max=2,397 min=2,347 avg=2,363\n",
        "    76: data region reached 2000 times\n",
        "        76: data copyin transfers: 8000\n",
        "             device time(us): total=16,942,581 max=2,141 min=2,114 avg=2,117\n",
        "        82: data copyout transfers: 9000\n",
        "             device time(us): total=16,956,924 max=2,134 min=13 avg=1,884\n",
        "\n",
        "```        \n",
        "\n",
        "The total runtime was roughly 130 seconds with the profiler turned on (roughly 120 without). We can see that calcNext required roughly 53 seconds to run by looking at the time(us) line under the calcNext line. We can also look at the data region section and determine that 34 seconds were spent copying data to the device and 17 seconds copying data out for the device. The swap function has very similar numbers. That means that the program is actually spending very little of its runtime doing calculations. Why is the program copying so much data around? The screenshot below comes from the Nsight Systems profiler and shows part of one step of our outer while loop. The greenish and pink colors are data movement and the blue colors are our kernels (calcNext and swap). Notice that for each kernel we have copies to the device (greenish) before and copies from the device (pink) after. The means we have 4 segments of data copies for every iteration of the outer while loop.\n",
        "\n",
        "Let's contrast this with the managed memory version. The image below shows the same program built with managed memory. Notice that there's a lot of \"data migration\" at the beginning, where the data is first used, but there's no data movement between the loops. This tells me that the data movement isn't really needed between these loops, but we need to tell the compiler that.\n",
        "\n",
        "Because the loops are in two separate files, the compiler can't really see that the data is reused on the GPU between those function. We need to move our data movement up to a higher level where we can reuse it for each step through the program. To do that, we'll add OpenACC data directives."
      ],
      "metadata": {
        "id": "TmpKMOYMCPrW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# OpenACC Structured Data Directive\n",
        "The OpenACC data directives allow the programmer to explicitly manage the data on the device (in our case, the GPU). Specifically, the structured data directive will mark a static region of our code as a data region.\n",
        "```\n",
        "< Initialize data on host (CPU) >\n",
        " \n",
        "!$acc data < data clauses >\n",
        "\n",
        "    < Code >\n",
        "\n",
        "!$acc end data\n",
        "```\n",
        "Device memory allocation happens at the beginning of the region, and device memory deallocation happens at the end of the region. Additionally, any data movement from the host to the device (CPU to GPU) happens at the beginning of the region, and any data movement from the device to the host (GPU to CPU) happens at the end of the region. Memory allocation/deallocation and data movement is defined by which clauses the programmer includes. This is a list of the most important data clauses that we can use:\n",
        "\n",
        "Encompassing Multiple Compute Regions\n",
        "A single data region can contain any number of parallel/kernels regions. Take the following example:\n",
        "```\n",
        "!$acc data copyin(A(1:N), B(1:N)) create(C(1:N))\n",
        "\n",
        "    !$acc parallel loop\n",
        "    do i = 1, N\n",
        "        C(i) = A(i) + B(i)\n",
        "    end do\n",
        "\n",
        "    !$acc parallel loop\n",
        "    do i = 1, N\n",
        "        A(i) = C(i) + B(i)\n",
        "    end do\n",
        "\n",
        "!$acc end data\n",
        "```\n",
        "You may also encompass function calls within the data region:\n",
        "```\n",
        "subroutine copy(A, B, N)\n",
        "\n",
        "    !$acc parallel loop\n",
        "    do i = 1, N\n",
        "        A(i) = B(i)\n",
        "    end do\n",
        "\n",
        "end subroutine\n",
        "\n",
        "...\n",
        "\n",
        "!$acc data copyout(A(1:N),B(1:N)) copyin(C(1:N))\n",
        "\n",
        "    call copy(A, C, N)\n",
        "\n",
        "    call copy(A, B, N)\n",
        "!$acc end data\n",
        "```\n",
        "\n",
        "\n",
        "> Indented block\n",
        "\n"
      ],
      "metadata": {
        "id": "63pwA-Wd92WA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding the Structured Data Directive to our Code\n",
        "Add a structured data directive to properly handle the arrays A and Anew. We've already added data clauses to our two functions, so this time we'll move up the calltree and add a structured data region around our while loop in the main program. Think about the input and output to this while loop and choose your data clauses for A and Anew accordingly.\n",
        "modify **`jacobi.f90`** Then, run the following script to check you solution. You code should run just as good as (or slightly better) than our managed memory code."
      ],
      "metadata": {
        "id": "kYRjGg0v-Ghn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file jacobi.f90\n",
        "\n",
        "\n",
        "program jacobi\n",
        "  use laplace2d\n",
        "  implicit none\n",
        "  integer, parameter :: fp_kind=kind(1.0d0)\n",
        "  integer, parameter :: n=4096, m=4096, iter_max=1000\n",
        "  integer :: i, j, iter\n",
        "  real(fp_kind), dimension (:,:), allocatable :: A, Anew\n",
        "  real(fp_kind) :: tol=1.0e-6_fp_kind, error=1.0_fp_kind\n",
        "  real(fp_kind) :: start_time, stop_time\n",
        "\n",
        "  call initialize(A, Anew, m, n)\n",
        "   \n",
        "  write(*,'(a,i5,a,i5,a)') 'Jacobi relaxation Calculation:', n, ' x', m, ' mesh'\n",
        " \n",
        "  call cpu_time(start_time) \n",
        "\n",
        "  iter=0\n",
        "  \n",
        "  !$acc data copy(A(:,:)) create(Anew(:,:))\n",
        "  do while ( error .gt. tol .and. iter .lt. iter_max )\n",
        "\n",
        "    error = calcNext(A, Anew, m, n)\n",
        "    call swap(A, Anew, m, n)\n",
        "\n",
        "    if(mod(iter,100).eq.0 ) write(*,'(i5,f10.6)'), iter, error\n",
        "\n",
        "    iter = iter + 1\n",
        "\n",
        "  end do\n",
        "  !$acc end data\n",
        "\n",
        "  call cpu_time(stop_time) \n",
        "  write(*,'(a,f10.3,a)')  ' completed in ', stop_time-start_time, ' seconds'\n",
        "\n",
        "  deallocate (A,Anew)\n",
        "end program jacobi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrGg-llODF91",
        "outputId": "e58cbfde-3ed5-4604-8566-879623302393"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting jacobi.f90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " ! make clean && make laplace_no_managed && ./laplace"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giw1LzSj-OrA",
        "outputId": "bceaeafe-c9d7-4e91-9a2e-506c230bc3cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm -f *.o laplace laplace_*\n",
            "nvfortran -fast -ta=tesla -Minfo=accel -o laplace laplace2d.f90 jacobi.f90\n",
            "laplace2d.f90:\n",
            "calcnext:\n",
            "     33, Generating copyin(a(:,:)) [if not already present]\n",
            "         Generating NVIDIA GPU code\n",
            "         34, !$acc loop gang ! blockidx%x\n",
            "             Generating reduction(max:error)\n",
            "         35, !$acc loop vector(128) ! threadidx%x\n",
            "     33, Generating implicit copy(error) [if not already present]\n",
            "         Generating copy(anew(:,:)) [if not already present]\n",
            "     35, Loop is parallelizable\n",
            "swap:\n",
            "     51, Generating copyout(a(:,:)) [if not already present]\n",
            "         Generating copyin(anew(:,:)) [if not already present]\n",
            "         Generating NVIDIA GPU code\n",
            "         52, !$acc loop gang ! blockidx%x\n",
            "         53, !$acc loop vector(128) ! threadidx%x\n",
            "     53, Loop is parallelizable\n",
            "jacobi.f90:\n",
            "jacobi:\n",
            "     21, Generating create(anew(anew$sd1:(anew$sd1-1)+anew$sd1,anew$sd1:(anew$sd1-1)+anew$sd1)) [if not already present]\n",
            "         Generating copy(a(a$sd2:(a$sd2-1)+a$sd2,a$sd2:(a$sd2-1)+a$sd2)) [if not already present]\n",
            "Jacobi relaxation Calculation: 4096 x 4096 mesh\n",
            "    0  0.250000\n",
            "  100  0.002397\n",
            "  200  0.001204\n",
            "  300  0.000804\n",
            "  400  0.000603\n",
            "  500  0.000483\n",
            "  600  0.000403\n",
            "  700  0.000345\n",
            "  800  0.000302\n",
            "  900  0.000269\n",
            " completed in      0.710 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zIKFitbFDTcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!make clean && make laplace_no_managed && PGI_ACC_TIME=1 ./laplace "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFK1rxotDTkK",
        "outputId": "18192256-271e-4130-c075-ded29780c2fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm -f *.o laplace laplace_*\n",
            "nvfortran -fast -ta=tesla -Minfo=accel -o laplace laplace2d.f90 jacobi.f90\n",
            "laplace2d.f90:\n",
            "calcnext:\n",
            "     33, Generating copyin(a(:,:)) [if not already present]\n",
            "         Generating NVIDIA GPU code\n",
            "         34, !$acc loop gang ! blockidx%x\n",
            "             Generating reduction(max:error)\n",
            "         35, !$acc loop vector(128) ! threadidx%x\n",
            "     33, Generating implicit copy(error) [if not already present]\n",
            "         Generating copy(anew(:,:)) [if not already present]\n",
            "     35, Loop is parallelizable\n",
            "swap:\n",
            "     51, Generating copyout(a(:,:)) [if not already present]\n",
            "         Generating copyin(anew(:,:)) [if not already present]\n",
            "         Generating NVIDIA GPU code\n",
            "         52, !$acc loop gang ! blockidx%x\n",
            "         53, !$acc loop vector(128) ! threadidx%x\n",
            "     53, Loop is parallelizable\n",
            "jacobi.f90:\n",
            "jacobi:\n",
            "     21, Generating create(anew(anew$sd1:(anew$sd1-1)+anew$sd1,anew$sd1:(anew$sd1-1)+anew$sd1)) [if not already present]\n",
            "         Generating copy(a(a$sd2:(a$sd2-1)+a$sd2,a$sd2:(a$sd2-1)+a$sd2)) [if not already present]\n",
            "Jacobi relaxation Calculation: 4096 x 4096 mesh\n",
            "libcupti.so not found\n",
            "    0  0.250000\n",
            "  100  0.002397\n",
            "  200  0.001204\n",
            "  300  0.000804\n",
            "  400  0.000603\n",
            "  500  0.000483\n",
            "  600  0.000403\n",
            "  700  0.000345\n",
            "  800  0.000302\n",
            "  900  0.000269\n",
            " completed in      0.771 seconds\n",
            "\n",
            "Accelerator Kernel Timing data\n",
            "/content/lab2/jacobi.f90\n",
            "  jacobi  NVIDIA  devicenum=0\n",
            "    time(us): 21,319\n",
            "    21: data region reached 2 times\n",
            "        21: data copyin transfers: 10\n",
            "             device time(us): total=11,021 max=1,377 min=7 avg=1,102\n",
            "        32: data copyout transfers: 9\n",
            "             device time(us): total=10,298 max=1,291 min=8 avg=1,144\n",
            "/content/lab2/laplace2d.f90\n",
            "  calcnext  NVIDIA  devicenum=0\n",
            "    time(us): 13,735\n",
            "    33: compute region reached 1000 times\n",
            "        33: kernel launched 1000 times\n",
            "            grid: [4094]  block: [128]\n",
            "            elapsed time(us): total=233,131 max=288 min=229 avg=233\n",
            "        33: reduction kernel launched 1000 times\n",
            "            grid: [1]  block: [256]\n",
            "            elapsed time(us): total=31,942 max=6,505 min=22 avg=31\n",
            "    33: data region reached 4000 times\n",
            "        33: data copyin transfers: 1000\n",
            "             device time(us): total=5,074 max=10 min=5 avg=5\n",
            "        41: data copyout transfers: 1000\n",
            "             device time(us): total=8,661 max=23 min=7 avg=8\n",
            "/content/lab2/laplace2d.f90\n",
            "  swap  NVIDIA  devicenum=0\n",
            "    time(us): 0\n",
            "    51: compute region reached 1000 times\n",
            "        51: kernel launched 1000 times\n",
            "            grid: [4094]  block: [128]\n",
            "            elapsed time(us): total=225,714 max=244 min=221 avg=225\n",
            "    51: data region reached 2000 times\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OpenACC Update Directive\n",
        "\n",
        "When we use the data clauses you are only able to copy data between host and device memory at the beginning and end of your regions, but what if you need to copy data in the middle? For example, what if we wanted to debug our code by printing out the array every 100 steps to make sure it looks right. In order to transfer data at those times, we can use the **`update`** directive. The update directive will explicitly transfer data between the host and the device. The **`update`** directive has two clauses:\n",
        "\n",
        "- **`self`** - The self clause will transfer data from the device to the host (GPU to CPU). You will sometimes see this clause called the **`host`** clause.\n",
        "- **`device`** - The device clause will transfer data from the host to the device (CPU to GPU).\n",
        "The syntax would look like:\n",
        "\n",
        "**`!$acc update self(A(1:N))`**\n",
        " \n",
        "**`!$acc update device(A(1:N))`**\n",
        "\n",
        "All of the array shaping rules apply.\n",
        "\n",
        "As an example, let's create a version of our laplace code where we want to print the array A after every 100 iterations of our loop. The code will look like this:\n",
        "```\n",
        "!acc data copyin( A(n,m), Anew(n,m))\n",
        "\n",
        "do while (error > tol && iter < iter_max)\n",
        "    error = calcNext(A, Anew, m, n)\n",
        "    swap(A, Anew, m, n)\n",
        "\n",
        "    if(mod(iter,100).eq.0 ) then\n",
        "      write(*,'(i5,f10.6)'), iter, error\n",
        "      do i=1,n\n",
        "        do j=1,m\n",
        "          write(*,'(f10.2)', advance=\"no\"), A(i,j)\n",
        "        enddo\n",
        "      enddo\n",
        "    end if\n",
        "\n",
        "    iter = iter+1\n",
        "\n",
        "end do\n",
        "\n",
        "!$acc end data\n",
        "```\n",
        "Let's run this code (on a very small data set, so that we don't overload the console by printing thousands of numbers; we've set m = n = 10)."
      ],
      "metadata": {
        "id": "b_2gYDK6EFFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file jacobi.f90\n",
        "\n",
        "\n",
        "program jacobi\n",
        "  use laplace2d\n",
        "  implicit none\n",
        "  integer, parameter :: fp_kind=kind(1.0d0)\n",
        "  integer, parameter :: n=10, m=10, iter_max=1000\n",
        "  integer :: i, j, iter\n",
        "  real(fp_kind), dimension (:,:), allocatable :: A, Anew\n",
        "  real(fp_kind) :: tol=1.0e-6_fp_kind, error=1.0_fp_kind\n",
        "  real(fp_kind) :: start_time, stop_time\n",
        "\n",
        "  call initialize(A, Anew, m, n)\n",
        "   \n",
        "  write(*,'(a,i5,a,i5,a)') 'Jacobi relaxation Calculation:', n, ' x', m, ' mesh'\n",
        " \n",
        "  call cpu_time(start_time) \n",
        "\n",
        "  iter=0\n",
        "  \n",
        "  !$acc data copyin(A, Anew)\n",
        "  do while ( error .gt. tol .and. iter .lt. iter_max )\n",
        "\n",
        "    error = calcNext(A, Anew, m, n)\n",
        "    call swap(A, Anew, m, n)\n",
        "\n",
        "    if(mod(iter,100).eq.0 ) then\n",
        "      write(*,'(i5,f10.6)'), iter, error\n",
        "      do i=1,n \n",
        "        do j=1,m \n",
        "          write(*,'(f10.2)', advance=\"no\"), A(i,j) \n",
        "        enddo \n",
        "      enddo\n",
        "    end if\n",
        "\n",
        "    iter = iter + 1\n",
        "\n",
        "  end do\n",
        "  !$acc end data\n",
        "\n",
        "  call cpu_time(stop_time) \n",
        "  write(*,'(a,f10.3,a)')  ' completed in ', stop_time-start_time, ' seconds'\n",
        "\n",
        "  deallocate (A,Anew)\n",
        "end program jacobi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEQwBj3rETUo",
        "outputId": "352843fa-fa9c-491b-bb49-f9f690ffbe06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting jacobi.f90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file laplace2d.f90\n",
        "\n",
        "module laplace2d\n",
        "  public :: initialize\n",
        "  public :: calcNext\n",
        "  public :: swap\n",
        "  public :: dealloc\n",
        "  contains\n",
        "    subroutine initialize(A, Anew, m, n)\n",
        "      integer, parameter :: fp_kind=kind(1.0d0)\n",
        "      real(fp_kind),allocatable,intent(out)   :: A(:,:)\n",
        "      real(fp_kind),allocatable,intent(out)   :: Anew(:,:)\n",
        "      integer,intent(in)          :: m, n\n",
        "\n",
        "      allocate ( A(0:n-1,0:m-1), Anew(0:n-1,0:m-1) )\n",
        "\n",
        "      A    = 0.0_fp_kind\n",
        "      Anew = 0.0_fp_kind\n",
        "\n",
        "      A(0,:)    = 1.0_fp_kind\n",
        "      Anew(0,:) = 1.0_fp_kind\n",
        "    end subroutine initialize\n",
        "\n",
        "    function calcNext(A, Anew, m, n)\n",
        "      integer, parameter          :: fp_kind=kind(1.0d0)\n",
        "      real(fp_kind),intent(inout) :: A(0:n-1,0:m-1)\n",
        "      real(fp_kind),intent(inout) :: Anew(0:n-1,0:m-1)\n",
        "      integer,intent(in)          :: m, n\n",
        "      integer                     :: i, j\n",
        "      real(fp_kind)               :: error\n",
        "\n",
        "      error=0.0_fp_kind\n",
        "\n",
        "      !$acc parallel loop reduction(max:error)\n",
        "      do j=1,m-2\n",
        "        do i=1,n-2\n",
        "          Anew(i,j) = 0.25_fp_kind * ( A(i+1,j  ) + A(i-1,j  ) + &\n",
        "                                       A(i  ,j-1) + A(i  ,j+1) )\n",
        "          error = max( error, abs(Anew(i,j)-A(i,j)) )\n",
        "        end do\n",
        "      end do\n",
        "      calcNext = error\n",
        "    end function calcNext\n",
        "\n",
        "    subroutine swap(A, Anew, m, n)\n",
        "      integer, parameter        :: fp_kind=kind(1.0d0)\n",
        "      real(fp_kind),intent(out) :: A(0:n-1,0:m-1)\n",
        "      real(fp_kind),intent(in)  :: Anew(0:n-1,0:m-1)\n",
        "      integer,intent(in)        :: m, n\n",
        "      integer                   :: i, j\n",
        "\n",
        "      !$acc parallel loop\n",
        "      do j=1,m-2\n",
        "        do i=1,n-2\n",
        "          A(i,j) = Anew(i,j)\n",
        "        end do\n",
        "      end do\n",
        "    end subroutine swap\n",
        "\n",
        "    subroutine dealloc(A, Anew)\n",
        "      integer, parameter :: fp_kind=kind(1.0d0)\n",
        "      real(fp_kind),allocatable,intent(in) :: A\n",
        "      real(fp_kind),allocatable,intent(in) :: Anew\n",
        "\n",
        "      deallocate (A,Anew)\n",
        "    end subroutine\n",
        "end module laplace2d"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "us6dLfXBF5dF",
        "outputId": "5567aeaf-0ece-4b7f-b68e-42c51dc0b947"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting laplace2d.f90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!make clean && make laplace_no_update && ./laplace_no_update 10 10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j04eDg7EETYB",
        "outputId": "c17ba037-9824-4d62-fad5-f02b51161f46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm -f *.o laplace laplace_*\n",
            "nvfortran -fast -ta=tesla -Minfo=accel -o laplace_no_update laplace2d.f90 jacobi.f90\n",
            "laplace2d.f90:\n",
            "calcnext:\n",
            "     34, Generating NVIDIA GPU code\n",
            "         35, !$acc loop gang ! blockidx%x\n",
            "             Generating reduction(max:error)\n",
            "         36, !$acc loop vector(128) ! threadidx%x\n",
            "     34, Generating implicit copyin(a(:n-1,:m-1)) [if not already present]\n",
            "         Generating implicit copy(error) [if not already present]\n",
            "         Generating implicit copyout(anew(1:n-2,1:m-2)) [if not already present]\n",
            "     36, Loop is parallelizable\n",
            "swap:\n",
            "     52, Generating NVIDIA GPU code\n",
            "         53, !$acc loop gang ! blockidx%x\n",
            "         54, !$acc loop vector(128) ! threadidx%x\n",
            "     52, Generating implicit copyout(a(1:n-2,1:m-2)) [if not already present]\n",
            "         Generating implicit copyin(anew(1:n-2,1:m-2)) [if not already present]\n",
            "     54, Loop is parallelizable\n",
            "jacobi.f90:\n",
            "jacobi:\n",
            "     21, Generating copyin(anew(:,:),a(:,:)) [if not already present]\n",
            "Jacobi relaxation Calculation:   10 x   10 mesh\n",
            "    0  0.250000\n",
            "      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      1.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      1.00      1.00      1.00      1.00      1.00      1.00      1.00      1.00      0.00      0.00  100  0.000046\n",
            "      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      1.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      1.00      1.00      1.00      1.00      1.00      1.00      1.00      1.00      0.00      0.00 completed in      0.173 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VA9aPa3pFe-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file jacobi.f90\n",
        "\n",
        "\n",
        "program jacobi\n",
        "  use laplace2d\n",
        "  implicit none\n",
        "  integer, parameter :: fp_kind=kind(1.0d0)\n",
        "  integer, parameter :: n=10, m=10, iter_max=1000\n",
        "  integer :: i, j, iter\n",
        "  real(fp_kind), dimension (:,:), allocatable :: A, Anew\n",
        "  real(fp_kind) :: tol=1.0e-6_fp_kind, error=1.0_fp_kind\n",
        "  real(fp_kind) :: start_time, stop_time\n",
        "\n",
        "  allocate ( A(0:n-1,0:m-1), Anew(0:n-1,0:m-1) )\n",
        "\n",
        "  call initialize(A, Anew, m, n)\n",
        "   \n",
        "  write(*,'(a,i5,a,i5,a)') 'Jacobi relaxation Calculation:', n, ' x', m, ' mesh'\n",
        " \n",
        "  call cpu_time(start_time) \n",
        "\n",
        "  iter=0\n",
        "  \n",
        "  !$acc data copyin(A, Anew)\n",
        "  do while ( error .gt. tol .and. iter .lt. iter_max )\n",
        "\n",
        "    error = calcNext(A, Anew, m, n)\n",
        "    call swap(A, Anew, m, n)\n",
        "\n",
        "    if(mod(iter,100).eq.0 ) then\n",
        "      write(*,'(i5,f10.6)'), iter, error\n",
        "      !$acc update self(A)\n",
        "      do i=1,n \n",
        "        do j=1,m \n",
        "          write(*,'(f10.2)', advance=\"no\"), A(i,j) \n",
        "        enddo \n",
        "      enddo\n",
        "    end if\n",
        "\n",
        "    iter = iter + 1\n",
        "\n",
        "  end do\n",
        "  !$acc end data\n",
        "\n",
        "  call cpu_time(stop_time) \n",
        "  write(*,'(a,f10.3,a)')  ' completed in ', stop_time-start_time, ' seconds'\n",
        "\n",
        "  deallocate (A,Anew)\n",
        "end program jacobi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JD0QND6FfK8",
        "outputId": "ad8deacf-1345-47a4-830b-82675253ac92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting jacobi.f90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!  make clean && make laplace_update && ./laplace_update 10 10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "id8S6XntGn7P",
        "outputId": "e732e305-5499-4a06-a586-5a951d954efe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm -f *.o laplace laplace_*\n",
            "nvfortran -fast -ta=tesla -Minfo=accel -o laplace_update laplace2d.f90 jacobi.f90\n",
            "laplace2d.f90:\n",
            "calcnext:\n",
            "     34, Generating NVIDIA GPU code\n",
            "         35, !$acc loop gang ! blockidx%x\n",
            "             Generating reduction(max:error)\n",
            "         36, !$acc loop vector(128) ! threadidx%x\n",
            "     34, Generating implicit copyin(a(:n-1,:m-1)) [if not already present]\n",
            "         Generating implicit copy(error) [if not already present]\n",
            "         Generating implicit copyout(anew(1:n-2,1:m-2)) [if not already present]\n",
            "     36, Loop is parallelizable\n",
            "swap:\n",
            "     52, Generating NVIDIA GPU code\n",
            "         53, !$acc loop gang ! blockidx%x\n",
            "         54, !$acc loop vector(128) ! threadidx%x\n",
            "     52, Generating implicit copyout(a(1:n-2,1:m-2)) [if not already present]\n",
            "         Generating implicit copyin(anew(1:n-2,1:m-2)) [if not already present]\n",
            "     54, Loop is parallelizable\n",
            "jacobi.f90:\n",
            "jacobi:\n",
            "     23, Generating copyin(a(:,:),anew(:,:)) [if not already present]\n",
            "     31, Generating update self(a(:,:))\n",
            "Jacobi relaxation Calculation:   10 x   10 mesh\n",
            "    0  0.250000\n",
            "      0.25      0.25      0.25      0.25      0.25      0.25      0.25      0.25      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      1.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      1.00      1.00      1.00      1.00      1.00      1.00      1.00      1.00      0.00      0.00  100  0.000046\n",
            "      0.49      0.67      0.74      0.77      0.77      0.74      0.67      0.49      0.00      0.00      0.28      0.45      0.54      0.58      0.58      0.54      0.45      0.28      0.00      0.00      0.17      0.30      0.38      0.42      0.42      0.38      0.30      0.17      0.00      0.00      0.11      0.20      0.26      0.29      0.29      0.26      0.20      0.11      0.00      1.00      0.07      0.14      0.18      0.20      0.20      0.18      0.14      0.07      0.00      0.00      0.05      0.09      0.12      0.14      0.14      0.12      0.09      0.05      0.00      0.00      0.03      0.05      0.07      0.08      0.08      0.07      0.05      0.03      0.00      0.00      0.01      0.03      0.03      0.04      0.04      0.03      0.03      0.01      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00      1.00      1.00      1.00      1.00      1.00      1.00      1.00      1.00      0.00      0.00 completed in      0.165 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JlZLZV17KcCI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter3 profiling\n",
        "\n",
        "<img src=\"https://www.nvidia.com/content/dam/en-zz/Solutions/about-nvidia/logo-and-brand/02-nvidia-logo-color-wht-500x200-4c25-d@2x.png\" height=\"100\"></td></tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "Kwo2hlPxrtg5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CPU profling with gprof"
      ],
      "metadata": {
        "id": "cSB-UEwhsBKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/lab3 "
      ],
      "metadata": {
        "id": "2W34iA9zG5uG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/lab3 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXZu5GkKHAQa",
        "outputId": "82fd15ae-5860-4d11-d093-d283be1d0136"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/lab3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file test_gprof.c\n",
        "#include<stdio.h>\n",
        "\n",
        "\n",
        "void subroutine_c(void)\n",
        "{\n",
        "    printf(\"\\n   Inside subroutine_c()\\n\");\n",
        "    int i = 0;\n",
        "\n",
        "    for(;i<200000000;i++);\n",
        "\n",
        "    return;\n",
        "}\n",
        "\n",
        "\n",
        "void subroutine_d(void)\n",
        "{\n",
        "    printf(\"\\n   Inside subroutine_d()\\n\");\n",
        "    int i = 0;\n",
        "\n",
        "    for(;i<800000000;i++);\n",
        "\n",
        "    return;\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "void func1(void)\n",
        "{\n",
        "    printf(\"\\n Inside func1 \\n\");\n",
        "    int i = 0;\n",
        "\n",
        "    for(;i<400000000;i++);\n",
        " \n",
        "    subroutine_c();\n",
        "    subroutine_c(); \n",
        "\n",
        "    return;\n",
        "}\n",
        "\n",
        "static void func2(void)\n",
        "{\n",
        "    printf(\"\\n Inside func2 \\n\");\n",
        "    int i = 0;\n",
        "\n",
        "    for(;i<800000000;i++);\n",
        "    subroutine_d(); \n",
        "    return;\n",
        "}\n",
        "\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "    printf(\"\\n Inside main()\\n\");\n",
        "    int i = 0;\n",
        "\n",
        "    for(;i<800000000;i++);\n",
        "    func1();\n",
        "    func2();\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExD6u5VbtYKp",
        "outputId": "d29a8457-3c61-437e-c836-22ff9fc459de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test_gprof.c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvc -pg test_gprof.c -o a.out_profile"
      ],
      "metadata": {
        "id": "2Y7D1-V8sBiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./a.out_profile"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqF2eDHHsBti",
        "outputId": "e7b76bd5-e8f3-4eb1-f7f6-6e9df4728f79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Inside main()\n",
            "\n",
            " Inside func1 \n",
            "\n",
            "   Inside subroutine_c()\n",
            "\n",
            "   Inside subroutine_c()\n",
            "\n",
            " Inside func2 \n",
            "\n",
            "   Inside subroutine_d()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -alh gmon.out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gQlSn_OsVj_",
        "outputId": "ae4ab379-24d2-4fdd-b9ee-639a11aebcc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 713 Sep  1 08:13 gmon.out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gprof a.out_profile gmon.out > result.txt"
      ],
      "metadata": {
        "id": "3rmk18t1sZY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat result.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMHJgnLJsjRf",
        "outputId": "81eda5dc-500f-4ff9-e6e4-08b15be693c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flat profile:\n",
            "\n",
            "Each sample counts as 0.01 seconds.\n",
            "  %   cumulative   self              self     total           \n",
            " time   seconds   seconds    calls   s/call   s/call  name    \n",
            " 25.13      1.56     1.56        1     1.56     1.56  subroutine_d\n",
            " 25.13      3.11     1.56                             main\n",
            " 24.97      4.66     1.55        1     1.55     3.10  func2\n",
            " 12.65      5.44     0.78        2     0.39     0.39  subroutine_c\n",
            " 12.48      6.21     0.77        1     0.77     1.56  func1\n",
            "\n",
            " %         the percentage of the total running time of the\n",
            "time       program used by this function.\n",
            "\n",
            "cumulative a running sum of the number of seconds accounted\n",
            " seconds   for by this function and those listed above it.\n",
            "\n",
            " self      the number of seconds accounted for by this\n",
            "seconds    function alone.  This is the major sort for this\n",
            "           listing.\n",
            "\n",
            "calls      the number of times this function was invoked, if\n",
            "           this function is profiled, else blank.\n",
            "\n",
            " self      the average number of milliseconds spent in this\n",
            "ms/call    function per call, if this function is profiled,\n",
            "\t   else blank.\n",
            "\n",
            " total     the average number of milliseconds spent in this\n",
            "ms/call    function and its descendents per call, if this\n",
            "\t   function is profiled, else blank.\n",
            "\n",
            "name       the name of the function.  This is the minor sort\n",
            "           for this listing. The index shows the location of\n",
            "\t   the function in the gprof listing. If the index is\n",
            "\t   in parenthesis it shows where it would appear in\n",
            "\t   the gprof listing if it were to be printed.\n",
            "\f\n",
            "Copyright (C) 2012-2018 Free Software Foundation, Inc.\n",
            "\n",
            "Copying and distribution of this file, with or without modification,\n",
            "are permitted in any medium without royalty provided the copyright\n",
            "notice and this notice are preserved.\n",
            "\f\n",
            "\t\t     Call graph (explanation follows)\n",
            "\n",
            "\n",
            "granularity: each sample hit covers 2 byte(s) for 0.16% of 6.21 seconds\n",
            "\n",
            "index % time    self  children    called     name\n",
            "                                                 <spontaneous>\n",
            "[1]    100.0    1.56    4.66                 main [1]\n",
            "                1.55    1.56       1/1           func2 [2]\n",
            "                0.77    0.78       1/1           func1 [3]\n",
            "-----------------------------------------------\n",
            "                1.55    1.56       1/1           main [1]\n",
            "[2]     49.9    1.55    1.56       1         func2 [2]\n",
            "                1.56    0.00       1/1           subroutine_d [4]\n",
            "-----------------------------------------------\n",
            "                0.77    0.78       1/1           main [1]\n",
            "[3]     25.0    0.77    0.78       1         func1 [3]\n",
            "                0.78    0.00       2/2           subroutine_c [5]\n",
            "-----------------------------------------------\n",
            "                1.56    0.00       1/1           func2 [2]\n",
            "[4]     25.0    1.56    0.00       1         subroutine_d [4]\n",
            "-----------------------------------------------\n",
            "                0.78    0.00       2/2           func1 [3]\n",
            "[5]     12.6    0.78    0.00       2         subroutine_c [5]\n",
            "-----------------------------------------------\n",
            "\n",
            " This table describes the call tree of the program, and was sorted by\n",
            " the total amount of time spent in each function and its children.\n",
            "\n",
            " Each entry in this table consists of several lines.  The line with the\n",
            " index number at the left hand margin lists the current function.\n",
            " The lines above it list the functions that called this function,\n",
            " and the lines below it list the functions this one called.\n",
            " This line lists:\n",
            "     index\tA unique number given to each element of the table.\n",
            "\t\tIndex numbers are sorted numerically.\n",
            "\t\tThe index number is printed next to every function name so\n",
            "\t\tit is easier to look up where the function is in the table.\n",
            "\n",
            "     % time\tThis is the percentage of the `total' time that was spent\n",
            "\t\tin this function and its children.  Note that due to\n",
            "\t\tdifferent viewpoints, functions excluded by options, etc,\n",
            "\t\tthese numbers will NOT add up to 100%.\n",
            "\n",
            "     self\tThis is the total amount of time spent in this function.\n",
            "\n",
            "     children\tThis is the total amount of time propagated into this\n",
            "\t\tfunction by its children.\n",
            "\n",
            "     called\tThis is the number of times the function was called.\n",
            "\t\tIf the function called itself recursively, the number\n",
            "\t\tonly includes non-recursive calls, and is followed by\n",
            "\t\ta `+' and the number of recursive calls.\n",
            "\n",
            "     name\tThe name of the current function.  The index number is\n",
            "\t\tprinted after it.  If the function is a member of a\n",
            "\t\tcycle, the cycle number is printed between the\n",
            "\t\tfunction's name and the index number.\n",
            "\n",
            "\n",
            " For the function's parents, the fields have the following meanings:\n",
            "\n",
            "     self\tThis is the amount of time that was propagated directly\n",
            "\t\tfrom the function into this parent.\n",
            "\n",
            "     children\tThis is the amount of time that was propagated from\n",
            "\t\tthe function's children into this parent.\n",
            "\n",
            "     called\tThis is the number of times this parent called the\n",
            "\t\tfunction `/' the total number of times the function\n",
            "\t\twas called.  Recursive calls to the function are not\n",
            "\t\tincluded in the number after the `/'.\n",
            "\n",
            "     name\tThis is the name of the parent.  The parent's index\n",
            "\t\tnumber is printed after it.  If the parent is a\n",
            "\t\tmember of a cycle, the cycle number is printed between\n",
            "\t\tthe name and the index number.\n",
            "\n",
            " If the parents of the function cannot be determined, the word\n",
            " `<spontaneous>' is printed in the `name' field, and all the other\n",
            " fields are blank.\n",
            "\n",
            " For the function's children, the fields have the following meanings:\n",
            "\n",
            "     self\tThis is the amount of time that was propagated directly\n",
            "\t\tfrom the child into the function.\n",
            "\n",
            "     children\tThis is the amount of time that was propagated from the\n",
            "\t\tchild's children to the function.\n",
            "\n",
            "     called\tThis is the number of times the function called\n",
            "\t\tthis child `/' the total number of times the child\n",
            "\t\twas called.  Recursive calls by the child are not\n",
            "\t\tlisted in the number after the `/'.\n",
            "\n",
            "     name\tThis is the name of the child.  The child's index\n",
            "\t\tnumber is printed after it.  If the child is a\n",
            "\t\tmember of a cycle, the cycle number is printed\n",
            "\t\tbetween the name and the index number.\n",
            "\n",
            " If there are any cycles (circles) in the call graph, there is an\n",
            " entry for the cycle-as-a-whole.  This entry shows who called the\n",
            " cycle (as parents) and the members of the cycle (as children.)\n",
            " The `+' recursive calls entry shows the number of function calls that\n",
            " were internal to the cycle, and the calls entry for each member shows,\n",
            " for that member, how many times it was called from other members of\n",
            " the cycle.\n",
            "\f\n",
            "Copyright (C) 2012-2018 Free Software Foundation, Inc.\n",
            "\n",
            "Copying and distribution of this file, with or without modification,\n",
            "are permitted in any medium without royalty provided the copyright\n",
            "notice and this notice are preserved.\n",
            "\f\n",
            "Index by function name\n",
            "\n",
            "   [3] func1                   [1] main                    [4] subroutine_d\n",
            "   [2] func2 (test_gprof.c)    [5] subroutine_c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## nvtx for host  code profile in C/C++"
      ],
      "metadata": {
        "id": "59FqEtfa3pQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file test_gprof.c\n",
        "#include<stdio.h>\n",
        "#include <nvtx3/nvToolsExt.h>\n",
        "\n",
        "\n",
        "void subroutine_c(void)\n",
        "{\n",
        "    printf(\"\\n   Inside subroutine_c()\\n\");\n",
        "    int i = 0;\n",
        "\n",
        "    for(;i<200000000;i++);\n",
        "\n",
        "    return;\n",
        "}\n",
        "\n",
        "\n",
        "void subroutine_d(void)\n",
        "{\n",
        "    printf(\"\\n   Inside subroutine_d()\\n\");\n",
        "    int i = 0;\n",
        "\n",
        "    for(;i<800000000;i++);\n",
        "\n",
        "    return;\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "void func1(void)\n",
        "{\n",
        "    printf(\"\\n Inside func1 \\n\");\n",
        "    int i = 0;\n",
        "\n",
        "    for(;i<400000000;i++);\n",
        " \n",
        "    subroutine_c();\n",
        "    subroutine_c(); \n",
        "\n",
        "    return;\n",
        "}\n",
        "\n",
        "static void func2(void)\n",
        "{\n",
        "    printf(\"\\n Inside func2 \\n\");\n",
        "    int i = 0;\n",
        "\n",
        "    for(;i<800000000;i++);\n",
        "    subroutine_d(); \n",
        "    return;\n",
        "}\n",
        "\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "    printf(\"\\n Inside main()\\n\");\n",
        "    int i = 0;\n",
        "    nvtxRangePush(\"before\"); \n",
        "    for(;i<800000000;i++);\n",
        "    nvtxRangePop(); \n",
        " \n",
        "     nvtxRangePushA(\"fucn1\");\n",
        "    func1();\n",
        "     nvtxRangePop(); \n",
        " \n",
        "    nvtxRangePushA(\"fucn2\");\n",
        "    func2();\n",
        "    nvtxRangePop();  \n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXX38GVh4m85",
        "outputId": "ca6748b8-4537-4d40-e042-39e876a5f7bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting test_gprof.c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvc  test_gprof.c -o a.out_profile -I/opt/nvidia/hpc_sdk/Linux_x86_64/22.7/cuda/11.7/include  -L/opt/nvidia/hpc_sdk/Linux_x86_64/22.7/cuda/11.7/lib64 -lnvToolsExt"
      ],
      "metadata": {
        "id": "wAzz7U1A3vwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nsys profile -t nvtx  --stats=true ./a.out_profile"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UYA22as3wHw",
        "outputId": "2909d1a3-cf70-49b5-99cf-c90fb6f7b9b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
            "\n",
            " Inside main()\n",
            "\n",
            " Inside func1 \n",
            "\n",
            "   Inside subroutine_c()\n",
            "\n",
            "   Inside subroutine_c()\n",
            "\n",
            " Inside func2 \n",
            "\n",
            "   Inside subroutine_d()\n",
            "Generating '/tmp/nsys-report-2031.qdstrm'\n",
            "[1/3] [========================100%] report2.nsys-rep\n",
            "[2/3] [========================100%] report2.sqlite\n",
            "[3/3] Executing 'nvtxsum' stats report\n",
            "\n",
            "NVTX Range Statistics:\n",
            "\n",
            " Time (%)  Total Time (ns)  Instances     Avg (ns)         Med (ns)        Min (ns)       Max (ns)     StdDev (ns)   Style   Range \n",
            " --------  ---------------  ---------  ---------------  ---------------  -------------  -------------  -----------  -------  ------\n",
            "     50.0    3,188,182,902          1  3,188,182,902.0  3,188,182,902.0  3,188,182,902  3,188,182,902          0.0  PushPop  fucn2 \n",
            "     25.0    1,594,219,093          1  1,594,219,093.0  1,594,219,093.0  1,594,219,093  1,594,219,093          0.0  PushPop  fucn1 \n",
            "     25.0    1,592,995,529          1  1,592,995,529.0  1,592,995,529.0  1,592,995,529  1,592,995,529          0.0  PushPop  before\n",
            "\n",
            "Generated:\n",
            "    /content/lab3/report2.nsys-rep\n",
            "    /content/lab3/report2.sqlite\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## nvtx for fortran "
      ],
      "metadata": {
        "id": "dxezmp_BIkr_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NTScYP5wIuzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file nvtx.f90\n",
        "\n",
        "module nvtx\n",
        "\n",
        "use iso_c_binding\n",
        "implicit none\n",
        "\n",
        "integer,private :: col(7) = [ Z'0000ff00', Z'000000ff', Z'00ffff00', Z'00ff00ff', Z'0000ffff', Z'00ff0000', Z'00ffffff']\n",
        "character,private,target :: tempName(256)\n",
        "\n",
        "type, bind(C):: nvtxEventAttributes\n",
        "  integer(C_INT16_T):: version=1\n",
        "  integer(C_INT16_T):: size=48 !\n",
        "  integer(C_INT):: category=0\n",
        "  integer(C_INT):: colorType=1 ! NVTX_COLOR_ARGB = 1\n",
        "  integer(C_INT):: color\n",
        "  integer(C_INT):: payloadType=0 ! NVTX_PAYLOAD_UNKNOWN = 0\n",
        "  integer(C_INT):: reserved0\n",
        "  integer(C_INT64_T):: payload   ! union uint,int,double\n",
        "  integer(C_INT):: messageType=1  ! NVTX_MESSAGE_TYPE_ASCII     = 1 \n",
        "  type(C_PTR):: message  ! ascii char\n",
        "end type\n",
        "\n",
        "interface nvtxRangePush\n",
        "  ! push range with custom label and standard color\n",
        "  subroutine nvtxRangePushA(name) bind(C, name='nvtxRangePushA')\n",
        "  use iso_c_binding\n",
        "  character(kind=C_CHAR) :: name(256)\n",
        "  end subroutine\n",
        "\n",
        "  ! push range with custom label and custom color\n",
        "  subroutine nvtxRangePushEx(event) bind(C, name='nvtxRangePushEx')\n",
        "  use iso_c_binding\n",
        "  import:: nvtxEventAttributes\n",
        "  type(nvtxEventAttributes):: event\n",
        "  end subroutine\n",
        "end interface\n",
        "\n",
        "interface nvtxRangePop\n",
        "  subroutine nvtxRangePop() bind(C, name='nvtxRangePop')\n",
        "  end subroutine\n",
        "end interface\n",
        "\n",
        "contains\n",
        "\n",
        "subroutine nvtxStartRange(name,id)\n",
        "  character(kind=c_char,len=*) :: name\n",
        "  integer, optional:: id\n",
        "  type(nvtxEventAttributes):: event\n",
        "  character(kind=c_char,len=256) :: trimmed_name\n",
        "  integer:: i\n",
        "\n",
        "  trimmed_name=trim(name)//c_null_char\n",
        "\n",
        "  ! move scalar trimmed_name into character array tempName\n",
        "  do i=1,LEN(trim(name)) + 1\n",
        "     tempName(i) = trimmed_name(i:i)\n",
        "  enddo\n",
        "\n",
        "\n",
        "  if ( .not. present(id)) then\n",
        "    call nvtxRangePush(tempName)\n",
        "  else\n",
        "    event%color=col(mod(id,7)+1)\n",
        "    event%message=c_loc(tempName)\n",
        "    call nvtxRangePushEx(event)\n",
        "  end if\n",
        "end subroutine\n",
        "\n",
        "subroutine nvtxEndRange\n",
        "  call nvtxRangePop\n",
        "end subroutine\n",
        "\n",
        "end module nvtx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgCgKEADIv3l",
        "outputId": "6856955d-5ba3-45e8-da5e-c61d9316d65d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing nvtx.f90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file main.f90\n",
        "program main\n",
        "  use nvtx\n",
        "  character(len=4) :: itcount\n",
        "\n",
        "  ! First range with standard color\n",
        "  call nvtxStartRange(\"First label\")\n",
        "\n",
        "  do n=1,14\n",
        "    ! Create custom label for each marker\n",
        "    write(itcount,'(i4)') n\n",
        "\n",
        "    ! Range with custom  color\n",
        "    call nvtxStartRange(\"Label \"//itcount,n)\n",
        "\n",
        "    print *,\"Generate label\",n\n",
        "    ! Add sleep to make markers big \n",
        "    call sleep(1)\n",
        "\n",
        "    call nvtxEndRange\n",
        "  end do\n",
        "\n",
        "  call nvtxEndRange\n",
        "end program main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuaJlgyhI3nl",
        "outputId": "c14d150e-541a-408f-b54f-9d94281f10d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing main.f90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file Makefile\n",
        "\n",
        "all: nvtx_example\n",
        "\n",
        "FC=pgf90\n",
        "#FC=gfortran\n",
        "\n",
        "nvtx_example:: main.f90 nvtx.f90\n",
        "\t$(FC) -o nvtx_example nvtx.f90 main.f90 -L/opt/nvidia/hpc_sdk/Linux_x86_64/22.7/cuda/11.7/lib64  -lnvToolsExt\n",
        "\n",
        "clean:\n",
        "\trm  nvtx_example main.o nvtx.o   nvtx.mod"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uzm-xnVdJAnd",
        "outputId": "4aadc5b7-e09e-4059-84e4-ffc9be46a9e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting Makefile\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!make  nvtx_example && ./nvtx_example"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SO4YT9f3JOVN",
        "outputId": "e8b4c486-cd7a-4c1d-e61e-2c68d000ff07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "make: 'nvtx_example' is up to date.\n",
            " Generate label            1\n",
            " Generate label            2\n",
            " Generate label            3\n",
            " Generate label            4\n",
            " Generate label            5\n",
            " Generate label            6\n",
            " Generate label            7\n",
            " Generate label            8\n",
            " Generate label            9\n",
            " Generate label           10\n",
            " Generate label           11\n",
            " Generate label           12\n",
            " Generate label           13\n",
            " Generate label           14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3pvY0EnVHcna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nsys profile -t nvtx --stats=true   ./nvtx_example"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rK7xD-qDHcwm",
        "outputId": "63a02da2-94ce-485c-943e-556eb40ed06b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
            " Generate label            1\n",
            " Generate label            2\n",
            " Generate label            3\n",
            " Generate label            4\n",
            " Generate label            5\n",
            " Generate label            6\n",
            " Generate label            7\n",
            " Generate label            8\n",
            " Generate label            9\n",
            " Generate label           10\n",
            " Generate label           11\n",
            " Generate label           12\n",
            " Generate label           13\n",
            " Generate label           14\n",
            "Generating '/tmp/nsys-report-6fe0.qdstrm'\n",
            "[1/3] [========================100%] report3.nsys-rep\n",
            "[2/3] [========================100%] report3.sqlite\n",
            "[3/3] Executing 'nvtxsum' stats report\n",
            "\n",
            "NVTX Range Statistics:\n",
            "\n",
            " Time (%)  Total Time (ns)  Instances      Avg (ns)          Med (ns)         Min (ns)        Max (ns)     StdDev (ns)   Style      Range   \n",
            " --------  ---------------  ---------  ----------------  ----------------  --------------  --------------  -----------  -------  -----------\n",
            "     50.0   14,002,437,519          1  14,002,437,519.0  14,002,437,519.0  14,002,437,519  14,002,437,519          0.0  PushPop  First label\n",
            "      3.6    1,000,223,645          1   1,000,223,645.0   1,000,223,645.0   1,000,223,645   1,000,223,645          0.0  PushPop  Label    9 \n",
            "      3.6    1,000,133,423          1   1,000,133,423.0   1,000,133,423.0   1,000,133,423   1,000,133,423          0.0  PushPop  Label    7 \n",
            "      3.6    1,000,133,167          1   1,000,133,167.0   1,000,133,167.0   1,000,133,167   1,000,133,167          0.0  PushPop  Label   12 \n",
            "      3.6    1,000,131,639          1   1,000,131,639.0   1,000,131,639.0   1,000,131,639   1,000,131,639          0.0  PushPop  Label    1 \n",
            "      3.6    1,000,129,360          1   1,000,129,360.0   1,000,129,360.0   1,000,129,360   1,000,129,360          0.0  PushPop  Label    4 \n",
            "      3.6    1,000,118,910          1   1,000,118,910.0   1,000,118,910.0   1,000,118,910   1,000,118,910          0.0  PushPop  Label    3 \n",
            "      3.6    1,000,118,717          1   1,000,118,717.0   1,000,118,717.0   1,000,118,717   1,000,118,717          0.0  PushPop  Label   10 \n",
            "      3.6    1,000,117,702          1   1,000,117,702.0   1,000,117,702.0   1,000,117,702   1,000,117,702          0.0  PushPop  Label    8 \n",
            "      3.6    1,000,117,077          1   1,000,117,077.0   1,000,117,077.0   1,000,117,077   1,000,117,077          0.0  PushPop  Label   13 \n",
            "      3.6    1,000,117,072          1   1,000,117,072.0   1,000,117,072.0   1,000,117,072   1,000,117,072          0.0  PushPop  Label    2 \n",
            "      3.6    1,000,112,611          1   1,000,112,611.0   1,000,112,611.0   1,000,112,611   1,000,112,611          0.0  PushPop  Label   14 \n",
            "      3.6    1,000,112,199          1   1,000,112,199.0   1,000,112,199.0   1,000,112,199   1,000,112,199          0.0  PushPop  Label    5 \n",
            "      3.6    1,000,111,971          1   1,000,111,971.0   1,000,111,971.0   1,000,111,971   1,000,111,971          0.0  PushPop  Label   11 \n",
            "      3.6    1,000,111,845          1   1,000,111,845.0   1,000,111,845.0   1,000,111,845   1,000,111,845          0.0  PushPop  Label    6 \n",
            "\n",
            "Generated:\n",
            "    /content/lab3/report3.nsys-rep\n",
            "    /content/lab3/report3.sqlite\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://github.com/openhackathons-org/gpubootcamp/raw/5c077a6c70989492f4bd850240fdcaf8b16fd555/hpc/openacc/English/Fortran/jupyter_notebook/images/development-cycle.png\" width=600>"
      ],
      "metadata": {
        "id": "N2492gBwLZP6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## prepare baseline code"
      ],
      "metadata": {
        "id": "3Tqlzxf4K20r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file Makefile\n",
        "\n",
        "FC := nvfortran\n",
        "ACCFLAGS:= -fast -ta=tesla -Minfo=accel\n",
        "\n",
        "laplace: laplace2d.f90 jacobi.f90\n",
        "\t${FC} ${ACCFLAGS} -o laplace laplace2d.f90 jacobi.f90\n",
        "\n",
        "clean:\n",
        "\trm -f *.o laplace "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "so0T1MtUK75g",
        "outputId": "a9fc9888-60a6-4018-9d31-b011c4ddca53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting Makefile\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file jacobi.f90\n",
        "\n",
        "\n",
        "program jacobi\n",
        "  use laplace2d\n",
        "  implicit none\n",
        "  integer, parameter :: fp_kind=kind(1.0d0)\n",
        "  integer, parameter :: n=4096, m=4096, iter_max=1000\n",
        "  integer :: i, j, iter\n",
        "  real(fp_kind), dimension (:,:), allocatable :: A, Anew\n",
        "  real(fp_kind) :: tol=1.0e-6_fp_kind, error=1.0_fp_kind\n",
        "  real(fp_kind) :: start_time, stop_time\n",
        "\n",
        "  ! allocate ( A(0:n-1,0:m-1), Anew(0:n-1,0:m-1) )\n",
        "\n",
        "  ! A    = 0.0_fp_kind\n",
        "  ! Anew = 0.0_fp_kind\n",
        "\n",
        "  ! Set B.C.\n",
        "  ! A(0,:)    = 1.0_fp_kind\n",
        "  ! Anew(0,:) = 1.0_fp_kind\n",
        "  \n",
        "  call initialize(A, Anew, m, n)\n",
        "   \n",
        "  write(*,'(a,i5,a,i5,a)') 'Jacobi relaxation Calculation:', n, ' x', m, ' mesh'\n",
        " \n",
        "  call cpu_time(start_time) \n",
        "\n",
        "  iter=0\n",
        "  \n",
        "  !$acc data copy(A) create(Anew)\n",
        "  do while ( error .gt. tol .and. iter .lt. iter_max )\n",
        "\n",
        "    error = calcNext(A, Anew, m, n)\n",
        "    call swap(A, Anew, m, n)\n",
        "\n",
        "    if(mod(iter,100).eq.0 ) write(*,'(i5,f10.6)'), iter, error\n",
        "\n",
        "    iter = iter + 1\n",
        "\n",
        "  end do\n",
        "  !$acc end data\n",
        "\n",
        "  call cpu_time(stop_time) \n",
        "  write(*,'(a,f10.3,a)')  ' completed in ', stop_time-start_time, ' seconds'\n",
        "\n",
        "  ! deallocate (A,Anew)\n",
        "end program jacobi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQXwWDTvLF_g",
        "outputId": "f095b105-84ca-4709-8c67-151c01a0bd64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting jacobi.f90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file laplace2d.f90 \n",
        "module laplace2d\n",
        "  public :: initialize\n",
        "  public :: calcNext\n",
        "  public :: swap\n",
        "  public :: dealloc\n",
        "  contains\n",
        "    subroutine initialize(A, Anew, m, n)\n",
        "      integer, parameter :: fp_kind=kind(1.0d0)\n",
        "      real(fp_kind),allocatable,intent(out)   :: A(:,:)\n",
        "      real(fp_kind),allocatable,intent(out)   :: Anew(:,:)\n",
        "      integer,intent(in)          :: m, n\n",
        "\n",
        "      allocate ( A(0:n-1,0:m-1), Anew(0:n-1,0:m-1) )\n",
        "\n",
        "      A    = 0.0_fp_kind\n",
        "      Anew = 0.0_fp_kind\n",
        "\n",
        "      A(0,:)    = 1.0_fp_kind\n",
        "      Anew(0,:) = 1.0_fp_kind\n",
        "    end subroutine initialize\n",
        "\n",
        "    function calcNext(A, Anew, m, n)\n",
        "      integer, parameter          :: fp_kind=kind(1.0d0)\n",
        "      real(fp_kind),intent(inout) :: A(0:n-1,0:m-1)\n",
        "      real(fp_kind),intent(inout) :: Anew(0:n-1,0:m-1)\n",
        "      integer,intent(in)          :: m, n\n",
        "      integer                     :: i, j\n",
        "      real(fp_kind)               :: error\n",
        "\n",
        "      error=0.0_fp_kind\n",
        "\n",
        "      !$acc parallel loop reduction(max:error) copyin(A) copyout(Anew)\n",
        "      do j=1,m-2\n",
        "        do i=1,n-2\n",
        "          Anew(i,j) = 0.25_fp_kind * ( A(i+1,j  ) + A(i-1,j  ) + &\n",
        "                                       A(i  ,j-1) + A(i  ,j+1) )\n",
        "          error = max( error, abs(Anew(i,j)-A(i,j)) )\n",
        "        end do\n",
        "      end do\n",
        "      calcNext = error\n",
        "    end function calcNext\n",
        "\n",
        "    subroutine swap(A, Anew, m, n)\n",
        "      integer, parameter        :: fp_kind=kind(1.0d0)\n",
        "      real(fp_kind),intent(out) :: A(0:n-1,0:m-1)\n",
        "      real(fp_kind),intent(in)  :: Anew(0:n-1,0:m-1)\n",
        "      integer,intent(in)        :: m, n\n",
        "      integer                   :: i, j\n",
        "\n",
        "      !$acc parallel loop copyin(Anew) copyout(A)\n",
        "      do j=1,m-2\n",
        "        do i=1,n-2\n",
        "          A(i,j) = Anew(i,j)\n",
        "        end do\n",
        "      end do\n",
        "    end subroutine swap\n",
        "\n",
        "    subroutine dealloc(A, Anew)\n",
        "      integer, parameter :: fp_kind=kind(1.0d0)\n",
        "      real(fp_kind),allocatable,intent(in) :: A\n",
        "      real(fp_kind),allocatable,intent(in) :: Anew\n",
        "\t  \n",
        "\t  deallocate (A,Anew)\n",
        "    end subroutine\n",
        "end module laplace2d"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSGypKgbLOqE",
        "outputId": "bd181c26-2a46-4859-d3f1-4d2d4acd4c2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing laplace2d.f90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!make clean && make && ./laplace"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UWOfao-Lk84",
        "outputId": "5bd2ce77-a631-4fcb-efa8-4e3ca58a1451"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm -f *.o laplace \n",
            "nvfortran -fast -ta=tesla -Minfo=accel -o laplace laplace2d.f90 jacobi.f90\n",
            "laplace2d.f90:\n",
            "calcnext:\n",
            "     32, Generating copyin(a(:,:)) [if not already present]\n",
            "         Generating NVIDIA GPU code\n",
            "         33, !$acc loop gang ! blockidx%x\n",
            "             Generating reduction(max:error)\n",
            "         34, !$acc loop vector(128) ! threadidx%x\n",
            "     32, Generating implicit copy(error) [if not already present]\n",
            "         Generating copyout(anew(:,:)) [if not already present]\n",
            "     34, Loop is parallelizable\n",
            "swap:\n",
            "     50, Generating copyout(a(:,:)) [if not already present]\n",
            "         Generating copyin(anew(:,:)) [if not already present]\n",
            "         Generating NVIDIA GPU code\n",
            "         51, !$acc loop gang ! blockidx%x\n",
            "         52, !$acc loop vector(128) ! threadidx%x\n",
            "     52, Loop is parallelizable\n",
            "jacobi.f90:\n",
            "jacobi:\n",
            "     30, Generating create(anew(:,:)) [if not already present]\n",
            "         Generating copy(a(:,:)) [if not already present]\n",
            "Jacobi relaxation Calculation: 4096 x 4096 mesh\n",
            "    0  0.250000\n",
            "  100  0.002397\n",
            "  200  0.001204\n",
            "  300  0.000804\n",
            "  400  0.000603\n",
            "  500  0.000483\n",
            "  600  0.000403\n",
            "  700  0.000345\n",
            "  800  0.000302\n",
            "  900  0.000269\n",
            " completed in      0.704 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!make clean && make &&   PGI_ACC_TIME=1 ./laplace "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKXliUIuL7vA",
        "outputId": "9b76dc81-b991-4827-8021-a012c84dfa17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm -f *.o laplace \n",
            "nvfortran -fast -ta=tesla -Minfo=accel -o laplace laplace2d.f90 jacobi.f90\n",
            "laplace2d.f90:\n",
            "calcnext:\n",
            "     32, Generating copyin(a(:,:)) [if not already present]\n",
            "         Generating NVIDIA GPU code\n",
            "         33, !$acc loop gang ! blockidx%x\n",
            "             Generating reduction(max:error)\n",
            "         34, !$acc loop vector(128) ! threadidx%x\n",
            "     32, Generating implicit copy(error) [if not already present]\n",
            "         Generating copyout(anew(:,:)) [if not already present]\n",
            "     34, Loop is parallelizable\n",
            "swap:\n",
            "     50, Generating copyout(a(:,:)) [if not already present]\n",
            "         Generating copyin(anew(:,:)) [if not already present]\n",
            "         Generating NVIDIA GPU code\n",
            "         51, !$acc loop gang ! blockidx%x\n",
            "         52, !$acc loop vector(128) ! threadidx%x\n",
            "     52, Loop is parallelizable\n",
            "jacobi.f90:\n",
            "jacobi:\n",
            "     30, Generating create(anew(:,:)) [if not already present]\n",
            "         Generating copy(a(:,:)) [if not already present]\n",
            "Jacobi relaxation Calculation: 4096 x 4096 mesh\n",
            "libcupti.so not found\n",
            "    0  0.250000\n",
            "  100  0.002397\n",
            "  200  0.001204\n",
            "  300  0.000804\n",
            "  400  0.000603\n",
            "  500  0.000483\n",
            "  600  0.000403\n",
            "  700  0.000345\n",
            "  800  0.000302\n",
            "  900  0.000269\n",
            " completed in      0.761 seconds\n",
            "\n",
            "Accelerator Kernel Timing data\n",
            "/content/lab3/jacobi.f90\n",
            "  jacobi  NVIDIA  devicenum=0\n",
            "    time(us): 21,277\n",
            "    30: data region reached 2 times\n",
            "        30: data copyin transfers: 10\n",
            "             device time(us): total=10,998 max=1,372 min=7 avg=1,099\n",
            "        41: data copyout transfers: 9\n",
            "             device time(us): total=10,279 max=1,289 min=8 avg=1,142\n",
            "/content/lab3/laplace2d.f90\n",
            "  calcnext  NVIDIA  devicenum=0\n",
            "    time(us): 14,696\n",
            "    32: compute region reached 1000 times\n",
            "        32: kernel launched 1000 times\n",
            "            grid: [4094]  block: [128]\n",
            "            elapsed time(us): total=233,892 max=281 min=229 avg=233\n",
            "        32: reduction kernel launched 1000 times\n",
            "            grid: [1]  block: [256]\n",
            "            elapsed time(us): total=25,269 max=57 min=22 avg=25\n",
            "    32: data region reached 4000 times\n",
            "        32: data copyin transfers: 1000\n",
            "             device time(us): total=5,049 max=9 min=5 avg=5\n",
            "        40: data copyout transfers: 1000\n",
            "             device time(us): total=9,647 max=22 min=7 avg=9\n",
            "/content/lab3/laplace2d.f90\n",
            "  swap  NVIDIA  devicenum=0\n",
            "    time(us): 0\n",
            "    50: compute region reached 1000 times\n",
            "        50: kernel launched 1000 times\n",
            "            grid: [4094]  block: [128]\n",
            "            elapsed time(us): total=226,684 max=247 min=221 avg=226\n",
            "    50: data region reached 2000 times\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nsys profile -t openacc,nvtx --stats=true   ./laplace"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-MLMGTPLrOT",
        "outputId": "6024785d-707c-4d4c-f690-ba2ad544c128"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
            "WARNING: CUDA tracing has been automatically enabled since it is a prerequisite for tracing OpenACC.\n",
            "Jacobi relaxation Calculation: 4096 x 4096 mesh\n",
            "    0  0.250000\n",
            "  100  0.002397\n",
            "  200  0.001204\n",
            "  300  0.000804\n",
            "  400  0.000603\n",
            "  500  0.000483\n",
            "  600  0.000403\n",
            "  700  0.000345\n",
            "  800  0.000302\n",
            "  900  0.000269\n",
            " completed in      1.129 seconds\n",
            "Generating '/tmp/nsys-report-4f83.qdstrm'\n",
            "[1/8] [========================100%] report5.nsys-rep\n",
            "[2/8] [========================100%] report5.sqlite\n",
            "[3/8] Executing 'nvtxsum' stats report\n",
            "SKIPPED: /content/lab3/report5.sqlite does not contain NV Tools Extension (NVTX) data.\n",
            "[4/8] Executing 'cudaapisum' stats report\n",
            "\n",
            "CUDA API Statistics:\n",
            "\n",
            " Time (%)  Total Time (ns)  Num Calls    Avg (ns)      Med (ns)     Min (ns)    Max (ns)   StdDev (ns)          Name        \n",
            " --------  ---------------  ---------  ------------  ------------  ----------  ----------  -----------  --------------------\n",
            "     87.5      474,959,319      6,002      79,133.5       9,531.5       1,139   1,268,399    105,052.0  cuStreamSynchronize \n",
            "      4.9       26,818,275          1  26,818,275.0  26,818,275.0  26,818,275  26,818,275          0.0  cuMemHostAlloc      \n",
            "      3.4       18,194,252      3,000       6,064.8       5,161.0       3,368   1,796,097     32,782.4  cuLaunchKernel      \n",
            "      2.0       10,899,641      1,024      10,644.2       1,650.0       1,552   1,284,529    105,528.1  cuEventSynchronize  \n",
            "      0.9        4,793,081      1,009       4,750.3       4,468.0       3,611      24,224      1,691.3  cuMemcpyDtoHAsync_v2\n",
            "      0.6        3,191,293      1,000       3,191.3       2,952.0       2,432      14,269      1,269.7  cuMemsetD32Async    \n",
            "      0.3        1,599,437      1,026       1,558.9       1,432.0       1,192       9,293        779.2  cuEventRecord       \n",
            "      0.2        1,000,137          1   1,000,137.0   1,000,137.0   1,000,137   1,000,137          0.0  cuMemAllocHost_v2   \n",
            "      0.2          927,801          7     132,543.0       8,163.0       4,772     404,591    168,313.3  cuMemAlloc_v2       \n",
            "      0.0          176,908         10      17,690.8      16,050.0       7,306      29,476      5,977.1  cuMemcpyHtoDAsync_v2\n",
            "      0.0          174,861          1     174,861.0     174,861.0     174,861     174,861          0.0  cuModuleLoadDataEx  \n",
            "      0.0            7,175          4       1,793.8       2,053.5         447       2,621        957.4  cuEventCreate       \n",
            "\n",
            "[5/8] Executing 'gpukernsum' stats report\n",
            "\n",
            "CUDA Kernel Statistics:\n",
            "\n",
            " Time (%)  Total Time (ns)  Instances  Avg (ns)   Med (ns)   Min (ns)  Max (ns)  StdDev (ns)               Name             \n",
            " --------  ---------------  ---------  ---------  ---------  --------  --------  -----------  ------------------------------\n",
            "     49.8      221,453,568      1,000  221,453.6  220,110.5   217,951   227,423      2,265.5  laplace2d_calcnext_32_gpu     \n",
            "     47.3      210,683,806      1,000  210,683.8  210,590.0   207,583   215,231      1,316.9  laplace2d_swap_50_gpu         \n",
            "      2.9       12,950,211      1,000   12,950.2   12,256.0    11,519    14,656      1,118.8  laplace2d_calcnext_32_gpu__red\n",
            "\n",
            "[6/8] Executing 'gpumemtimesum' stats report\n",
            "\n",
            "CUDA Memory Operation Statistics (by time):\n",
            "\n",
            " Time (%)  Total Time (ns)  Count   Avg (ns)     Med (ns)    Min (ns)  Max (ns)   StdDev (ns)      Operation     \n",
            " --------  ---------------  -----  -----------  -----------  --------  ---------  -----------  ------------------\n",
            "     48.6       12,150,991  1,009     12,042.6      2,048.0     1,823  1,266,103    112,140.5  [CUDA memcpy DtoH]\n",
            "     43.6       10,901,585     10  1,090,158.5  1,361,366.0     2,592  1,363,318    572,970.7  [CUDA memcpy HtoD]\n",
            "      7.9        1,963,850  1,000      1,963.9      1,888.0     1,791      2,464        157.4  [CUDA memset]     \n",
            "\n",
            "[7/8] Executing 'gpumemsizesum' stats report\n",
            "\n",
            "CUDA Memory Operation Statistics (by size):\n",
            "\n",
            " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)      Operation     \n",
            " ----------  -----  --------  --------  --------  --------  -----------  ------------------\n",
            "    134.226  1,009     0.133     0.000     0.000    16.777        1.489  [CUDA memcpy DtoH]\n",
            "    134.218     10    13.422    16.777     0.000    16.777        7.074  [CUDA memcpy HtoD]\n",
            "      0.008  1,000     0.000     0.000     0.000     0.000        0.000  [CUDA memset]     \n",
            "\n",
            "[8/8] Executing 'openaccsum' stats report\n",
            "\n",
            "OpenACC event Statistics:\n",
            "\n",
            " Time(%)  Total Time (ns)  Num Calls    Avg (ns)      Med (ns)     Min (ns)    Max (ns)   StdDev (ns)                 Name               \n",
            " -------  ---------------  ---------  ------------  ------------  ----------  ----------  -----------  ----------------------------------\n",
            "    22.0      250,749,696      1,000     250,749.7     252,269.0     244,445     322,701      5,632.3  Compute Construct@laplace2d.f90:32\n",
            "    21.8      248,432,354      3,000      82,810.8      10,206.5       2,226     251,919    107,815.2  Wait@laplace2d.f90:32             \n",
            "    19.9      227,188,934      1,000     227,188.9     225,053.5     221,162   2,016,368     56,681.2  Compute Construct@laplace2d.f90:50\n",
            "    19.2      219,087,109      2,000     109,543.6     111,031.5       2,198     225,752    106,713.6  Wait@laplace2d.f90:50             \n",
            "     4.6       52,497,677          1  52,497,677.0  52,497,677.0  52,497,677  52,497,677          0.0  Enter Data@jacobi.f90:30          \n",
            "     2.9       33,535,062          1  33,535,062.0  33,535,062.0  33,535,062  33,535,062          0.0  Exit Data@jacobi.f90:30           \n",
            "     2.3       26,118,342      2,000      13,059.2      17,069.5       1,398      59,058     11,532.6  Exit Data@laplace2d.f90:32        \n",
            "     2.0       23,295,828      2,000      11,647.9      16,175.5       4,094      45,061      6,848.5  Enter Data@laplace2d.f90:32       \n",
            "     1.1       12,876,415      2,000       6,438.2       5,885.0       4,325      75,917      2,968.3  Enqueue Launch@laplace2d.f90:32   \n",
            "     1.1       12,739,122      1,000      12,739.1      12,750.0       5,030      45,406      2,645.0  Wait@laplace2d.f90:40             \n",
            "     0.8        8,819,256      1,000       8,819.3       6,826.0       5,569   1,800,174     56,725.2  Enqueue Launch@laplace2d.f90:50   \n",
            "     0.7        7,616,612      1,000       7,616.6       7,246.0       5,891      27,556      1,926.2  Enqueue Download@laplace2d.f90:40 \n",
            "     0.4        4,920,843      1,000       4,920.8       4,669.0       4,040      17,994      1,276.5  Enter Data@laplace2d.f90:50       \n",
            "     0.4        4,156,074          1   4,156,074.0   4,156,074.0   4,156,074   4,156,074          0.0  Wait@jacobi.f90:41                \n",
            "     0.4        4,124,056      1,000       4,124.1       3,809.5       3,232      15,725      1,475.5  Enqueue Upload@laplace2d.f90:32   \n",
            "     0.2        1,933,588      1,000       1,933.6       1,730.0       1,526       9,068        616.8  Exit Data@laplace2d.f90:50        \n",
            "     0.1          891,920          1     891,920.0     891,920.0     891,920     891,920          0.0  Wait@jacobi.f90:30                \n",
            "     0.0          256,767         10      25,676.7      23,655.0       8,697      39,553      8,557.7  Enqueue Upload@jacobi.f90:30      \n",
            "     0.0          207,925          1     207,925.0     207,925.0     207,925     207,925          0.0  Device Init@jacobi.f90:30         \n",
            "     0.0          161,821          9      17,980.1      16,048.0       5,975      28,655      8,314.1  Enqueue Download@jacobi.f90:41    \n",
            "     0.0                0          4           0.0           0.0           0           0          0.0  Alloc@jacobi.f90:30               \n",
            "     0.0                0          1           0.0           0.0           0           0          0.0  Alloc@laplace2d.f90:32            \n",
            "     0.0                0          4           0.0           0.0           0           0          0.0  Create@jacobi.f90:30              \n",
            "     0.0                0      1,000           0.0           0.0           0           0          0.0  Create@laplace2d.f90:32           \n",
            "     0.0                0          4           0.0           0.0           0           0          0.0  Delete@jacobi.f90:41              \n",
            "     0.0                0      1,000           0.0           0.0           0           0          0.0  Delete@laplace2d.f90:40           \n",
            "\n",
            "Generated:\n",
            "    /content/lab3/report5.nsys-rep\n",
            "    /content/lab3/report5.sqlite\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ncu   ./laplace"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYJeCcozMrcS",
        "outputId": "31b62743-eafc-4b6b-f80e-1d14f81a30b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jacobi relaxation Calculation: 4096 x 4096 mesh\n",
            "==PROF== Connected to process 6510 (/content/lab3/laplace)\n",
            "==PROF== Profiling \"laplace2d_calcnext_32_gpu\" - 0: 0%....50%....100% - 10 passes\n",
            "==PROF== Profiling \"laplace2d_calcnext_32_gpu__red\" - 1: 0%....50%....100% - 10 passes\n",
            "==PROF== Profiling \"laplace2d_swap_50_gpu\" - 2: 0%....50%....100% - 10 passes\n",
            "    0  0.250000\n",
            "==PROF== Profiling \"laplace2d_calcnext_32_gpu\" - 3: 0%....50%....100% - 10 passes\n",
            "==PROF== Profiling \"laplace2d_calcnext_32_gpu__red\" - 4: 0%....50%....100% - 10 passes\n",
            "==PROF== Profiling \"laplace2d_swap_50_gpu\" - 5: 0%....50%....100% - 10 passes\n",
            "==PROF== Profiling \"laplace2d_calcnext_32_gpu\" - 6: 0%....50%....100% - 10 passes\n",
            "==PROF== Profiling \"laplace2d_calcnext_32_gpu__red\" - 7: 0%....50%....100% - 10 passes\n",
            "==PROF== Profiling \"laplace2d_swap_50_gpu\" - 8: 0%....50%....100% - 10 passes\n",
            "==PROF== Profiling \"laplace2d_calcnext_32_gpu\" - 9: 0%....50%....100% - 10 passes\n",
            "==PROF== Profiling \"laplace2d_calcnext_32_gpu__red\" - 10: 0%....50%....100% - 10 passes\n",
            "==PROF== Profiling \"laplace2d_swap_50_gpu\" - 11: 0%....50%....100% - 10 passes\n",
            "==PROF== Profiling \"laplace2d_calcnext_32_gpu\" - 12: 0%....50%....100% - 10 passes\n",
            "==PROF== Profiling \"laplace2d_calcnext_32_gpu__red\" - 13: 0%....50%....100% - 10 passes\n",
            "==PROF== Profiling \"laplace2d_swap_50_gpu\" - 14: 0%....50%....100% - 10 passes\n",
            "==PROF== Profiling \"laplace2d_calcnext_32_gpu\" - 15: 0%....50%....100% - 10 passes\n",
            "==PROF== Profiling \"laplace2d_calcnext_32_gpu__red\" - 16: 0%....50%....100% - 10 passes\n",
            "==PROF== Profiling \"laplace2d_swap_50_gpu\" - 17: 0%....50%....100% - 10 passes\n",
            "==PROF== Profiling \"laplace2d_calcnext_32_gpu\" - 18: 0%....50%....100% - 10 passes\n",
            "==PROF== Profiling \"laplace2d_calcnext_32_gpu__red\" - 19: 0%....50%....100% - 10 passes\n",
            "==PROF== Profiling \"laplace2d_swap_50_gpu\" - 20: 0%....50%....100% - 10 passes\n",
            "==PROF== Profiling \"laplace2d_calcnext_32_gpu\" - 21: 0%....50%....100% - 10 passes\n",
            "==PROF== Profiling \"laplace2d_calcnext_32_gpu__red\" - 22: 0%....50%....100% - 10 passes\n",
            "==PROF== Profiling \"laplace2d_swap_50_gpu\" - 23: 0%....50%....100% - 10 passes\n",
            "==PROF== Profiling \"laplace2d_calcnext_32_gpu\" - 24: 0%....50%....100% - 10 passes\n",
            "==PROF== Profiling \"laplace2d_calcnext_32_gpu__red\" - 25: 0%....50%....100% - 10 passes\n",
            "==PROF== Profiling \"laplace2d_swap_50_gpu\" - 26: 0%==PROF== Received signal\n",
            "==PROF== Trying to shutdown target application\n",
            "\n",
            "==ERROR== Failed to profile kernel \"laplace2d_swap_50_gpu\" in process 6510\n",
            "==PROF== Trying to shutdown target application\n",
            "==ERROR== An error occurred while trying to profile.\n",
            "[6510] laplace@127.0.0.1\n",
            "  laplace2d_calcnext_32_gpu, 2022-Sep-01 08:41:14, Context 1, Stream 13\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    DRAM Frequency                                                           cycle/nsecond                           1.19\n",
            "    SM Frequency                                                             cycle/nsecond                           1.07\n",
            "    Elapsed Cycles                                                                   cycle                        235,251\n",
            "    Memory [%]                                                                           %                          76.43\n",
            "    DRAM Throughput                                                                      %                          76.43\n",
            "    Duration                                                                       usecond                         218.78\n",
            "    L1/TEX Cache Throughput                                                              %                          33.89\n",
            "    L2 Cache Throughput                                                                  %                          78.27\n",
            "    SM Active Cycles                                                                 cycle                     226,283.06\n",
            "    Compute (SM) [%]                                                                     %                          16.73\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    \n",
            "          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       \n",
            "          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  \n",
            "          whether there are values you can (re)compute.                                                                 \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Size                                                                                                        128\n",
            "    Function Cache Configuration                                                                  cudaFuncCachePreferNone\n",
            "    Grid Size                                                                                                       4,094\n",
            "    Registers Per Thread                                                   register/thread                             48\n",
            "    Shared Memory Configuration Size                                                 Kbyte                          65.54\n",
            "    Driver Shared Memory Per Block                                             Kbyte/block                           1.02\n",
            "    Dynamic Shared Memory Per Block                                            Kbyte/block                           2.05\n",
            "    Static Shared Memory Per Block                                              byte/block                              0\n",
            "    Threads                                                                         thread                        524,032\n",
            "    Waves Per SM                                                                                                     3.79\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "\n",
            "    Section: Occupancy\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Limit SM                                                                   block                             32\n",
            "    Block Limit Registers                                                            block                             10\n",
            "    Block Limit Shared Mem                                                           block                             21\n",
            "    Block Limit Warps                                                                block                             16\n",
            "    Theoretical Active Warps per SM                                                   warp                             40\n",
            "    Theoretical Occupancy                                                                %                          62.50\n",
            "    Achieved Occupancy                                                                   %                          60.02\n",
            "    Achieved Active Warps Per SM                                                      warp                          38.41\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   This kernel's theoretical occupancy (62.5%) is limited by the number of required registers See the CUDA Best  \n",
            "          Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more      \n",
            "          details on optimizing occupancy.                                                                              \n",
            "\n",
            "  laplace2d_calcnext_32_gpu__red, 2022-Sep-01 08:41:15, Context 1, Stream 13\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    DRAM Frequency                                                           cycle/nsecond                           1.11\n",
            "    SM Frequency                                                             cycle/usecond                         999.35\n",
            "    Elapsed Cycles                                                                   cycle                         13,798\n",
            "    Memory [%]                                                                           %                           0.27\n",
            "    DRAM Throughput                                                                      %                           0.17\n",
            "    Duration                                                                       usecond                          13.76\n",
            "    L1/TEX Cache Throughput                                                              %                           4.92\n",
            "    L2 Cache Throughput                                                                  %                           0.27\n",
            "    SM Active Cycles                                                                 cycle                         106.05\n",
            "    Compute (SM) [%]                                                                     %                           0.04\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Size                                                                                                        256\n",
            "    Function Cache Configuration                                                                  cudaFuncCachePreferNone\n",
            "    Grid Size                                                                                                           1\n",
            "    Registers Per Thread                                                   register/thread                             24\n",
            "    Shared Memory Configuration Size                                                 Kbyte                          65.54\n",
            "    Driver Shared Memory Per Block                                             Kbyte/block                           1.02\n",
            "    Dynamic Shared Memory Per Block                                            Kbyte/block                           2.05\n",
            "    Static Shared Memory Per Block                                              byte/block                              0\n",
            "    Threads                                                                         thread                            256\n",
            "    Waves Per SM                                                                                                     0.00\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Limit SM                                                                   block                             32\n",
            "    Block Limit Registers                                                            block                             10\n",
            "    Block Limit Shared Mem                                                           block                             21\n",
            "    Block Limit Warps                                                                block                              8\n",
            "    Theoretical Active Warps per SM                                                   warp                             64\n",
            "    Theoretical Occupancy                                                                %                            100\n",
            "    Achieved Occupancy                                                                   %                          11.49\n",
            "    Achieved Active Warps Per SM                                                      warp                           7.36\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     \n",
            "          theoretical (100.0%) and measured achieved occupancy (11.5%) can be the result of warp scheduling overheads   \n",
            "          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    \n",
            "          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                \n",
            "          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n",
            "  laplace2d_swap_50_gpu, 2022-Sep-01 08:41:16, Context 1, Stream 13\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    DRAM Frequency                                                           cycle/nsecond                           1.19\n",
            "    SM Frequency                                                             cycle/nsecond                           1.07\n",
            "    Elapsed Cycles                                                                   cycle                        220,465\n",
            "    Memory [%]                                                                           %                          81.33\n",
            "    DRAM Throughput                                                                      %                          81.33\n",
            "    Duration                                                                       usecond                         205.60\n",
            "    L1/TEX Cache Throughput                                                              %                          28.15\n",
            "    L2 Cache Throughput                                                                  %                          69.20\n",
            "    SM Active Cycles                                                                 cycle                     210,215.88\n",
            "    Compute (SM) [%]                                                                     %                          10.72\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   \n",
            "          further improve performance, work will likely need to be shifted from the most utilized to another unit.      \n",
            "          Start by analyzing DRAM in the Memory Workload Analysis section.                                              \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Size                                                                                                        128\n",
            "    Function Cache Configuration                                                                  cudaFuncCachePreferNone\n",
            "    Grid Size                                                                                                       4,094\n",
            "    Registers Per Thread                                                   register/thread                             25\n",
            "    Shared Memory Configuration Size                                                 Kbyte                          32.77\n",
            "    Driver Shared Memory Per Block                                             Kbyte/block                           1.02\n",
            "    Dynamic Shared Memory Per Block                                             byte/block                              0\n",
            "    Static Shared Memory Per Block                                              byte/block                              0\n",
            "    Threads                                                                         thread                        524,032\n",
            "    Waves Per SM                                                                                                     2.37\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "\n",
            "    Section: Occupancy\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Limit SM                                                                   block                             32\n",
            "    Block Limit Registers                                                            block                             16\n",
            "    Block Limit Shared Mem                                                           block                             32\n",
            "    Block Limit Warps                                                                block                             16\n",
            "    Theoretical Active Warps per SM                                                   warp                             64\n",
            "    Theoretical Occupancy                                                                %                            100\n",
            "    Achieved Occupancy                                                                   %                          87.45\n",
            "    Achieved Active Warps Per SM                                                      warp                          55.97\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     \n",
            "          theoretical (100.0%) and measured achieved occupancy (87.5%) can be the result of warp scheduling overheads   \n",
            "          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    \n",
            "          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                \n",
            "          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n",
            "  laplace2d_calcnext_32_gpu, 2022-Sep-01 08:41:16, Context 1, Stream 13\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    DRAM Frequency                                                           cycle/nsecond                           1.19\n",
            "    SM Frequency                                                             cycle/nsecond                           1.07\n",
            "    Elapsed Cycles                                                                   cycle                        236,107\n",
            "    Memory [%]                                                                           %                          76.22\n",
            "    DRAM Throughput                                                                      %                          76.22\n",
            "    Duration                                                                       usecond                         219.74\n",
            "    L1/TEX Cache Throughput                                                              %                          33.52\n",
            "    L2 Cache Throughput                                                                  %                          76.73\n",
            "    SM Active Cycles                                                                 cycle                     228,444.70\n",
            "    Compute (SM) [%]                                                                     %                          16.68\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    \n",
            "          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       \n",
            "          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  \n",
            "          whether there are values you can (re)compute.                                                                 \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Size                                                                                                        128\n",
            "    Function Cache Configuration                                                                  cudaFuncCachePreferNone\n",
            "    Grid Size                                                                                                       4,094\n",
            "    Registers Per Thread                                                   register/thread                             48\n",
            "    Shared Memory Configuration Size                                                 Kbyte                          65.54\n",
            "    Driver Shared Memory Per Block                                             Kbyte/block                           1.02\n",
            "    Dynamic Shared Memory Per Block                                            Kbyte/block                           2.05\n",
            "    Static Shared Memory Per Block                                              byte/block                              0\n",
            "    Threads                                                                         thread                        524,032\n",
            "    Waves Per SM                                                                                                     3.79\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "\n",
            "    Section: Occupancy\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Limit SM                                                                   block                             32\n",
            "    Block Limit Registers                                                            block                             10\n",
            "    Block Limit Shared Mem                                                           block                             21\n",
            "    Block Limit Warps                                                                block                             16\n",
            "    Theoretical Active Warps per SM                                                   warp                             40\n",
            "    Theoretical Occupancy                                                                %                          62.50\n",
            "    Achieved Occupancy                                                                   %                          59.39\n",
            "    Achieved Active Warps Per SM                                                      warp                          38.01\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   This kernel's theoretical occupancy (62.5%) is limited by the number of required registers See the CUDA Best  \n",
            "          Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more      \n",
            "          details on optimizing occupancy.                                                                              \n",
            "\n",
            "  laplace2d_calcnext_32_gpu__red, 2022-Sep-01 08:41:17, Context 1, Stream 13\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    DRAM Frequency                                                           cycle/nsecond                           1.12\n",
            "    SM Frequency                                                             cycle/nsecond                           1.01\n",
            "    Elapsed Cycles                                                                   cycle                         14,013\n",
            "    Memory [%]                                                                           %                           0.27\n",
            "    DRAM Throughput                                                                      %                           0.17\n",
            "    Duration                                                                       usecond                          13.82\n",
            "    L1/TEX Cache Throughput                                                              %                           4.85\n",
            "    L2 Cache Throughput                                                                  %                           0.27\n",
            "    SM Active Cycles                                                                 cycle                         107.44\n",
            "    Compute (SM) [%]                                                                     %                           0.04\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Size                                                                                                        256\n",
            "    Function Cache Configuration                                                                  cudaFuncCachePreferNone\n",
            "    Grid Size                                                                                                           1\n",
            "    Registers Per Thread                                                   register/thread                             24\n",
            "    Shared Memory Configuration Size                                                 Kbyte                          65.54\n",
            "    Driver Shared Memory Per Block                                             Kbyte/block                           1.02\n",
            "    Dynamic Shared Memory Per Block                                            Kbyte/block                           2.05\n",
            "    Static Shared Memory Per Block                                              byte/block                              0\n",
            "    Threads                                                                         thread                            256\n",
            "    Waves Per SM                                                                                                     0.00\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Limit SM                                                                   block                             32\n",
            "    Block Limit Registers                                                            block                             10\n",
            "    Block Limit Shared Mem                                                           block                             21\n",
            "    Block Limit Warps                                                                block                              8\n",
            "    Theoretical Active Warps per SM                                                   warp                             64\n",
            "    Theoretical Occupancy                                                                %                            100\n",
            "    Achieved Occupancy                                                                   %                          11.50\n",
            "    Achieved Active Warps Per SM                                                      warp                           7.36\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     \n",
            "          theoretical (100.0%) and measured achieved occupancy (11.5%) can be the result of warp scheduling overheads   \n",
            "          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    \n",
            "          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                \n",
            "          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n",
            "  laplace2d_swap_50_gpu, 2022-Sep-01 08:41:18, Context 1, Stream 13\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    DRAM Frequency                                                           cycle/nsecond                           1.19\n",
            "    SM Frequency                                                             cycle/nsecond                           1.07\n",
            "    Elapsed Cycles                                                                   cycle                        220,465\n",
            "    Memory [%]                                                                           %                          81.35\n",
            "    DRAM Throughput                                                                      %                          81.35\n",
            "    Duration                                                                       usecond                         205.06\n",
            "    L1/TEX Cache Throughput                                                              %                          27.93\n",
            "    L2 Cache Throughput                                                                  %                          69.49\n",
            "    SM Active Cycles                                                                 cycle                     211,908.02\n",
            "    Compute (SM) [%]                                                                     %                          10.73\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   \n",
            "          further improve performance, work will likely need to be shifted from the most utilized to another unit.      \n",
            "          Start by analyzing DRAM in the Memory Workload Analysis section.                                              \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Size                                                                                                        128\n",
            "    Function Cache Configuration                                                                  cudaFuncCachePreferNone\n",
            "    Grid Size                                                                                                       4,094\n",
            "    Registers Per Thread                                                   register/thread                             25\n",
            "    Shared Memory Configuration Size                                                 Kbyte                          32.77\n",
            "    Driver Shared Memory Per Block                                             Kbyte/block                           1.02\n",
            "    Dynamic Shared Memory Per Block                                             byte/block                              0\n",
            "    Static Shared Memory Per Block                                              byte/block                              0\n",
            "    Threads                                                                         thread                        524,032\n",
            "    Waves Per SM                                                                                                     2.37\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "\n",
            "    Section: Occupancy\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Limit SM                                                                   block                             32\n",
            "    Block Limit Registers                                                            block                             16\n",
            "    Block Limit Shared Mem                                                           block                             32\n",
            "    Block Limit Warps                                                                block                             16\n",
            "    Theoretical Active Warps per SM                                                   warp                             64\n",
            "    Theoretical Occupancy                                                                %                            100\n",
            "    Achieved Occupancy                                                                   %                          86.98\n",
            "    Achieved Active Warps Per SM                                                      warp                          55.67\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     \n",
            "          theoretical (100.0%) and measured achieved occupancy (87.0%) can be the result of warp scheduling overheads   \n",
            "          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    \n",
            "          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                \n",
            "          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n",
            "  laplace2d_calcnext_32_gpu, 2022-Sep-01 08:41:18, Context 1, Stream 13\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    DRAM Frequency                                                           cycle/nsecond                           1.20\n",
            "    SM Frequency                                                             cycle/nsecond                           1.08\n",
            "    Elapsed Cycles                                                                   cycle                        236,326\n",
            "    Memory [%]                                                                           %                          76.25\n",
            "    DRAM Throughput                                                                      %                          76.25\n",
            "    Duration                                                                       usecond                         218.40\n",
            "    L1/TEX Cache Throughput                                                              %                          33.32\n",
            "    L2 Cache Throughput                                                                  %                          77.76\n",
            "    SM Active Cycles                                                                 cycle                     229,949.48\n",
            "    Compute (SM) [%]                                                                     %                          16.69\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    \n",
            "          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       \n",
            "          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  \n",
            "          whether there are values you can (re)compute.                                                                 \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Size                                                                                                        128\n",
            "    Function Cache Configuration                                                                  cudaFuncCachePreferNone\n",
            "    Grid Size                                                                                                       4,094\n",
            "    Registers Per Thread                                                   register/thread                             48\n",
            "    Shared Memory Configuration Size                                                 Kbyte                          65.54\n",
            "    Driver Shared Memory Per Block                                             Kbyte/block                           1.02\n",
            "    Dynamic Shared Memory Per Block                                            Kbyte/block                           2.05\n",
            "    Static Shared Memory Per Block                                              byte/block                              0\n",
            "    Threads                                                                         thread                        524,032\n",
            "    Waves Per SM                                                                                                     3.79\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "\n",
            "    Section: Occupancy\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Limit SM                                                                   block                             32\n",
            "    Block Limit Registers                                                            block                             10\n",
            "    Block Limit Shared Mem                                                           block                             21\n",
            "    Block Limit Warps                                                                block                             16\n",
            "    Theoretical Active Warps per SM                                                   warp                             40\n",
            "    Theoretical Occupancy                                                                %                          62.50\n",
            "    Achieved Occupancy                                                                   %                          59.13\n",
            "    Achieved Active Warps Per SM                                                      warp                          37.84\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   This kernel's theoretical occupancy (62.5%) is limited by the number of required registers See the CUDA Best  \n",
            "          Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more      \n",
            "          details on optimizing occupancy.                                                                              \n",
            "\n",
            "  laplace2d_calcnext_32_gpu__red, 2022-Sep-01 08:41:19, Context 1, Stream 13\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    DRAM Frequency                                                           cycle/nsecond                           1.10\n",
            "    SM Frequency                                                             cycle/usecond                         996.07\n",
            "    Elapsed Cycles                                                                   cycle                         14,172\n",
            "    Memory [%]                                                                           %                           0.27\n",
            "    DRAM Throughput                                                                      %                           0.16\n",
            "    Duration                                                                       usecond                          14.21\n",
            "    L1/TEX Cache Throughput                                                              %                           4.80\n",
            "    L2 Cache Throughput                                                                  %                           0.27\n",
            "    SM Active Cycles                                                                 cycle                         108.66\n",
            "    Compute (SM) [%]                                                                     %                           0.04\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Size                                                                                                        256\n",
            "    Function Cache Configuration                                                                  cudaFuncCachePreferNone\n",
            "    Grid Size                                                                                                           1\n",
            "    Registers Per Thread                                                   register/thread                             24\n",
            "    Shared Memory Configuration Size                                                 Kbyte                          65.54\n",
            "    Driver Shared Memory Per Block                                             Kbyte/block                           1.02\n",
            "    Dynamic Shared Memory Per Block                                            Kbyte/block                           2.05\n",
            "    Static Shared Memory Per Block                                              byte/block                              0\n",
            "    Threads                                                                         thread                            256\n",
            "    Waves Per SM                                                                                                     0.00\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Limit SM                                                                   block                             32\n",
            "    Block Limit Registers                                                            block                             10\n",
            "    Block Limit Shared Mem                                                           block                             21\n",
            "    Block Limit Warps                                                                block                              8\n",
            "    Theoretical Active Warps per SM                                                   warp                             64\n",
            "    Theoretical Occupancy                                                                %                            100\n",
            "    Achieved Occupancy                                                                   %                          11.51\n",
            "    Achieved Active Warps Per SM                                                      warp                           7.37\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     \n",
            "          theoretical (100.0%) and measured achieved occupancy (11.5%) can be the result of warp scheduling overheads   \n",
            "          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    \n",
            "          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                \n",
            "          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n",
            "  laplace2d_swap_50_gpu, 2022-Sep-01 08:41:20, Context 1, Stream 13\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    DRAM Frequency                                                           cycle/nsecond                           1.20\n",
            "    SM Frequency                                                             cycle/nsecond                           1.07\n",
            "    Elapsed Cycles                                                                   cycle                        218,067\n",
            "    Memory [%]                                                                           %                          82.21\n",
            "    DRAM Throughput                                                                      %                          82.21\n",
            "    Duration                                                                       usecond                         202.34\n",
            "    L1/TEX Cache Throughput                                                              %                          27.96\n",
            "    L2 Cache Throughput                                                                  %                          70.04\n",
            "    SM Active Cycles                                                                 cycle                     211,799.44\n",
            "    Compute (SM) [%]                                                                     %                          10.84\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   \n",
            "          further improve performance, work will likely need to be shifted from the most utilized to another unit.      \n",
            "          Start by analyzing DRAM in the Memory Workload Analysis section.                                              \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Size                                                                                                        128\n",
            "    Function Cache Configuration                                                                  cudaFuncCachePreferNone\n",
            "    Grid Size                                                                                                       4,094\n",
            "    Registers Per Thread                                                   register/thread                             25\n",
            "    Shared Memory Configuration Size                                                 Kbyte                          32.77\n",
            "    Driver Shared Memory Per Block                                             Kbyte/block                           1.02\n",
            "    Dynamic Shared Memory Per Block                                             byte/block                              0\n",
            "    Static Shared Memory Per Block                                              byte/block                              0\n",
            "    Threads                                                                         thread                        524,032\n",
            "    Waves Per SM                                                                                                     2.37\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "\n",
            "    Section: Occupancy\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Limit SM                                                                   block                             32\n",
            "    Block Limit Registers                                                            block                             16\n",
            "    Block Limit Shared Mem                                                           block                             32\n",
            "    Block Limit Warps                                                                block                             16\n",
            "    Theoretical Active Warps per SM                                                   warp                             64\n",
            "    Theoretical Occupancy                                                                %                            100\n",
            "    Achieved Occupancy                                                                   %                          86.72\n",
            "    Achieved Active Warps Per SM                                                      warp                          55.50\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     \n",
            "          theoretical (100.0%) and measured achieved occupancy (86.7%) can be the result of warp scheduling overheads   \n",
            "          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    \n",
            "          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                \n",
            "          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n",
            "  laplace2d_calcnext_32_gpu, 2022-Sep-01 08:41:20, Context 1, Stream 13\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    DRAM Frequency                                                           cycle/nsecond                           1.20\n",
            "    SM Frequency                                                             cycle/nsecond                           1.07\n",
            "    Elapsed Cycles                                                                   cycle                        236,523\n",
            "    Memory [%]                                                                           %                          76.13\n",
            "    DRAM Throughput                                                                      %                          76.13\n",
            "    Duration                                                                       usecond                         219.30\n",
            "    L1/TEX Cache Throughput                                                              %                          33.65\n",
            "    L2 Cache Throughput                                                                  %                          76.81\n",
            "    SM Active Cycles                                                                 cycle                     227,682.44\n",
            "    Compute (SM) [%]                                                                     %                          16.66\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    \n",
            "          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       \n",
            "          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  \n",
            "          whether there are values you can (re)compute.                                                                 \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Size                                                                                                        128\n",
            "    Function Cache Configuration                                                                  cudaFuncCachePreferNone\n",
            "    Grid Size                                                                                                       4,094\n",
            "    Registers Per Thread                                                   register/thread                             48\n",
            "    Shared Memory Configuration Size                                                 Kbyte                          65.54\n",
            "    Driver Shared Memory Per Block                                             Kbyte/block                           1.02\n",
            "    Dynamic Shared Memory Per Block                                            Kbyte/block                           2.05\n",
            "    Static Shared Memory Per Block                                              byte/block                              0\n",
            "    Threads                                                                         thread                        524,032\n",
            "    Waves Per SM                                                                                                     3.79\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "\n",
            "    Section: Occupancy\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Limit SM                                                                   block                             32\n",
            "    Block Limit Registers                                                            block                             10\n",
            "    Block Limit Shared Mem                                                           block                             21\n",
            "    Block Limit Warps                                                                block                             16\n",
            "    Theoretical Active Warps per SM                                                   warp                             40\n",
            "    Theoretical Occupancy                                                                %                          62.50\n",
            "    Achieved Occupancy                                                                   %                          59.81\n",
            "    Achieved Active Warps Per SM                                                      warp                          38.28\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   This kernel's theoretical occupancy (62.5%) is limited by the number of required registers See the CUDA Best  \n",
            "          Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more      \n",
            "          details on optimizing occupancy.                                                                              \n",
            "\n",
            "  laplace2d_calcnext_32_gpu__red, 2022-Sep-01 08:41:21, Context 1, Stream 13\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    DRAM Frequency                                                           cycle/nsecond                           1.12\n",
            "    SM Frequency                                                             cycle/nsecond                           1.00\n",
            "    Elapsed Cycles                                                                   cycle                         14,032\n",
            "    Memory [%]                                                                           %                           0.27\n",
            "    DRAM Throughput                                                                      %                           0.17\n",
            "    Duration                                                                       usecond                          13.95\n",
            "    L1/TEX Cache Throughput                                                              %                           4.81\n",
            "    L2 Cache Throughput                                                                  %                           0.27\n",
            "    SM Active Cycles                                                                 cycle                         108.32\n",
            "    Compute (SM) [%]                                                                     %                           0.04\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Size                                                                                                        256\n",
            "    Function Cache Configuration                                                                  cudaFuncCachePreferNone\n",
            "    Grid Size                                                                                                           1\n",
            "    Registers Per Thread                                                   register/thread                             24\n",
            "    Shared Memory Configuration Size                                                 Kbyte                          65.54\n",
            "    Driver Shared Memory Per Block                                             Kbyte/block                           1.02\n",
            "    Dynamic Shared Memory Per Block                                            Kbyte/block                           2.05\n",
            "    Static Shared Memory Per Block                                              byte/block                              0\n",
            "    Threads                                                                         thread                            256\n",
            "    Waves Per SM                                                                                                     0.00\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Limit SM                                                                   block                             32\n",
            "    Block Limit Registers                                                            block                             10\n",
            "    Block Limit Shared Mem                                                           block                             21\n",
            "    Block Limit Warps                                                                block                              8\n",
            "    Theoretical Active Warps per SM                                                   warp                             64\n",
            "    Theoretical Occupancy                                                                %                            100\n",
            "    Achieved Occupancy                                                                   %                          11.51\n",
            "    Achieved Active Warps Per SM                                                      warp                           7.37\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     \n",
            "          theoretical (100.0%) and measured achieved occupancy (11.5%) can be the result of warp scheduling overheads   \n",
            "          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    \n",
            "          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                \n",
            "          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n",
            "  laplace2d_swap_50_gpu, 2022-Sep-01 08:41:22, Context 1, Stream 13\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    DRAM Frequency                                                           cycle/nsecond                           1.21\n",
            "    SM Frequency                                                             cycle/nsecond                           1.08\n",
            "    Elapsed Cycles                                                                   cycle                        219,322\n",
            "    Memory [%]                                                                           %                          81.62\n",
            "    DRAM Throughput                                                                      %                          81.62\n",
            "    Duration                                                                       usecond                         201.86\n",
            "    L1/TEX Cache Throughput                                                              %                          27.96\n",
            "    L2 Cache Throughput                                                                  %                          69.57\n",
            "    SM Active Cycles                                                                 cycle                     211,649.66\n",
            "    Compute (SM) [%]                                                                     %                          10.77\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   \n",
            "          further improve performance, work will likely need to be shifted from the most utilized to another unit.      \n",
            "          Start by analyzing DRAM in the Memory Workload Analysis section.                                              \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Size                                                                                                        128\n",
            "    Function Cache Configuration                                                                  cudaFuncCachePreferNone\n",
            "    Grid Size                                                                                                       4,094\n",
            "    Registers Per Thread                                                   register/thread                             25\n",
            "    Shared Memory Configuration Size                                                 Kbyte                          32.77\n",
            "    Driver Shared Memory Per Block                                             Kbyte/block                           1.02\n",
            "    Dynamic Shared Memory Per Block                                             byte/block                              0\n",
            "    Static Shared Memory Per Block                                              byte/block                              0\n",
            "    Threads                                                                         thread                        524,032\n",
            "    Waves Per SM                                                                                                     2.37\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "\n",
            "    Section: Occupancy\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Limit SM                                                                   block                             32\n",
            "    Block Limit Registers                                                            block                             16\n",
            "    Block Limit Shared Mem                                                           block                             32\n",
            "    Block Limit Warps                                                                block                             16\n",
            "    Theoretical Active Warps per SM                                                   warp                             64\n",
            "    Theoretical Occupancy                                                                %                            100\n",
            "    Achieved Occupancy                                                                   %                          86.98\n",
            "    Achieved Active Warps Per SM                                                      warp                          55.66\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     \n",
            "          theoretical (100.0%) and measured achieved occupancy (87.0%) can be the result of warp scheduling overheads   \n",
            "          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    \n",
            "          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                \n",
            "          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n",
            "  laplace2d_calcnext_32_gpu, 2022-Sep-01 08:41:23, Context 1, Stream 13\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    DRAM Frequency                                                           cycle/nsecond                           1.20\n",
            "    SM Frequency                                                             cycle/nsecond                           1.08\n",
            "    Elapsed Cycles                                                                   cycle                        235,849\n",
            "    Memory [%]                                                                           %                          76.24\n",
            "    DRAM Throughput                                                                      %                          76.24\n",
            "    Duration                                                                       usecond                         218.53\n",
            "    L1/TEX Cache Throughput                                                              %                          33.71\n",
            "    L2 Cache Throughput                                                                  %                          77.41\n",
            "    SM Active Cycles                                                                 cycle                     227,254.15\n",
            "    Compute (SM) [%]                                                                     %                          16.70\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    \n",
            "          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       \n",
            "          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  \n",
            "          whether there are values you can (re)compute.                                                                 \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Size                                                                                                        128\n",
            "    Function Cache Configuration                                                                  cudaFuncCachePreferNone\n",
            "    Grid Size                                                                                                       4,094\n",
            "    Registers Per Thread                                                   register/thread                             48\n",
            "    Shared Memory Configuration Size                                                 Kbyte                          65.54\n",
            "    Driver Shared Memory Per Block                                             Kbyte/block                           1.02\n",
            "    Dynamic Shared Memory Per Block                                            Kbyte/block                           2.05\n",
            "    Static Shared Memory Per Block                                              byte/block                              0\n",
            "    Threads                                                                         thread                        524,032\n",
            "    Waves Per SM                                                                                                     3.79\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "\n",
            "    Section: Occupancy\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Limit SM                                                                   block                             32\n",
            "    Block Limit Registers                                                            block                             10\n",
            "    Block Limit Shared Mem                                                           block                             21\n",
            "    Block Limit Warps                                                                block                             16\n",
            "    Theoretical Active Warps per SM                                                   warp                             40\n",
            "    Theoretical Occupancy                                                                %                          62.50\n",
            "    Achieved Occupancy                                                                   %                          59.82\n",
            "    Achieved Active Warps Per SM                                                      warp                          38.28\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   This kernel's theoretical occupancy (62.5%) is limited by the number of required registers See the CUDA Best  \n",
            "          Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more      \n",
            "          details on optimizing occupancy.                                                                              \n",
            "\n",
            "  laplace2d_calcnext_32_gpu__red, 2022-Sep-01 08:41:23, Context 1, Stream 13\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    DRAM Frequency                                                           cycle/nsecond                           1.09\n",
            "    SM Frequency                                                             cycle/usecond                         977.54\n",
            "    Elapsed Cycles                                                                   cycle                         13,794\n",
            "    Memory [%]                                                                           %                           0.27\n",
            "    DRAM Throughput                                                                      %                           0.17\n",
            "    Duration                                                                       usecond                          14.08\n",
            "    L1/TEX Cache Throughput                                                              %                           4.88\n",
            "    L2 Cache Throughput                                                                  %                           0.27\n",
            "    SM Active Cycles                                                                 cycle                         106.83\n",
            "    Compute (SM) [%]                                                                     %                           0.04\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Size                                                                                                        256\n",
            "    Function Cache Configuration                                                                  cudaFuncCachePreferNone\n",
            "    Grid Size                                                                                                           1\n",
            "    Registers Per Thread                                                   register/thread                             24\n",
            "    Shared Memory Configuration Size                                                 Kbyte                          65.54\n",
            "    Driver Shared Memory Per Block                                             Kbyte/block                           1.02\n",
            "    Dynamic Shared Memory Per Block                                            Kbyte/block                           2.05\n",
            "    Static Shared Memory Per Block                                              byte/block                              0\n",
            "    Threads                                                                         thread                            256\n",
            "    Waves Per SM                                                                                                     0.00\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Limit SM                                                                   block                             32\n",
            "    Block Limit Registers                                                            block                             10\n",
            "    Block Limit Shared Mem                                                           block                             21\n",
            "    Block Limit Warps                                                                block                              8\n",
            "    Theoretical Active Warps per SM                                                   warp                             64\n",
            "    Theoretical Occupancy                                                                %                            100\n",
            "    Achieved Occupancy                                                                   %                          11.50\n",
            "    Achieved Active Warps Per SM                                                      warp                           7.36\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     \n",
            "          theoretical (100.0%) and measured achieved occupancy (11.5%) can be the result of warp scheduling overheads   \n",
            "          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    \n",
            "          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                \n",
            "          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n",
            "  laplace2d_swap_50_gpu, 2022-Sep-01 08:41:24, Context 1, Stream 13\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    DRAM Frequency                                                           cycle/nsecond                           1.19\n",
            "    SM Frequency                                                             cycle/nsecond                           1.07\n",
            "    Elapsed Cycles                                                                   cycle                        218,482\n",
            "    Memory [%]                                                                           %                          82.13\n",
            "    DRAM Throughput                                                                      %                          82.13\n",
            "    Duration                                                                       usecond                         202.94\n",
            "    L1/TEX Cache Throughput                                                              %                          27.84\n",
            "    L2 Cache Throughput                                                                  %                          69.79\n",
            "    SM Active Cycles                                                                 cycle                     212,746.43\n",
            "    Compute (SM) [%]                                                                     %                          10.83\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   \n",
            "          further improve performance, work will likely need to be shifted from the most utilized to another unit.      \n",
            "          Start by analyzing DRAM in the Memory Workload Analysis section.                                              \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Size                                                                                                        128\n",
            "    Function Cache Configuration                                                                  cudaFuncCachePreferNone\n",
            "    Grid Size                                                                                                       4,094\n",
            "    Registers Per Thread                                                   register/thread                             25\n",
            "    Shared Memory Configuration Size                                                 Kbyte                          32.77\n",
            "    Driver Shared Memory Per Block                                             Kbyte/block                           1.02\n",
            "    Dynamic Shared Memory Per Block                                             byte/block                              0\n",
            "    Static Shared Memory Per Block                                              byte/block                              0\n",
            "    Threads                                                                         thread                        524,032\n",
            "    Waves Per SM                                                                                                     2.37\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "\n",
            "    Section: Occupancy\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Limit SM                                                                   block                             32\n",
            "    Block Limit Registers                                                            block                             16\n",
            "    Block Limit Shared Mem                                                           block                             32\n",
            "    Block Limit Warps                                                                block                             16\n",
            "    Theoretical Active Warps per SM                                                   warp                             64\n",
            "    Theoretical Occupancy                                                                %                            100\n",
            "    Achieved Occupancy                                                                   %                          86.48\n",
            "    Achieved Active Warps Per SM                                                      warp                          55.35\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     \n",
            "          theoretical (100.0%) and measured achieved occupancy (86.5%) can be the result of warp scheduling overheads   \n",
            "          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    \n",
            "          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                \n",
            "          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n",
            "  laplace2d_calcnext_32_gpu, 2022-Sep-01 08:41:25, Context 1, Stream 13\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    DRAM Frequency                                                           cycle/nsecond                           1.21\n",
            "    SM Frequency                                                             cycle/nsecond                           1.08\n",
            "    Elapsed Cycles                                                                   cycle                        237,333\n",
            "    Memory [%]                                                                           %                          75.72\n",
            "    DRAM Throughput                                                                      %                          75.72\n",
            "    Duration                                                                       usecond                         218.18\n",
            "    L1/TEX Cache Throughput                                                              %                          33.95\n",
            "    L2 Cache Throughput                                                                  %                          77.70\n",
            "    SM Active Cycles                                                                 cycle                     225,508.44\n",
            "    Compute (SM) [%]                                                                     %                          16.58\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    \n",
            "          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       \n",
            "          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  \n",
            "          whether there are values you can (re)compute.                                                                 \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Size                                                                                                        128\n",
            "    Function Cache Configuration                                                                  cudaFuncCachePreferNone\n",
            "    Grid Size                                                                                                       4,094\n",
            "    Registers Per Thread                                                   register/thread                             48\n",
            "    Shared Memory Configuration Size                                                 Kbyte                          65.54\n",
            "    Driver Shared Memory Per Block                                             Kbyte/block                           1.02\n",
            "    Dynamic Shared Memory Per Block                                            Kbyte/block                           2.05\n",
            "    Static Shared Memory Per Block                                              byte/block                              0\n",
            "    Threads                                                                         thread                        524,032\n",
            "    Waves Per SM                                                                                                     3.79\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "\n",
            "    Section: Occupancy\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Limit SM                                                                   block                             32\n",
            "    Block Limit Registers                                                            block                             10\n",
            "    Block Limit Shared Mem                                                           block                             21\n",
            "    Block Limit Warps                                                                block                             16\n",
            "    Theoretical Active Warps per SM                                                   warp                             40\n",
            "    Theoretical Occupancy                                                                %                          62.50\n",
            "    Achieved Occupancy                                                                   %                          60.15\n",
            "    Achieved Active Warps Per SM                                                      warp                          38.49\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   This kernel's theoretical occupancy (62.5%) is limited by the number of required registers See the CUDA Best  \n",
            "          Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more      \n",
            "          details on optimizing occupancy.                                                                              \n",
            "\n",
            "  laplace2d_calcnext_32_gpu__red, 2022-Sep-01 08:41:25, Context 1, Stream 13\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    DRAM Frequency                                                           cycle/nsecond                           1.10\n",
            "    SM Frequency                                                             cycle/usecond                         993.46\n",
            "    Elapsed Cycles                                                                   cycle                         13,853\n",
            "    Memory [%]                                                                           %                           0.27\n",
            "    DRAM Throughput                                                                      %                           0.17\n",
            "    Duration                                                                       usecond                          13.92\n",
            "    L1/TEX Cache Throughput                                                              %                           4.84\n",
            "    L2 Cache Throughput                                                                  %                           0.27\n",
            "    SM Active Cycles                                                                 cycle                         107.69\n",
            "    Compute (SM) [%]                                                                     %                           0.04\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Size                                                                                                        256\n",
            "    Function Cache Configuration                                                                  cudaFuncCachePreferNone\n",
            "    Grid Size                                                                                                           1\n",
            "    Registers Per Thread                                                   register/thread                             24\n",
            "    Shared Memory Configuration Size                                                 Kbyte                          65.54\n",
            "    Driver Shared Memory Per Block                                             Kbyte/block                           1.02\n",
            "    Dynamic Shared Memory Per Block                                            Kbyte/block                           2.05\n",
            "    Static Shared Memory Per Block                                              byte/block                              0\n",
            "    Threads                                                                         thread                            256\n",
            "    Waves Per SM                                                                                                     0.00\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Limit SM                                                                   block                             32\n",
            "    Block Limit Registers                                                            block                             10\n",
            "    Block Limit Shared Mem                                                           block                             21\n",
            "    Block Limit Warps                                                                block                              8\n",
            "    Theoretical Active Warps per SM                                                   warp                             64\n",
            "    Theoretical Occupancy                                                                %                            100\n",
            "    Achieved Occupancy                                                                   %                          11.51\n",
            "    Achieved Active Warps Per SM                                                      warp                           7.36\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     \n",
            "          theoretical (100.0%) and measured achieved occupancy (11.5%) can be the result of warp scheduling overheads   \n",
            "          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    \n",
            "          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                \n",
            "          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n",
            "  laplace2d_swap_50_gpu, 2022-Sep-01 08:41:26, Context 1, Stream 13\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    DRAM Frequency                                                           cycle/nsecond                           1.22\n",
            "    SM Frequency                                                             cycle/nsecond                           1.09\n",
            "    Elapsed Cycles                                                                   cycle                        222,706\n",
            "    Memory [%]                                                                           %                          80.60\n",
            "    DRAM Throughput                                                                      %                          80.60\n",
            "    Duration                                                                       usecond                         202.78\n",
            "    L1/TEX Cache Throughput                                                              %                          28.06\n",
            "    L2 Cache Throughput                                                                  %                          68.66\n",
            "    SM Active Cycles                                                                 cycle                     210,876.55\n",
            "    Compute (SM) [%]                                                                     %                          10.63\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   \n",
            "          further improve performance, work will likely need to be shifted from the most utilized to another unit.      \n",
            "          Start by analyzing DRAM in the Memory Workload Analysis section.                                              \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Size                                                                                                        128\n",
            "    Function Cache Configuration                                                                  cudaFuncCachePreferNone\n",
            "    Grid Size                                                                                                       4,094\n",
            "    Registers Per Thread                                                   register/thread                             25\n",
            "    Shared Memory Configuration Size                                                 Kbyte                          32.77\n",
            "    Driver Shared Memory Per Block                                             Kbyte/block                           1.02\n",
            "    Dynamic Shared Memory Per Block                                             byte/block                              0\n",
            "    Static Shared Memory Per Block                                              byte/block                              0\n",
            "    Threads                                                                         thread                        524,032\n",
            "    Waves Per SM                                                                                                     2.37\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "\n",
            "    Section: Occupancy\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Limit SM                                                                   block                             32\n",
            "    Block Limit Registers                                                            block                             16\n",
            "    Block Limit Shared Mem                                                           block                             32\n",
            "    Block Limit Warps                                                                block                             16\n",
            "    Theoretical Active Warps per SM                                                   warp                             64\n",
            "    Theoretical Occupancy                                                                %                            100\n",
            "    Achieved Occupancy                                                                   %                          86.94\n",
            "    Achieved Active Warps Per SM                                                      warp                          55.64\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     \n",
            "          theoretical (100.0%) and measured achieved occupancy (86.9%) can be the result of warp scheduling overheads   \n",
            "          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    \n",
            "          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                \n",
            "          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n",
            "  laplace2d_calcnext_32_gpu, 2022-Sep-01 08:41:27, Context 1, Stream 13\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    DRAM Frequency                                                           cycle/nsecond                           1.20\n",
            "    SM Frequency                                                             cycle/nsecond                           1.07\n",
            "    Elapsed Cycles                                                                   cycle                        236,941\n",
            "    Memory [%]                                                                           %                          75.88\n",
            "    DRAM Throughput                                                                      %                          75.88\n",
            "    Duration                                                                       usecond                         220.03\n",
            "    L1/TEX Cache Throughput                                                              %                          33.71\n",
            "    L2 Cache Throughput                                                                  %                          77.86\n",
            "    SM Active Cycles                                                                 cycle                     227,216.81\n",
            "    Compute (SM) [%]                                                                     %                          16.61\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    \n",
            "          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       \n",
            "          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  \n",
            "          whether there are values you can (re)compute.                                                                 \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Size                                                                                                        128\n",
            "    Function Cache Configuration                                                                  cudaFuncCachePreferNone\n",
            "    Grid Size                                                                                                       4,094\n",
            "    Registers Per Thread                                                   register/thread                             48\n",
            "    Shared Memory Configuration Size                                                 Kbyte                          65.54\n",
            "    Driver Shared Memory Per Block                                             Kbyte/block                           1.02\n",
            "    Dynamic Shared Memory Per Block                                            Kbyte/block                           2.05\n",
            "    Static Shared Memory Per Block                                              byte/block                              0\n",
            "    Threads                                                                         thread                        524,032\n",
            "    Waves Per SM                                                                                                     3.79\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "\n",
            "    Section: Occupancy\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Limit SM                                                                   block                             32\n",
            "    Block Limit Registers                                                            block                             10\n",
            "    Block Limit Shared Mem                                                           block                             21\n",
            "    Block Limit Warps                                                                block                             16\n",
            "    Theoretical Active Warps per SM                                                   warp                             40\n",
            "    Theoretical Occupancy                                                                %                          62.50\n",
            "    Achieved Occupancy                                                                   %                          59.80\n",
            "    Achieved Active Warps Per SM                                                      warp                          38.27\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   This kernel's theoretical occupancy (62.5%) is limited by the number of required registers See the CUDA Best  \n",
            "          Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more      \n",
            "          details on optimizing occupancy.                                                                              \n",
            "\n",
            "  laplace2d_calcnext_32_gpu__red, 2022-Sep-01 08:41:27, Context 1, Stream 13\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    DRAM Frequency                                                           cycle/nsecond                           1.11\n",
            "    SM Frequency                                                             cycle/usecond                         992.76\n",
            "    Elapsed Cycles                                                                   cycle                         14,090\n",
            "    Memory [%]                                                                           %                           0.27\n",
            "    DRAM Throughput                                                                      %                           0.16\n",
            "    Duration                                                                       usecond                          14.14\n",
            "    L1/TEX Cache Throughput                                                              %                           4.75\n",
            "    L2 Cache Throughput                                                                  %                           0.27\n",
            "    SM Active Cycles                                                                 cycle                         109.86\n",
            "    Compute (SM) [%]                                                                     %                           0.04\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Size                                                                                                        256\n",
            "    Function Cache Configuration                                                                  cudaFuncCachePreferNone\n",
            "    Grid Size                                                                                                           1\n",
            "    Registers Per Thread                                                   register/thread                             24\n",
            "    Shared Memory Configuration Size                                                 Kbyte                          65.54\n",
            "    Driver Shared Memory Per Block                                             Kbyte/block                           1.02\n",
            "    Dynamic Shared Memory Per Block                                            Kbyte/block                           2.05\n",
            "    Static Shared Memory Per Block                                              byte/block                              0\n",
            "    Threads                                                                         thread                            256\n",
            "    Waves Per SM                                                                                                     0.00\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Limit SM                                                                   block                             32\n",
            "    Block Limit Registers                                                            block                             10\n",
            "    Block Limit Shared Mem                                                           block                             21\n",
            "    Block Limit Warps                                                                block                              8\n",
            "    Theoretical Active Warps per SM                                                   warp                             64\n",
            "    Theoretical Occupancy                                                                %                            100\n",
            "    Achieved Occupancy                                                                   %                          11.52\n",
            "    Achieved Active Warps Per SM                                                      warp                           7.37\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     \n",
            "          theoretical (100.0%) and measured achieved occupancy (11.5%) can be the result of warp scheduling overheads   \n",
            "          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    \n",
            "          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                \n",
            "          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n",
            "  laplace2d_swap_50_gpu, 2022-Sep-01 08:41:28, Context 1, Stream 13\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    DRAM Frequency                                                           cycle/nsecond                           1.20\n",
            "    SM Frequency                                                             cycle/nsecond                           1.08\n",
            "    Elapsed Cycles                                                                   cycle                        220,859\n",
            "    Memory [%]                                                                           %                          81.28\n",
            "    DRAM Throughput                                                                      %                          81.28\n",
            "    Duration                                                                       usecond                         203.81\n",
            "    L1/TEX Cache Throughput                                                              %                          28.10\n",
            "    L2 Cache Throughput                                                                  %                          69.31\n",
            "    SM Active Cycles                                                                 cycle                     210,641.94\n",
            "    Compute (SM) [%]                                                                     %                          10.72\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   \n",
            "          further improve performance, work will likely need to be shifted from the most utilized to another unit.      \n",
            "          Start by analyzing DRAM in the Memory Workload Analysis section.                                              \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Size                                                                                                        128\n",
            "    Function Cache Configuration                                                                  cudaFuncCachePreferNone\n",
            "    Grid Size                                                                                                       4,094\n",
            "    Registers Per Thread                                                   register/thread                             25\n",
            "    Shared Memory Configuration Size                                                 Kbyte                          32.77\n",
            "    Driver Shared Memory Per Block                                             Kbyte/block                           1.02\n",
            "    Dynamic Shared Memory Per Block                                             byte/block                              0\n",
            "    Static Shared Memory Per Block                                              byte/block                              0\n",
            "    Threads                                                                         thread                        524,032\n",
            "    Waves Per SM                                                                                                     2.37\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "\n",
            "    Section: Occupancy\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Limit SM                                                                   block                             32\n",
            "    Block Limit Registers                                                            block                             16\n",
            "    Block Limit Shared Mem                                                           block                             32\n",
            "    Block Limit Warps                                                                block                             16\n",
            "    Theoretical Active Warps per SM                                                   warp                             64\n",
            "    Theoretical Occupancy                                                                %                            100\n",
            "    Achieved Occupancy                                                                   %                          86.74\n",
            "    Achieved Active Warps Per SM                                                      warp                          55.51\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     \n",
            "          theoretical (100.0%) and measured achieved occupancy (86.7%) can be the result of warp scheduling overheads   \n",
            "          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    \n",
            "          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                \n",
            "          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n",
            "  laplace2d_calcnext_32_gpu, 2022-Sep-01 08:41:29, Context 1, Stream 13\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    DRAM Frequency                                                           cycle/nsecond                           1.20\n",
            "    SM Frequency                                                             cycle/nsecond                           1.08\n",
            "    Elapsed Cycles                                                                   cycle                        236,196\n",
            "    Memory [%]                                                                           %                          76.11\n",
            "    DRAM Throughput                                                                      %                          76.11\n",
            "    Duration                                                                       usecond                         217.95\n",
            "    L1/TEX Cache Throughput                                                              %                          33.49\n",
            "    L2 Cache Throughput                                                                  %                          78.78\n",
            "    SM Active Cycles                                                                 cycle                     228,812.55\n",
            "    Compute (SM) [%]                                                                     %                          16.66\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    \n",
            "          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       \n",
            "          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  \n",
            "          whether there are values you can (re)compute.                                                                 \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Size                                                                                                        128\n",
            "    Function Cache Configuration                                                                  cudaFuncCachePreferNone\n",
            "    Grid Size                                                                                                       4,094\n",
            "    Registers Per Thread                                                   register/thread                             48\n",
            "    Shared Memory Configuration Size                                                 Kbyte                          65.54\n",
            "    Driver Shared Memory Per Block                                             Kbyte/block                           1.02\n",
            "    Dynamic Shared Memory Per Block                                            Kbyte/block                           2.05\n",
            "    Static Shared Memory Per Block                                              byte/block                              0\n",
            "    Threads                                                                         thread                        524,032\n",
            "    Waves Per SM                                                                                                     3.79\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "\n",
            "    Section: Occupancy\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Limit SM                                                                   block                             32\n",
            "    Block Limit Registers                                                            block                             10\n",
            "    Block Limit Shared Mem                                                           block                             21\n",
            "    Block Limit Warps                                                                block                             16\n",
            "    Theoretical Active Warps per SM                                                   warp                             40\n",
            "    Theoretical Occupancy                                                                %                          62.50\n",
            "    Achieved Occupancy                                                                   %                          59.61\n",
            "    Achieved Active Warps Per SM                                                      warp                          38.15\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   This kernel's theoretical occupancy (62.5%) is limited by the number of required registers See the CUDA Best  \n",
            "          Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more      \n",
            "          details on optimizing occupancy.                                                                              \n",
            "\n",
            "  laplace2d_calcnext_32_gpu__red, 2022-Sep-01 08:41:29, Context 1, Stream 13\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    DRAM Frequency                                                           cycle/nsecond                           1.11\n",
            "    SM Frequency                                                             cycle/usecond                         998.95\n",
            "    Elapsed Cycles                                                                   cycle                         13,959\n",
            "    Memory [%]                                                                           %                           0.27\n",
            "    DRAM Throughput                                                                      %                           0.17\n",
            "    Duration                                                                       usecond                          13.92\n",
            "    L1/TEX Cache Throughput                                                              %                           4.69\n",
            "    L2 Cache Throughput                                                                  %                           0.27\n",
            "    SM Active Cycles                                                                 cycle                         111.25\n",
            "    Compute (SM) [%]                                                                     %                           0.04\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Size                                                                                                        256\n",
            "    Function Cache Configuration                                                                  cudaFuncCachePreferNone\n",
            "    Grid Size                                                                                                           1\n",
            "    Registers Per Thread                                                   register/thread                             24\n",
            "    Shared Memory Configuration Size                                                 Kbyte                          65.54\n",
            "    Driver Shared Memory Per Block                                             Kbyte/block                           1.02\n",
            "    Dynamic Shared Memory Per Block                                            Kbyte/block                           2.05\n",
            "    Static Shared Memory Per Block                                              byte/block                              0\n",
            "    Threads                                                                         thread                            256\n",
            "    Waves Per SM                                                                                                     0.00\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Limit SM                                                                   block                             32\n",
            "    Block Limit Registers                                                            block                             10\n",
            "    Block Limit Shared Mem                                                           block                             21\n",
            "    Block Limit Warps                                                                block                              8\n",
            "    Theoretical Active Warps per SM                                                   warp                             64\n",
            "    Theoretical Occupancy                                                                %                            100\n",
            "    Achieved Occupancy                                                                   %                          11.54\n",
            "    Achieved Active Warps Per SM                                                      warp                           7.39\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     \n",
            "          theoretical (100.0%) and measured achieved occupancy (11.5%) can be the result of warp scheduling overheads   \n",
            "          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    \n",
            "          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                \n",
            "          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n",
            "  laplace2d_swap_50_gpu, 2022-Sep-01 08:41:30, Context 1, Stream 13\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    DRAM Frequency                                                           cycle/nsecond                           1.19\n",
            "    SM Frequency                                                             cycle/nsecond                           1.07\n",
            "    Elapsed Cycles                                                                   cycle                        219,564\n",
            "    Memory [%]                                                                           %                          81.64\n",
            "    DRAM Throughput                                                                      %                          81.64\n",
            "    Duration                                                                       usecond                         205.02\n",
            "    L1/TEX Cache Throughput                                                              %                          28.07\n",
            "    L2 Cache Throughput                                                                  %                          69.44\n",
            "    SM Active Cycles                                                                 cycle                     210,934.78\n",
            "    Compute (SM) [%]                                                                     %                          10.77\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   \n",
            "          further improve performance, work will likely need to be shifted from the most utilized to another unit.      \n",
            "          Start by analyzing DRAM in the Memory Workload Analysis section.                                              \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Size                                                                                                        128\n",
            "    Function Cache Configuration                                                                  cudaFuncCachePreferNone\n",
            "    Grid Size                                                                                                       4,094\n",
            "    Registers Per Thread                                                   register/thread                             25\n",
            "    Shared Memory Configuration Size                                                 Kbyte                          32.77\n",
            "    Driver Shared Memory Per Block                                             Kbyte/block                           1.02\n",
            "    Dynamic Shared Memory Per Block                                             byte/block                              0\n",
            "    Static Shared Memory Per Block                                              byte/block                              0\n",
            "    Threads                                                                         thread                        524,032\n",
            "    Waves Per SM                                                                                                     2.37\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "\n",
            "    Section: Occupancy\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Limit SM                                                                   block                             32\n",
            "    Block Limit Registers                                                            block                             16\n",
            "    Block Limit Shared Mem                                                           block                             32\n",
            "    Block Limit Warps                                                                block                             16\n",
            "    Theoretical Active Warps per SM                                                   warp                             64\n",
            "    Theoretical Occupancy                                                                %                            100\n",
            "    Achieved Occupancy                                                                   %                          86.69\n",
            "    Achieved Active Warps Per SM                                                      warp                          55.48\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     \n",
            "          theoretical (100.0%) and measured achieved occupancy (86.7%) can be the result of warp scheduling overheads   \n",
            "          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    \n",
            "          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                \n",
            "          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n",
            "  laplace2d_calcnext_32_gpu, 2022-Sep-01 08:41:31, Context 1, Stream 13\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    DRAM Frequency                                                           cycle/nsecond                           1.21\n",
            "    SM Frequency                                                             cycle/nsecond                           1.09\n",
            "    Elapsed Cycles                                                                   cycle                        237,968\n",
            "    Memory [%]                                                                           %                          75.50\n",
            "    DRAM Throughput                                                                      %                          75.50\n",
            "    Duration                                                                       usecond                         218.14\n",
            "    L1/TEX Cache Throughput                                                              %                          33.62\n",
            "    L2 Cache Throughput                                                                  %                          77.24\n",
            "    SM Active Cycles                                                                 cycle                     227,965.51\n",
            "    Compute (SM) [%]                                                                     %                          16.54\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    \n",
            "          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       \n",
            "          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  \n",
            "          whether there are values you can (re)compute.                                                                 \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Size                                                                                                        128\n",
            "    Function Cache Configuration                                                                  cudaFuncCachePreferNone\n",
            "    Grid Size                                                                                                       4,094\n",
            "    Registers Per Thread                                                   register/thread                             48\n",
            "    Shared Memory Configuration Size                                                 Kbyte                          65.54\n",
            "    Driver Shared Memory Per Block                                             Kbyte/block                           1.02\n",
            "    Dynamic Shared Memory Per Block                                            Kbyte/block                           2.05\n",
            "    Static Shared Memory Per Block                                              byte/block                              0\n",
            "    Threads                                                                         thread                        524,032\n",
            "    Waves Per SM                                                                                                     3.79\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "\n",
            "    Section: Occupancy\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Limit SM                                                                   block                             32\n",
            "    Block Limit Registers                                                            block                             10\n",
            "    Block Limit Shared Mem                                                           block                             21\n",
            "    Block Limit Warps                                                                block                             16\n",
            "    Theoretical Active Warps per SM                                                   warp                             40\n",
            "    Theoretical Occupancy                                                                %                          62.50\n",
            "    Achieved Occupancy                                                                   %                          59.91\n",
            "    Achieved Active Warps Per SM                                                      warp                          38.34\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   This kernel's theoretical occupancy (62.5%) is limited by the number of required registers See the CUDA Best  \n",
            "          Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more      \n",
            "          details on optimizing occupancy.                                                                              \n",
            "\n",
            "  laplace2d_calcnext_32_gpu__red, 2022-Sep-01 08:41:31, Context 1, Stream 13\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    DRAM Frequency                                                           cycle/nsecond                           1.10\n",
            "    SM Frequency                                                             cycle/usecond                         993.25\n",
            "    Elapsed Cycles                                                                   cycle                         13,694\n",
            "    Memory [%]                                                                           %                           0.28\n",
            "    DRAM Throughput                                                                      %                           0.17\n",
            "    Duration                                                                       usecond                          13.76\n",
            "    L1/TEX Cache Throughput                                                              %                           4.79\n",
            "    L2 Cache Throughput                                                                  %                           0.28\n",
            "    SM Active Cycles                                                                 cycle                         108.75\n",
            "    Compute (SM) [%]                                                                     %                           0.04\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Size                                                                                                        256\n",
            "    Function Cache Configuration                                                                  cudaFuncCachePreferNone\n",
            "    Grid Size                                                                                                           1\n",
            "    Registers Per Thread                                                   register/thread                             24\n",
            "    Shared Memory Configuration Size                                                 Kbyte                          65.54\n",
            "    Driver Shared Memory Per Block                                             Kbyte/block                           1.02\n",
            "    Dynamic Shared Memory Per Block                                            Kbyte/block                           2.05\n",
            "    Static Shared Memory Per Block                                              byte/block                              0\n",
            "    Threads                                                                         thread                            256\n",
            "    Waves Per SM                                                                                                     0.00\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    Block Limit SM                                                                   block                             32\n",
            "    Block Limit Registers                                                            block                             10\n",
            "    Block Limit Shared Mem                                                           block                             21\n",
            "    Block Limit Warps                                                                block                              8\n",
            "    Theoretical Active Warps per SM                                                   warp                             64\n",
            "    Theoretical Occupancy                                                                %                            100\n",
            "    Achieved Occupancy                                                                   %                          11.52\n",
            "    Achieved Active Warps Per SM                                                      warp                           7.37\n",
            "    ---------------------------------------------------------------------- --------------- ------------------------------\n",
            "    WRN   This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     \n",
            "          theoretical (100.0%) and measured achieved occupancy (11.5%) can be the result of warp scheduling overheads   \n",
            "          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    \n",
            "          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                \n",
            "          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## add nvtx "
      ],
      "metadata": {
        "id": "zwZjniFJN-oh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file Makefile\n",
        "\n",
        "FC := nvfortran\n",
        "ACCFLAGS:= -fast -ta=tesla -Minfo=accel\n",
        "\n",
        "laplace: laplace2d.f90 jacobi.f90 nvtx.f90\n",
        "\t${FC} ${ACCFLAGS} -o laplace laplace2d.f90 jacobi.f90 nvtx.f90 -L/opt/nvidia/hpc_sdk/Linux_x86_64/22.7/cuda/11.7/lib64  -lnvToolsExt\n",
        "\n",
        "clean:\n",
        "\trm -f *.o laplace "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTHEuoZzOBRc",
        "outputId": "837bc153-08ec-47e4-cd53-b1a1251be6e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting Makefile\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file nvtx.f90\n",
        "\n",
        "module nvtx\n",
        "use iso_c_binding\n",
        "implicit none\n",
        "\n",
        "integer,private :: col(7) = [ Z'0000ff00', Z'000000ff', Z'00ffff00', Z'00ff00ff', Z'0000ffff', Z'00ff0000', Z'00ffffff']\n",
        "character,private,target :: tempName(256)\n",
        "\n",
        "type, bind(C):: nvtxEventAttributes\n",
        "  integer(C_INT16_T):: version=1\n",
        "  integer(C_INT16_T):: size=48 !\n",
        "  integer(C_INT):: category=0\n",
        "  integer(C_INT):: colorType=1 ! NVTX_COLOR_ARGB = 1\n",
        "  integer(C_INT):: color\n",
        "  integer(C_INT):: payloadType=0 ! NVTX_PAYLOAD_UNKNOWN = 0\n",
        "  integer(C_INT):: reserved0\n",
        "  integer(C_INT64_T):: payload   ! union uint,int,double\n",
        "  integer(C_INT):: messageType=1  ! NVTX_MESSAGE_TYPE_ASCII     = 1 \n",
        "  type(C_PTR):: message  ! ascii char\n",
        "end type\n",
        "\n",
        "interface nvtxRangePush\n",
        "  ! push range with custom label and standard color\n",
        "  subroutine nvtxRangePushA(name) bind(C, name='nvtxRangePushA')\n",
        "  use iso_c_binding\n",
        "  character(kind=C_CHAR) :: name(256)\n",
        "  end subroutine\n",
        "\n",
        "  ! push range with custom label and custom color\n",
        "  subroutine nvtxRangePushEx(event) bind(C, name='nvtxRangePushEx')\n",
        "  use iso_c_binding\n",
        "  import:: nvtxEventAttributes\n",
        "  type(nvtxEventAttributes):: event\n",
        "  end subroutine\n",
        "end interface\n",
        "\n",
        "interface nvtxRangePop\n",
        "  subroutine nvtxRangePop() bind(C, name='nvtxRangePop')\n",
        "  end subroutine\n",
        "end interface\n",
        "\n",
        "contains\n",
        "\n",
        "subroutine nvtxStartRange(name,id)\n",
        "  character(kind=c_char,len=*) :: name\n",
        "  integer, optional:: id\n",
        "  type(nvtxEventAttributes):: event\n",
        "  character(kind=c_char,len=256) :: trimmed_name\n",
        "  integer:: i\n",
        "\n",
        "  trimmed_name=trim(name)//c_null_char\n",
        "\n",
        "  ! move scalar trimmed_name into character array tempName\n",
        "  do i=1,LEN(trim(name)) + 1\n",
        "     tempName(i) = trimmed_name(i:i)\n",
        "  enddo\n",
        "\n",
        "\n",
        "  if ( .not. present(id)) then\n",
        "    call nvtxRangePush(tempName)\n",
        "  else\n",
        "    event%color=col(mod(id,7)+1)\n",
        "    event%message=c_loc(tempName)\n",
        "    call nvtxRangePushEx(event)\n",
        "  end if\n",
        "end subroutine\n",
        "\n",
        "subroutine nvtxEndRange\n",
        "  call nvtxRangePop\n",
        "end subroutine\n",
        "\n",
        "end module nvtx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6TTIXA8OXHX",
        "outputId": "a5999977-35e3-4452-d61b-95a8f715e6bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting nvtx.f90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file jacobi.f90\n",
        "\n",
        "\n",
        "program jacobi\n",
        "  use laplace2d\n",
        "  use nvtx\n",
        "  \n",
        "  implicit none\n",
        "  integer, parameter :: fp_kind=kind(1.0d0)\n",
        "  integer, parameter :: n=4096, m=4096, iter_max=1000\n",
        "  integer :: i, j, iter\n",
        "  real(fp_kind), dimension (:,:), allocatable :: A, Anew\n",
        "  real(fp_kind) :: tol=1.0e-6_fp_kind, error=1.0_fp_kind\n",
        "  real(fp_kind) :: start_time, stop_time\n",
        "\n",
        "  ! allocate ( A(0:n-1,0:m-1), Anew(0:n-1,0:m-1) )\n",
        "\n",
        "  ! A    = 0.0_fp_kind\n",
        "  ! Anew = 0.0_fp_kind\n",
        "\n",
        "  ! Set B.C.\n",
        "  ! A(0,:)    = 1.0_fp_kind\n",
        "  ! Anew(0,:) = 1.0_fp_kind\n",
        "  \n",
        "  call nvtxStartRange(\"init\")\n",
        "  call initialize(A, Anew, m, n)\n",
        "  call nvtxEndRange\n",
        "   \n",
        "  write(*,'(a,i5,a,i5,a)') 'Jacobi relaxation Calculation:', n, ' x', m, ' mesh'\n",
        " \n",
        "  call cpu_time(start_time) \n",
        "\n",
        "  iter=0\n",
        "  \n",
        "  call nvtxStartRange(\"outer loop\")\n",
        "  !$acc data copy(A) create(Anew)\n",
        "  do while ( error .gt. tol .and. iter .lt. iter_max )\n",
        "\n",
        "    error = calcNext(A, Anew, m, n)\n",
        "    call swap(A, Anew, m, n)\n",
        "\n",
        "    if(mod(iter,100).eq.0 ) write(*,'(i5,f10.6)'), iter, error\n",
        "\n",
        "    iter = iter + 1\n",
        "\n",
        "  end do\n",
        "  !$acc end data\n",
        "  call nvtxEndRange\n",
        "\n",
        "  call cpu_time(stop_time) \n",
        "  write(*,'(a,f10.3,a)')  ' completed in ', stop_time-start_time, ' seconds'\n",
        "\n",
        "  ! deallocate (A,Anew)\n",
        "end program jacobi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "As3clmyWOItf",
        "outputId": "2c1a3f72-169d-4a9d-f91d-df037a7e519b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting jacobi.f90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file laplace2d.f90 \n",
        "\n",
        "module laplace2d\n",
        "  public :: initialize\n",
        "  public :: calcNext\n",
        "  public :: swap\n",
        "  public :: dealloc\n",
        "  contains\n",
        "    subroutine initialize(A, Anew, m, n)\n",
        "      integer, parameter :: fp_kind=kind(1.0d0)\n",
        "      real(fp_kind),allocatable,intent(out)   :: A(:,:)\n",
        "      real(fp_kind),allocatable,intent(out)   :: Anew(:,:)\n",
        "      integer,intent(in)          :: m, n\n",
        "\n",
        "      allocate ( A(0:n-1,0:m-1), Anew(0:n-1,0:m-1) )\n",
        "\n",
        "      A    = 0.0_fp_kind\n",
        "      Anew = 0.0_fp_kind\n",
        "\n",
        "      A(0,:)    = 1.0_fp_kind\n",
        "      Anew(0,:) = 1.0_fp_kind\n",
        "    end subroutine initialize\n",
        "\n",
        "    function calcNext(A, Anew, m, n)\n",
        "      integer, parameter          :: fp_kind=kind(1.0d0)\n",
        "      real(fp_kind),intent(inout) :: A(0:n-1,0:m-1)\n",
        "      real(fp_kind),intent(inout) :: Anew(0:n-1,0:m-1)\n",
        "      integer,intent(in)          :: m, n\n",
        "      integer                     :: i, j\n",
        "      real(fp_kind)               :: error\n",
        "\n",
        "      error=0.0_fp_kind\n",
        "\n",
        "      !$acc parallel loop reduction(max:error) copyin(A) copyout(Anew)\n",
        "      do j=1,m-2\n",
        "        do i=1,n-2\n",
        "          Anew(i,j) = 0.25_fp_kind * ( A(i+1,j  ) + A(i-1,j  ) + &\n",
        "                                       A(i  ,j-1) + A(i  ,j+1) )\n",
        "          error = max( error, abs(Anew(i,j)-A(i,j)) )\n",
        "        end do\n",
        "      end do\n",
        "      calcNext = error\n",
        "    end function calcNext\n",
        "\n",
        "    subroutine swap(A, Anew, m, n)\n",
        "      integer, parameter        :: fp_kind=kind(1.0d0)\n",
        "      real(fp_kind),intent(out) :: A(0:n-1,0:m-1)\n",
        "      real(fp_kind),intent(in)  :: Anew(0:n-1,0:m-1)\n",
        "      integer,intent(in)        :: m, n\n",
        "      integer                   :: i, j\n",
        "\n",
        "      !$acc parallel loop copyin(Anew) copyout(A)\n",
        "      do j=1,m-2\n",
        "        do i=1,n-2\n",
        "          A(i,j) = Anew(i,j)\n",
        "        end do\n",
        "      end do\n",
        "    end subroutine swap\n",
        "\n",
        "    subroutine dealloc(A, Anew)\n",
        "      integer, parameter :: fp_kind=kind(1.0d0)\n",
        "      real(fp_kind),allocatable,intent(in) :: A\n",
        "      real(fp_kind),allocatable,intent(in) :: Anew\n",
        "\t  \n",
        "\t  deallocate (A,Anew)\n",
        "    end subroutine\n",
        "end module laplace2d"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewAp0mf4ONXO",
        "outputId": "d7860a13-9882-4ba8-f7a4-c1deacb0a0ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting laplace2d.f90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!make clean && make &&  ./laplace "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEs6Nd85PWq0",
        "outputId": "328af761-c8e6-43ce-89d7-f0349be13b6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm -f *.o laplace \n",
            "nvfortran -fast -ta=tesla -Minfo=accel -o laplace laplace2d.f90 jacobi.f90 nvtx.f90 -L/opt/nvidia/hpc_sdk/Linux_x86_64/22.7/cuda/11.7/lib64  -lnvToolsExt\n",
            "laplace2d.f90:\n",
            "calcnext:\n",
            "     33, Generating copyin(a(:,:)) [if not already present]\n",
            "         Generating NVIDIA GPU code\n",
            "         34, !$acc loop gang ! blockidx%x\n",
            "             Generating reduction(max:error)\n",
            "         35, !$acc loop vector(128) ! threadidx%x\n",
            "     33, Generating implicit copy(error) [if not already present]\n",
            "         Generating copyout(anew(:,:)) [if not already present]\n",
            "     35, Loop is parallelizable\n",
            "swap:\n",
            "     51, Generating copyout(a(:,:)) [if not already present]\n",
            "         Generating copyin(anew(:,:)) [if not already present]\n",
            "         Generating NVIDIA GPU code\n",
            "         52, !$acc loop gang ! blockidx%x\n",
            "         53, !$acc loop vector(128) ! threadidx%x\n",
            "     53, Loop is parallelizable\n",
            "jacobi.f90:\n",
            "jacobi:\n",
            "     34, Generating create(anew(:,:)) [if not already present]\n",
            "         Generating copy(a(:,:)) [if not already present]\n",
            "nvtx.f90:\n",
            "Jacobi relaxation Calculation: 4096 x 4096 mesh\n",
            "    0  0.250000\n",
            "  100  0.002397\n",
            "  200  0.001204\n",
            "  300  0.000804\n",
            "  400  0.000603\n",
            "  500  0.000483\n",
            "  600  0.000403\n",
            "  700  0.000345\n",
            "  800  0.000302\n",
            "  900  0.000269\n",
            " completed in      0.705 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nsys profile -t openacc,nvtx --stats=true   ./laplace"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3J6diHLPSeL",
        "outputId": "6fcd9681-34d2-46b8-986a-6ff7984d3080"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
            "WARNING: CUDA tracing has been automatically enabled since it is a prerequisite for tracing OpenACC.\n",
            "Jacobi relaxation Calculation: 4096 x 4096 mesh\n",
            "    0  0.250000\n",
            "  100  0.002397\n",
            "  200  0.001204\n",
            "  300  0.000804\n",
            "  400  0.000603\n",
            "  500  0.000483\n",
            "  600  0.000403\n",
            "  700  0.000345\n",
            "  800  0.000302\n",
            "  900  0.000269\n",
            " completed in      1.127 seconds\n",
            "Generating '/tmp/nsys-report-a64f.qdstrm'\n",
            "[1/8] [========================100%] report6.nsys-rep\n",
            "[2/8] [========================100%] report6.sqlite\n",
            "[3/8] Executing 'nvtxsum' stats report\n",
            "\n",
            "NVTX Range Statistics:\n",
            "\n",
            " Time (%)  Total Time (ns)  Instances     Avg (ns)         Med (ns)        Min (ns)       Max (ns)     StdDev (ns)   Style     Range   \n",
            " --------  ---------------  ---------  ---------------  ---------------  -------------  -------------  -----------  -------  ----------\n",
            "     90.8    1,127,155,942          1  1,127,155,942.0  1,127,155,942.0  1,127,155,942  1,127,155,942          0.0  PushPop  outer loop\n",
            "      9.2      114,647,348          1    114,647,348.0    114,647,348.0    114,647,348    114,647,348          0.0  PushPop  init      \n",
            "\n",
            "[4/8] Executing 'cudaapisum' stats report\n",
            "\n",
            "CUDA API Statistics:\n",
            "\n",
            " Time (%)  Total Time (ns)  Num Calls    Avg (ns)      Med (ns)     Min (ns)    Max (ns)   StdDev (ns)          Name        \n",
            " --------  ---------------  ---------  ------------  ------------  ----------  ----------  -----------  --------------------\n",
            "     87.6      473,980,915      6,002      78,970.5       9,006.0       1,162   1,272,841    104,911.3  cuStreamSynchronize \n",
            "      5.0       27,323,624          1  27,323,624.0  27,323,624.0  27,323,624  27,323,624          0.0  cuMemHostAlloc      \n",
            "      3.1       17,028,362      3,000       5,676.1       5,286.5       3,519      62,556      2,251.7  cuLaunchKernel      \n",
            "      2.0       10,858,382      1,024      10,603.9       1,621.0       1,513   1,284,099    105,572.7  cuEventSynchronize  \n",
            "      0.9        4,949,900      1,009       4,905.7       4,560.0       2,992      21,641      1,685.2  cuMemcpyDtoHAsync_v2\n",
            "      0.6        3,355,004      1,000       3,355.0       2,991.0       2,377      18,210      1,669.3  cuMemsetD32Async    \n",
            "      0.3        1,620,424      1,026       1,579.4       1,457.0       1,183      10,178        667.5  cuEventRecord       \n",
            "      0.2          985,904          1     985,904.0     985,904.0     985,904     985,904          0.0  cuMemAllocHost_v2   \n",
            "      0.2          847,945          7     121,135.0       9,783.0       4,024     337,792    151,875.2  cuMemAlloc_v2       \n",
            "      0.0          183,699         10      18,369.9      18,506.5       8,414      26,778      4,804.0  cuMemcpyHtoDAsync_v2\n",
            "      0.0          159,549          1     159,549.0     159,549.0     159,549     159,549          0.0  cuModuleLoadDataEx  \n",
            "      0.0            7,448          4       1,862.0       2,100.5         727       2,520        802.9  cuEventCreate       \n",
            "\n",
            "[5/8] Executing 'gpukernsum' stats report\n",
            "\n",
            "CUDA Kernel Statistics:\n",
            "\n",
            " Time (%)  Total Time (ns)  Instances  Avg (ns)   Med (ns)   Min (ns)  Max (ns)  StdDev (ns)               Name             \n",
            " --------  ---------------  ---------  ---------  ---------  --------  --------  -----------  ------------------------------\n",
            "     49.8      221,083,109      1,000  221,083.1  219,662.5   218,078   227,103      2,227.0  laplace2d_calcnext_33_gpu     \n",
            "     47.4      210,464,670      1,000  210,464.7  210,303.0   207,294   216,575      1,330.5  laplace2d_swap_51_gpu         \n",
            "      2.9       12,780,692      1,000   12,780.7   12,064.0    11,488    14,689      1,100.0  laplace2d_calcnext_33_gpu__red\n",
            "\n",
            "[6/8] Executing 'gpumemtimesum' stats report\n",
            "\n",
            "CUDA Memory Operation Statistics (by time):\n",
            "\n",
            " Time (%)  Total Time (ns)  Count   Avg (ns)     Med (ns)    Min (ns)  Max (ns)   StdDev (ns)      Operation     \n",
            " --------  ---------------  -----  -----------  -----------  --------  ---------  -----------  ------------------\n",
            "     48.6       12,124,156  1,009     12,016.0      1,888.0     1,823  1,266,711    112,165.1  [CUDA memcpy DtoH]\n",
            "     43.7       10,899,729     10  1,089,972.9  1,361,526.5     2,655  1,362,999    572,949.1  [CUDA memcpy HtoD]\n",
            "      7.8        1,940,388  1,000      1,940.4      1,824.0     1,791      2,560        152.9  [CUDA memset]     \n",
            "\n",
            "[7/8] Executing 'gpumemsizesum' stats report\n",
            "\n",
            "CUDA Memory Operation Statistics (by size):\n",
            "\n",
            " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)      Operation     \n",
            " ----------  -----  --------  --------  --------  --------  -----------  ------------------\n",
            "    134.226  1,009     0.133     0.000     0.000    16.777        1.489  [CUDA memcpy DtoH]\n",
            "    134.218     10    13.422    16.777     0.000    16.777        7.074  [CUDA memcpy HtoD]\n",
            "      0.008  1,000     0.000     0.000     0.000     0.000        0.000  [CUDA memset]     \n",
            "\n",
            "[8/8] Executing 'openaccsum' stats report\n",
            "\n",
            "OpenACC event Statistics:\n",
            "\n",
            " Time(%)  Total Time (ns)  Num Calls    Avg (ns)      Med (ns)     Min (ns)    Max (ns)   StdDev (ns)                 Name               \n",
            " -------  ---------------  ---------  ------------  ------------  ----------  ----------  -----------  ----------------------------------\n",
            "    22.0      250,607,357      1,000     250,607.4     250,127.0     244,064     335,659      5,314.0  Compute Construct@laplace2d.f90:33\n",
            "    21.8      247,997,804      3,000      82,665.9       9,736.5       2,324     255,451    107,495.5  Wait@laplace2d.f90:33             \n",
            "    19.8      225,559,442      1,000     225,559.4     224,978.5     221,206     253,767      2,965.0  Compute Construct@laplace2d.f90:51\n",
            "    19.2      218,893,832      2,000     109,446.9     113,337.0       2,274     238,267    106,606.6  Wait@laplace2d.f90:51             \n",
            "     4.7       53,962,687          1  53,962,687.0  53,962,687.0  53,962,687  53,962,687          0.0  Enter Data@jacobi.f90:34          \n",
            "     2.9       33,505,499          1  33,505,499.0  33,505,499.0  33,505,499  33,505,499          0.0  Exit Data@jacobi.f90:34           \n",
            "     2.3       26,432,454      2,000      13,216.2      15,756.5       1,456      96,532     11,782.4  Exit Data@laplace2d.f90:33        \n",
            "     2.1       23,830,326      2,000      11,915.2      16,149.0       4,018      41,026      7,095.9  Enter Data@laplace2d.f90:33       \n",
            "     1.2       13,321,343      2,000       6,660.7       6,075.0       4,528      67,685      2,579.3  Enqueue Launch@laplace2d.f90:33   \n",
            "     1.1       12,634,373      1,000      12,634.4      11,865.5       5,012      78,539      3,176.7  Wait@laplace2d.f90:41             \n",
            "     0.7        7,813,986      1,000       7,814.0       7,394.0       5,859      26,616      2,108.4  Enqueue Download@laplace2d.f90:41 \n",
            "     0.7        7,422,310      1,000       7,422.3       7,110.0       5,760      39,734      2,348.5  Enqueue Launch@laplace2d.f90:51   \n",
            "     0.4        4,999,745      1,000       4,999.7       4,713.5       4,306      28,181      1,531.1  Enter Data@laplace2d.f90:51       \n",
            "     0.4        4,317,441      1,000       4,317.4       3,868.5       3,178      19,298      1,863.3  Enqueue Upload@laplace2d.f90:33   \n",
            "     0.4        4,231,332          1   4,231,332.0   4,231,332.0   4,231,332   4,231,332          0.0  Wait@jacobi.f90:45                \n",
            "     0.2        1,903,077      1,000       1,903.1       1,677.5       1,445      15,005        839.0  Exit Data@laplace2d.f90:51        \n",
            "     0.1          946,070          1     946,070.0     946,070.0     946,070     946,070          0.0  Wait@jacobi.f90:34                \n",
            "     0.0          289,409         10      28,940.9      30,519.0      10,051      38,449      8,190.2  Enqueue Upload@jacobi.f90:34      \n",
            "     0.0          186,954          1     186,954.0     186,954.0     186,954     186,954          0.0  Device Init@jacobi.f90:34         \n",
            "     0.0          134,048          9      14,894.2      15,272.0       5,362      27,768      6,311.5  Enqueue Download@jacobi.f90:45    \n",
            "     0.0                0          4           0.0           0.0           0           0          0.0  Alloc@jacobi.f90:34               \n",
            "     0.0                0          1           0.0           0.0           0           0          0.0  Alloc@laplace2d.f90:33            \n",
            "     0.0                0          4           0.0           0.0           0           0          0.0  Create@jacobi.f90:34              \n",
            "     0.0                0      1,000           0.0           0.0           0           0          0.0  Create@laplace2d.f90:33           \n",
            "     0.0                0          4           0.0           0.0           0           0          0.0  Delete@jacobi.f90:45              \n",
            "     0.0                0      1,000           0.0           0.0           0           0          0.0  Delete@laplace2d.f90:41           \n",
            "\n",
            "Generated:\n",
            "    /content/lab3/report6.nsys-rep\n",
            "    /content/lab3/report6.sqlite\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "T7n_VwBrQ9Mf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DrS9UftFQ9l2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%file jacobi.f90\n",
        "\n",
        "\n",
        "program jacobi\n",
        "  use laplace2d\n",
        "  use nvtx\n",
        "  \n",
        "  implicit none\n",
        "  integer, parameter :: fp_kind=kind(1.0d0)\n",
        "  integer, parameter :: n=4096, m=4096, iter_max=1000\n",
        "  integer :: i, j, iter\n",
        "  real(fp_kind), dimension (:,:), allocatable :: A, Anew\n",
        "  real(fp_kind) :: tol=1.0e-6_fp_kind, error=1.0_fp_kind\n",
        "  real(fp_kind) :: start_time, stop_time\n",
        "\n",
        "  ! allocate ( A(0:n-1,0:m-1), Anew(0:n-1,0:m-1) )\n",
        "\n",
        "  ! A    = 0.0_fp_kind\n",
        "  ! Anew = 0.0_fp_kind\n",
        "\n",
        "  ! Set B.C.\n",
        "  ! A(0,:)    = 1.0_fp_kind\n",
        "  ! Anew(0,:) = 1.0_fp_kind\n",
        "  \n",
        "\n",
        "  call initialize(A, Anew, m, n)\n",
        "\n",
        "   \n",
        "  write(*,'(a,i5,a,i5,a)') 'Jacobi relaxation Calculation:', n, ' x', m, ' mesh'\n",
        " \n",
        "  call cpu_time(start_time) \n",
        "\n",
        "  iter=0\n",
        "  \n",
        "\n",
        "  !$acc data copy(A) create(Anew)\n",
        "  do while ( error .gt. tol .and. iter .lt. iter_max )\n",
        "\n",
        "    call nvtxStartRange(\"calcNext \")\n",
        "    error = calcNext(A, Anew, m, n)\n",
        "    call nvtxEndRange\n",
        "\n",
        "    call nvtxStartRange(\"swap \")\n",
        "    call swap(A, Anew, m, n)\n",
        "    call nvtxEndRange\n",
        "\n",
        "    if(mod(iter,100).eq.0 ) write(*,'(i5,f10.6)'), iter, error\n",
        "\n",
        "    iter = iter + 1\n",
        "\n",
        "  end do\n",
        "  !$acc end data\n",
        "\n",
        "\n",
        "  call cpu_time(stop_time) \n",
        "  write(*,'(a,f10.3,a)')  ' completed in ', stop_time-start_time, ' seconds'\n",
        "\n",
        "  ! deallocate (A,Anew)\n",
        "end program jacobi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_62_gaTQ9pD",
        "outputId": "c6e0e811-be42-4b9c-84ec-8df5a870b417"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting jacobi.f90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!make clean && make "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6X7e0NAzRq3-",
        "outputId": "e0f7f0d8-540d-466d-8e27-3375b8d608ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm -f *.o laplace \n",
            "nvfortran -fast -ta=tesla -Minfo=accel -o laplace laplace2d.f90 jacobi.f90 nvtx.f90 -L/opt/nvidia/hpc_sdk/Linux_x86_64/22.7/cuda/11.7/lib64  -lnvToolsExt\n",
            "laplace2d.f90:\n",
            "calcnext:\n",
            "     35, Generating copyin(a(:,:)) [if not already present]\n",
            "         Generating NVIDIA GPU code\n",
            "         36, !$acc loop gang ! blockidx%x\n",
            "             Generating reduction(max:error)\n",
            "         37, !$acc loop vector(128) ! threadidx%x\n",
            "     35, Generating implicit copy(error) [if not already present]\n",
            "         Generating copyout(anew(:,:)) [if not already present]\n",
            "     37, Loop is parallelizable\n",
            "swap:\n",
            "     55, Generating copyout(a(:,:)) [if not already present]\n",
            "         Generating copyin(anew(:,:)) [if not already present]\n",
            "         Generating NVIDIA GPU code\n",
            "         56, !$acc loop gang ! blockidx%x\n",
            "         57, !$acc loop vector(128) ! threadidx%x\n",
            "     57, Loop is parallelizable\n",
            "jacobi.f90:\n",
            "jacobi:\n",
            "     35, Generating create(anew(:,:)) [if not already present]\n",
            "         Generating copy(a(:,:)) [if not already present]\n",
            "nvtx.f90:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nsys profile -t nvtx --stats=true   ./laplace"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JSTsxPhRv6S",
        "outputId": "6d185da7-89ac-47c2-a3fc-5e3f27a1bc95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
            "Jacobi relaxation Calculation: 4096 x 4096 mesh\n",
            "    0  0.250000\n",
            "  100  0.002397\n",
            "  200  0.001204\n",
            "  300  0.000804\n",
            "  400  0.000603\n",
            "  500  0.000483\n",
            "  600  0.000403\n",
            "  700  0.000345\n",
            "  800  0.000302\n",
            "  900  0.000269\n",
            " completed in      0.705 seconds\n",
            "Generating '/tmp/nsys-report-ee6c.qdstrm'\n",
            "[1/3] [========================100%] report12.nsys-rep\n",
            "[2/3] [========================100%] report12.sqlite\n",
            "[3/3] Executing 'nvtxsum' stats report\n",
            "\n",
            "NVTX Range Statistics:\n",
            "\n",
            " Time (%)  Total Time (ns)  Instances  Avg (ns)   Med (ns)   Min (ns)  Max (ns)   StdDev (ns)   Style          Range       \n",
            " --------  ---------------  ---------  ---------  ---------  --------  ---------  -----------  -------  -------------------\n",
            "     27.6      275,195,434      1,000  275,195.4  271,358.0   264,175  2,491,952     70,558.2  PushPop  calcNext           \n",
            "     27.3      271,923,912      1,000  271,923.9  269,720.0   263,551    397,148      7,107.8  PushPop  calcNext inner loop\n",
            "     22.6      225,666,721      1,000  225,666.7  224,600.0   219,896    315,023      4,393.5  PushPop  swap               \n",
            "     22.5      224,697,568      1,000  224,697.6  223,691.0   219,284    313,880      4,259.0  PushPop  swap inner loop    \n",
            "\n",
            "Generated:\n",
            "    /content/lab3/report12.nsys-rep\n",
            "    /content/lab3/report12.sqlite\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tcbCS_JFRq7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hrLL8_a_Rq-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%file laplace2d.f90 \n",
        "\n",
        "module laplace2d\n",
        "  use nvtx \n",
        "  public :: initialize\n",
        "  public :: calcNext\n",
        "  public :: swap\n",
        "  public :: dealloc\n",
        "  contains\n",
        "    subroutine initialize(A, Anew, m, n)\n",
        "      integer, parameter :: fp_kind=kind(1.0d0)\n",
        "      real(fp_kind),allocatable,intent(out)   :: A(:,:)\n",
        "      real(fp_kind),allocatable,intent(out)   :: Anew(:,:)\n",
        "      integer,intent(in)          :: m, n\n",
        "\n",
        "      allocate ( A(0:n-1,0:m-1), Anew(0:n-1,0:m-1) )\n",
        "\n",
        "      A    = 0.0_fp_kind\n",
        "      Anew = 0.0_fp_kind\n",
        "\n",
        "      A(0,:)    = 1.0_fp_kind\n",
        "      Anew(0,:) = 1.0_fp_kind\n",
        "    end subroutine initialize\n",
        "\n",
        "    function calcNext(A, Anew, m, n)\n",
        "      integer, parameter          :: fp_kind=kind(1.0d0)\n",
        "      real(fp_kind),intent(inout) :: A(0:n-1,0:m-1)\n",
        "      real(fp_kind),intent(inout) :: Anew(0:n-1,0:m-1)\n",
        "      integer,intent(in)          :: m, n\n",
        "      integer                     :: i, j\n",
        "      real(fp_kind)               :: error\n",
        "\n",
        "      error=0.0_fp_kind\n",
        "      call nvtxStartRange(\"calcNext inner loop \" )\n",
        "\n",
        "      !$acc parallel loop reduction(max:error) copyin(A) copyout(Anew)\n",
        "      do j=1,m-2\n",
        "        do i=1,n-2\n",
        "          Anew(i,j) = 0.25_fp_kind * ( A(i+1,j  ) + A(i-1,j  ) + &\n",
        "                                       A(i  ,j-1) + A(i  ,j+1) )\n",
        "          error = max( error, abs(Anew(i,j)-A(i,j)) )\n",
        "        end do\n",
        "      end do\n",
        "      call nvtxEndRange\n",
        "      calcNext = error\n",
        "    end function calcNext\n",
        "\n",
        "    subroutine swap(A, Anew, m, n)\n",
        "      integer, parameter        :: fp_kind=kind(1.0d0)\n",
        "      real(fp_kind),intent(out) :: A(0:n-1,0:m-1)\n",
        "      real(fp_kind),intent(in)  :: Anew(0:n-1,0:m-1)\n",
        "      integer,intent(in)        :: m, n\n",
        "      integer                   :: i, j\n",
        "\n",
        "      call nvtxStartRange(\"swap inner loop \" )\n",
        "      !$acc parallel loop copyin(Anew) copyout(A)\n",
        "      do j=1,m-2\n",
        "        do i=1,n-2\n",
        "          A(i,j) = Anew(i,j)\n",
        "        end do\n",
        "      end do\n",
        "    call nvtxEndRange  \n",
        "    end subroutine swap\n",
        "\n",
        "    subroutine dealloc(A, Anew)\n",
        "      integer, parameter :: fp_kind=kind(1.0d0)\n",
        "      real(fp_kind),allocatable,intent(in) :: A\n",
        "      real(fp_kind),allocatable,intent(in) :: Anew\n",
        "\t  \n",
        "\t  deallocate (A,Anew)\n",
        "    end subroutine\n",
        "end module laplace2d"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAbmHsMSRDL8",
        "outputId": "636343d4-9e84-4811-9764-b3918df31438"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting laplace2d.f90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!make clean && make "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkZjSjyiSZVI",
        "outputId": "b713b8b6-b6e7-43fa-e3e4-28b3808eb181"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm -f *.o laplace \n",
            "nvfortran -fast -ta=tesla -Minfo=accel -o laplace laplace2d.f90 jacobi.f90 nvtx.f90 -L/opt/nvidia/hpc_sdk/Linux_x86_64/22.7/cuda/11.7/lib64  -lnvToolsExt\n",
            "laplace2d.f90:\n",
            "calcnext:\n",
            "     35, Generating copyin(a(:,:)) [if not already present]\n",
            "         Generating NVIDIA GPU code\n",
            "         36, !$acc loop gang ! blockidx%x\n",
            "             Generating reduction(max:error)\n",
            "         37, !$acc loop vector(128) ! threadidx%x\n",
            "     35, Generating implicit copy(error) [if not already present]\n",
            "         Generating copyout(anew(:,:)) [if not already present]\n",
            "     37, Loop is parallelizable\n",
            "swap:\n",
            "     55, Generating copyout(a(:,:)) [if not already present]\n",
            "         Generating copyin(anew(:,:)) [if not already present]\n",
            "         Generating NVIDIA GPU code\n",
            "         56, !$acc loop gang ! blockidx%x\n",
            "         57, !$acc loop vector(128) ! threadidx%x\n",
            "     57, Loop is parallelizable\n",
            "jacobi.f90:\n",
            "jacobi:\n",
            "     35, Generating create(anew(:,:)) [if not already present]\n",
            "         Generating copy(a(:,:)) [if not already present]\n",
            "nvtx.f90:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nsys profile -t nvtx --stats=true   ./laplace"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yt-EI53eScep",
        "outputId": "9503256e-9b47-4f80-ea9b-4dd7e6948f28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
            "Jacobi relaxation Calculation: 4096 x 4096 mesh\n",
            "    0  0.250000\n",
            "  100  0.002397\n",
            "  200  0.001204\n",
            "  300  0.000804\n",
            "  400  0.000603\n",
            "  500  0.000483\n",
            "  600  0.000403\n",
            "  700  0.000345\n",
            "  800  0.000302\n",
            "  900  0.000269\n",
            " completed in      0.706 seconds\n",
            "Generating '/tmp/nsys-report-b998.qdstrm'\n",
            "[1/3] [========================100%] report13.nsys-rep\n",
            "[2/3] [========================100%] report13.sqlite\n",
            "[3/3] Executing 'nvtxsum' stats report\n",
            "\n",
            "NVTX Range Statistics:\n",
            "\n",
            " Time (%)  Total Time (ns)  Instances  Avg (ns)   Med (ns)   Min (ns)  Max (ns)  StdDev (ns)   Style          Range       \n",
            " --------  ---------------  ---------  ---------  ---------  --------  --------  -----------  -------  -------------------\n",
            "     27.5      274,106,791      1,000  274,106.8  273,704.0   264,795   421,717      8,095.8  PushPop  calcNext           \n",
            "     27.4      273,156,593      1,000  273,156.6  272,745.5   264,054   417,450      7,870.5  PushPop  calcNext inner loop\n",
            "     22.6      225,161,800      1,000  225,161.8  224,228.0   219,665   291,037      3,935.0  PushPop  swap               \n",
            "     22.5      224,200,293      1,000  224,200.3  223,389.5   218,991   289,068      3,646.1  PushPop  swap inner loop    \n",
            "\n",
            "Generated:\n",
            "    /content/lab3/report13.nsys-rep\n",
            "    /content/lab3/report13.sqlite\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pK4zludYSn57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file jacobi.f90\n",
        "\n",
        "\n",
        "program jacobi\n",
        "  use laplace2d\n",
        "  use nvtx\n",
        "  \n",
        "  implicit none\n",
        "  integer, parameter :: fp_kind=kind(1.0d0)\n",
        "  integer, parameter :: n=4096, m=4096, iter_max=1000\n",
        "  integer :: i, j, iter\n",
        "  real(fp_kind), dimension (:,:), allocatable :: A, Anew\n",
        "  real(fp_kind) :: tol=1.0e-6_fp_kind, error=1.0_fp_kind\n",
        "  real(fp_kind) :: start_time, stop_time\n",
        "\n",
        "  ! allocate ( A(0:n-1,0:m-1), Anew(0:n-1,0:m-1) )\n",
        "\n",
        "  ! A    = 0.0_fp_kind\n",
        "  ! Anew = 0.0_fp_kind\n",
        "\n",
        "  ! Set B.C.\n",
        "  ! A(0,:)    = 1.0_fp_kind\n",
        "  ! Anew(0,:) = 1.0_fp_kind\n",
        "  \n",
        "\n",
        "  call initialize(A, Anew, m, n)\n",
        "\n",
        "   \n",
        "  write(*,'(a,i5,a,i5,a)') 'Jacobi relaxation Calculation:', n, ' x', m, ' mesh'\n",
        " \n",
        "  call cpu_time(start_time) \n",
        "\n",
        "  iter=0\n",
        "  \n",
        "\n",
        "  !$acc data copy(A) create(Anew)\n",
        "  do while ( error .gt. tol .and. iter .lt. iter_max )\n",
        "\n",
        " \n",
        "    error = calcNext(A, Anew, m, n)\n",
        " \n",
        " \n",
        "    call swap(A, Anew, m, n)\n",
        " \n",
        "\n",
        "    if(mod(iter,100).eq.0 ) write(*,'(i5,f10.6)'), iter, error\n",
        "\n",
        "    iter = iter + 1\n",
        "\n",
        "  end do\n",
        "  !$acc end data\n",
        "\n",
        "\n",
        "  call cpu_time(stop_time) \n",
        "  write(*,'(a,f10.3,a)')  ' completed in ', stop_time-start_time, ' seconds'\n",
        "\n",
        "  ! deallocate (A,Anew)\n",
        "end program jacobi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWXovvKpSoFe",
        "outputId": "a7b6a6c5-5e48-48af-d4df-714a687f5334"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting jacobi.f90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!make clean && make  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMOC-cbYSoIY",
        "outputId": "0c8ededc-c075-44b6-ef07-455bdb05f562"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm -f *.o laplace \n",
            "nvfortran -fast -ta=tesla -Minfo=accel -o laplace laplace2d.f90 jacobi.f90 nvtx.f90 -L/opt/nvidia/hpc_sdk/Linux_x86_64/22.7/cuda/11.7/lib64  -lnvToolsExt\n",
            "laplace2d.f90:\n",
            "calcnext:\n",
            "     35, Generating copyin(a(:,:)) [if not already present]\n",
            "         Generating NVIDIA GPU code\n",
            "         36, !$acc loop gang ! blockidx%x\n",
            "             Generating reduction(max:error)\n",
            "         37, !$acc loop vector(128) ! threadidx%x\n",
            "     35, Generating implicit copy(error) [if not already present]\n",
            "         Generating copyout(anew(:,:)) [if not already present]\n",
            "     37, Loop is parallelizable\n",
            "swap:\n",
            "     55, Generating copyout(a(:,:)) [if not already present]\n",
            "         Generating copyin(anew(:,:)) [if not already present]\n",
            "         Generating NVIDIA GPU code\n",
            "         56, !$acc loop gang ! blockidx%x\n",
            "         57, !$acc loop vector(128) ! threadidx%x\n",
            "     57, Loop is parallelizable\n",
            "jacobi.f90:\n",
            "jacobi:\n",
            "     35, Generating create(anew(:,:)) [if not already present]\n",
            "         Generating copy(a(:,:)) [if not already present]\n",
            "nvtx.f90:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nsys profile -t nvtx --stats=true   ./laplace"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQL7bAEKSoMK",
        "outputId": "f892c2c9-b636-4611-da92-98f34b521bd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
            "Jacobi relaxation Calculation: 4096 x 4096 mesh\n",
            "    0  0.250000\n",
            "  100  0.002397\n",
            "  200  0.001204\n",
            "  300  0.000804\n",
            "  400  0.000603\n",
            "  500  0.000483\n",
            "  600  0.000403\n",
            "  700  0.000345\n",
            "  800  0.000302\n",
            "  900  0.000269\n",
            " completed in      0.703 seconds\n",
            "Generating '/tmp/nsys-report-a571.qdstrm'\n",
            "[1/3] [========================100%] report14.nsys-rep\n",
            "[2/3] [========================100%] report14.sqlite\n",
            "[3/3] Executing 'nvtxsum' stats report\n",
            "\n",
            "NVTX Range Statistics:\n",
            "\n",
            " Time (%)  Total Time (ns)  Instances  Avg (ns)   Med (ns)   Min (ns)  Max (ns)  StdDev (ns)   Style          Range       \n",
            " --------  ---------------  ---------  ---------  ---------  --------  --------  -----------  -------  -------------------\n",
            "     54.9      273,138,146      1,000  273,138.1  273,568.5   264,059   414,084      7,505.8  PushPop  calcNext inner loop\n",
            "     45.1      224,000,825      1,000  224,000.8  223,370.0   219,577   254,441      2,681.7  PushPop  swap inner loop    \n",
            "\n",
            "Generated:\n",
            "    /content/lab3/report14.nsys-rep\n",
            "    /content/lab3/report14.sqlite\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QYCiSp-ESoOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimize Loop Schedules\n",
        "The compiler has analyzed the loops in our two main functions and scheduled the iterations of the loops to run in parallel on our GPU and Multicore CPU. The compiler is usually pretty good at choosing how to break up loop iterations to run well on parallel accelerators, but sometimes we can eke out just a little more performance by guiding the compiler to make specific choices. First, let's look at the choices the compiler made for us. We'll focus on the calcNext routine, but you should look at the swap routine too. Here's the compiler feedback for that routine:\n",
        "```\n",
        "calcnext:\n",
        "     58, Generating copyin(a(:,:))\n",
        "         Accelerator kernel generated\n",
        "         Generating Tesla code\n",
        "         58, Generating reduction(max:error)\n",
        "         59, !$acc loop gang ! blockidx%x\n",
        "         60, !$acc loop vector(128) ! threadidx%x\n",
        "     58, Generating implicit copy(error)\n",
        "         Generating copyout(anew(:,:))\n",
        "     60, Loop is parallelizable\n",
        "```     \n",
        "The main loops on interest in calcNext are on lines 59 and 60. I see that the compiler has told me what loop clauses it chose for each of those loops. The outermost loop is treated as a gang loop, meaning it broke that loop up into chunks that can be spread out across the GPU or CPU cores easily. If you have programmed in CUDA before, you'll recognize that the compiler is mapping this loop to the CUDA thread blocks. The innermost loop is mapped instead to vector parallelism. You can think of a vector as some number of data cells that get the same operation applied to them at the same time. On any modern processor technology you need this mixture of coarse grained and fine grained parallelism to effectively use the hardware. Vector (fine grained) parallelism can operate extremely efficiently when performing the same operation on a bunch of data, but there's limits to how long a vector you can build. Gang (coarse grained) parallelism is highly scalable, because each chunk of work can operate completely independently of each other chunk, making it ideal for allowing processor cores to operate independently of each other.\n",
        "\n",
        "Let's look at some loop clauses that allow you to tune how the compiler maps our loop iterations to these different types of parallelism.\n",
        "\n",
        "## Collapse Clause\n",
        "The collapse clause allows us to transform a multi-dimensional loop nest into a single-dimensional loop. This process is helpful for increasing the overall length (which usually increases parallelism) of our loops, and will often help with memory locality. Let's look at the syntax.\n",
        "\n",
        "```\n",
        "!$acc parallel loop collapse( N )\n",
        "```\n",
        "\n",
        "Where N is the number of loops to collapse.\n",
        "\n",
        "```\n",
        "!$acc parallel loop collapse( 3 )\n",
        "do i = 1, N\n",
        "    do j = 1, M\n",
        "        do k = 1, Q\n",
        "            < loop code >\n",
        "        end do\n",
        "    end do\n",
        "end do\n",
        "```\n",
        "This code will combine the 3-dimensional loop nest into a single 1-dimensional loop. The loops in our example code are fairly long-running, so I don't expect a lot of speed-up from collapsing them together, but let's try it anyway.\n",
        "\n",
        "Implementing the Collapse Clause"
      ],
      "metadata": {
        "id": "hV00cu7nILuH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fsclCo1zQycy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n-G6lH-IQyf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mJMjrn9KQyjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mo8_Idv1Qyl8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}